\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{chngcntr}
\usepackage{float}
\usepackage{tabu}
\usepackage{bm}
\usepackage[lite]{amsrefs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}\usepackage{xcolor}
%\graphicspath{ {./img/} }


\counterwithin*{equation}{section}

\newcommand{\R}{\mathbb{R}}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{1}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}
    
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

%\setlength{\parindent}{0pt}

\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}%
                     =}

\begin{document}


\title{NLP assignment \#1}
\author{Andrius Buinovskij - 18-940-270}
\date{}

\maketitle

\textbf{Q1 b) i)}
	
	Well since all inputs are 1 and all weights are 1 then
	
	\begin{align}
		\mathbf{s}^1_1 &= \sum^3_{i=1} \mathbf{x}_i\cdot \mathbf{w}^1_{i, 1} = 3\\
		\mathbf{s}^1_2 &= \sum^3_{i=1} \mathbf{x}_i\cdot \mathbf{w}^1_{i, 2} = 3
	\end{align}
	
	Where $\mathbf{s}^i_j$ is the sum input to the $j$'th neuron in the $i$'th layer, and $\mathbf{s}^i$ is a vector whose length is equal to the number of neurons in the $i$'th layer, with the input being the $0$'th layer. So $\mathbf{s}^1$ is of length 2 since $1$'st layer has 2 neurons. 
	
	Let $\mathbf{n}^i_j$ be the output of the $j$'th neuron in the $i$'th layer, then of course $\mathbf{n}^i$ is a vector of length equal to the number of neurons in the $i$'th layer:
	
	\begin{align}
		n^i_j &= ReLU\big(\mathbf{s}^i_j\big)
	\end{align}
	
	So in our case We get
	
	\begin{align}
		\mathbf{n}^1_1 &= ReLU\big(\mathbf{s}^1_1\big) =  ReLU\big( 3 \big) = 3\\
	\mathbf{n}^1_2 &= ReLU\big(\mathbf{s}^1_2\big) =  ReLU\big( 3 \big) = 3	
	\end{align}
	
	Same steps in the next layer:
	
	\begin{align}
		\mathbf{s}^2_1 = 3\cdot 1 + 3\cdot 1 = 6
	\end{align}
	
	And now We pass this through a sigmoid for our output instead of a $ReLU$ so We get
	
	\begin{align}
		out = \sigma\big(\mathbf{s}^2_1)\big) = \frac{1}{1+e^{-6}} = 0.99752737684
	\end{align}

\textbf{Q1 b) ii)}

	\begin{align}
		\frac{d }{d \mathbf{w}^1_{j, k}} \mathbf{x}_i\cdot\mathbf{w}^1_{j, k} = \mathbf{x}_i
	\end{align}
	
	Since the inputs are all 1 this simplifies to 

	\begin{align}
		\frac{d }{d \mathbf{w}^1_{j, k}} \mathbf{x}_i\cdot\mathbf{w}^1_{j, k} = \frac{d }{d \mathbf{w}^1_{j, k}} 1\cdot\mathbf{w}^1_{j, k} = 1
	\end{align}
	
	Now for the second layer:
	
	\begin{align}
		\frac{d}{d \mathbf{w}^2_{1, 1}} f = \frac{d}{d \mathbf{w}^2_{1, 1}} \mathbf{n}_{1, 1}\cdot\mathbf{w}^2_{1, 1} = \frac{d}{d \mathbf{w}^2_{1, 1}} ReLU\big(\mathbf{s}_{1, 1}\big)\cdot\mathbf{w}^2_{1, 1} = ReLU\big(\mathbf{s}_{1, 1}\big)
	\end{align}
	
	Since $ReLU\big(\mathbf{s}_{1, 1}\big)$ is just a constant w.r.t. $\mathbf{w}^2_{1, 1}$. In this case We simply get
	
	\begin{align}
		\frac{d}{d \mathbf{w}^2_{1, 1}} f = ReLU\big(\mathbf{s}_{1, 1}\big) = 3
	\end{align}
	
	And likewise for the other weight.
	
\textbf{Q1 b) iii)}

	\begin{align}
		L_{BCE} &= -(y\log(\hat{y}) + (1-y)\log(1-\hat{y}))\\
		&= -(0\cdot\log(0.99752737684) + (1-0)\log(1-0.99752737684))\\
		&= -(\log(0.00247262316))\\
		&= 2.60684206722
	\end{align}
	
\textbf{redone}

	Alright let's just roll all of these questions into one and do an iteration of backprop.
	
	The forward pass is trivial.
	
	Let $\mathbf{x}^{l}_i$ be the output of the $i$'th neuron in the $l$'th layer. 
	
	Let $\mathbf{w}^l_{i, j}$ be the weight belonging to $j$'th neuron multiplying the $i$'th input in the $l$'th layer.  $\mathbf{w}^l_{.,j}$ is then simply the weight vector associated with the $j$'th neuron in the $l$'th layer.
	
	Let $\mathbf{s}^l_{j}$ be $(\mathbf{x}^{l-1})^\top\mathbf{w}^l_{.,j}$, the weighted sum of inputs to the $j$'th neuron in the $l$'th layer.
	
	We can then say that $x^l_i = \sigma(\mathbf{s}^l_j)$, where $\sigma$ is the nonlinearity of choice.
	
	Then define
	
	\begin{align}
		\delta^l_j = \frac{\partial L_{BCE}}{\partial \mathbf{s}^l_j}
	\end{align}
	
	And finally 
	
	\begin{align}
		\delta^{l-1}_j &= \sum^{d^l}_{i=1}\frac{\partial L_{BCE}}{\partial \mathbf{s}^l_i} \frac{\partial \mathbf{s}^l_i}{\partial \mathbf{x}^{l-1}_j}\frac{\partial \mathbf{x}^{l-1}_j}{\partial \mathbf{s}^{l-1}_j}\\
		&= \sum^{d^l}_{i=1}  \delta^l_i \frac{\partial \mathbf{s}^l_i}{\partial \mathbf{x}^{l-1}_j}\frac{\partial \mathbf{x}^{l-1}_j}{\partial \mathbf{s}^{l-1}_j}
	\end{align}
	
	Where $d^l$ is the number of neurons in layer $l$. So now We have a recursive definition which uses dynamic programming. This could be further nuanced by expressing things in matrix notation, but it's good enough for present purposes.
	
	 Here is also a picture since just looking at symbols is a nightmare.
	 
	 So now
	 
	 \begin{align}
	 	\frac{\partial L_{BCE}}{\partial \mathbf{s}^2_1} = \frac{\partial L_{BCE}}{\partial \hat{y}} \frac{\partial \hat{y}}{\mathbf{s}^2_1} 
	 \end{align}
	 
	 \begin{align}
	 	\frac{\partial L_{BCE}}{\partial \hat{y}} &= \frac{\partial }{\partial \hat{y}}-\big(y\log(\hat{y}) + (1-y)\log(1-\hat{y}) \big)\\
	 	&=-y\frac{1}{\hat{y}}  - (1-y)\frac{\partial }{\partial \hat{y}} \log(1-\hat{y}) \\
	 	&=-y\frac{1}{\hat{y}}  - (1-y)\frac{-1}{1-\hat{y}}\\
	 	&=-y\frac{1}{\hat{y}}  + \frac{1-y}{1-\hat{y}}
	 \end{align}
	 
	 \begin{align}
	 	\frac{\partial \hat{y}}{\mathbf{s}^2_1}  &= \frac{\partial }{\mathbf{s}^2_1} \sigma(\mathbf{s}^2_1)\\
	 	&= \sigma(\mathbf{s}^2_1)\cdot(1-\sigma(\mathbf{s}^2_1))
	 \end{align}
	 
	 So then We have 
	
	\begin{align}
	 	\delta^2_1 = \frac{\partial L_{BCE}}{\partial \mathbf{s}^2_1} &= \frac{\partial L_{BCE}}{\partial \hat{y}} \frac{\partial \hat{y}}{\mathbf{s}^2_1} \\
	 	&= \bigg( -y\frac{1}{\hat{y}}  + \frac{1-y}{1-\hat{y}} \bigg) \bigg( \sigma(\mathbf{s}^2_1)\cdot(1-\sigma(\mathbf{s}^2_1)) \bigg)
	 \end{align}
	 
	 Alright. We only have one more of these to figure out:
	 
	 \begin{align}
	 	\delta^1_j &= \sum^{d^2}_{i=1}  \delta^2_i \frac{\partial \mathbf{s}^2_i}{\partial \mathbf{x}^{1}_j}\frac{\partial \mathbf{x}^{1}_j}{\partial \mathbf{s}^{1}_j}
	 \end{align}
	 
	 
	 But since $d^2 = 1$, i.e. there is only one neuron in the output layer, We have

	\begin{align}
	 	\delta^1_j &= \sum^{d^2}_{i=1}  \delta^2_i \frac{\partial \mathbf{s}^2_i}{\partial \mathbf{x}^{1}_j}\frac{\partial \mathbf{x}^{1}_j}{\partial \mathbf{s}^{1}_j}\\
	 	&=   \delta^2_1 \frac{\partial \mathbf{s}^2_1}{\partial \mathbf{x}^{1}_j}\frac{\partial \mathbf{x}^{1}_j}{\partial \mathbf{s}^{1}_j}\\
	 	&=   \delta^2_1 \mathbf{w}^2_j \sigma(\mathbf{s}^{1}_j)(1-\sigma(\mathbf{s}^{1}_j))
	 \end{align}
	 
	 Now for the gradient update We will of course need
	 
	 \begin{align}
	 	\frac{\partial L_{BCE}}{\partial \mathbf{w}^l_{i, j}}
	 \end{align}
	 
	 But this can be easily derived since
	 
	 \begin{align}
	 	\frac{\partial L_{BCE}}{\partial \mathbf{w}^l_{i, j}} &= \frac{\partial L_{BCE}}{\partial \mathbf{s}^l_j}\frac{\partial \mathbf{s}^l_j}{\partial \mathbf{w}^l_{i, j}}\\
	 	&= \delta^l_j\mathbf{x}^{l-1}_i
	 \end{align}
	 
	 Now We do a forward pass and We know that $\mathbf{s}^1_i = 3$ and $\mathbf{s}^2_1 = 6$. Plugging in values We then get

	\begin{align}
	 	\delta^2_1 &= \bigg( -y\frac{1}{\hat{y}}  + \frac{1-y}{1-\hat{y}} \bigg) \bigg( \sigma(\mathbf{s}^2_1)\cdot(1-\sigma(\mathbf{s}^2_1)) \bigg)\\
	 	&= \bigg( \frac{1}{1-0.99752737684} \bigg) \bigg( \sigma(6)\cdot(1-\sigma(6)) \bigg)\\
	 	&= \bigg( \frac{1}{1-0.99752737684} \bigg) \bigg( 0.99752737684\cdot(1-0.99752737684) \bigg)\\
	 	&= 404.428792942\cdot 0.00246650929\\
	 	&= 0.99752737493
	 \end{align}
	 
	 Similarly We have
	 
	 \begin{align}
	 	\delta^1_j &=   \delta^2_1 \mathbf{w}^2_j \sigma(\mathbf{s}^{1}_j)(1-\sigma(\mathbf{s}^{1}_j))\\
	 	\delta^1_1 &=   \delta^2_1 \mathbf{w}^2_1 \sigma(\mathbf{s}^{1}_1)(1-\sigma(\mathbf{s}^{1}_1))\\
	 	&=   0.997527374931\cdot  \sigma(3)(1-\sigma(3))\\
	 	&=   0.997527374931\cdot  0.95257412682 \cdot 0.04742587317\\
	 	&= 0.04506495478
	 \end{align}
	 
	 $\delta^1_2$ is equal to $\delta^1_1$. 
	 
	 Now that We have the deltas We can use

	\begin{align}
	 	\frac{\partial L_{BCE}}{\partial \mathbf{w}^l_{i, j}} &= \delta^l_j\mathbf{x}^{l-1}_i
	 \end{align}
	 
	 For weights in the first layer this means

	\begin{align}
	 	\frac{\partial L_{BCE}}{\partial \mathbf{w}^1_{i, j}} &= \delta^1_j\mathbf{x}^{0}_i\\
	 	&= 0.04506495478\cdot 1\\
	 	&= 0.04506495478
	 \end{align}
	 
	 Where the 0'th layer is the input layer. 

	\begin{align}
	 	\frac{\partial L_{BCE}}{\partial \mathbf{w}^2_{i, j}} &= \delta^2_j\mathbf{x}^{1}_i\\
	 	&= 0.99752737493\cdot 3\\
	 	&= 2.99258212479
	 \end{align}
	 
	 All that is left is to perform a step for each weight. All weights in the first layer simplify to 
	 
	 \begin{align}
	 	w^1_{i, j} &= 1 - 0.1\cdot 0.04506495478\\
	 	&= 0.99549350452
	 \end{align}
	 
	 \textbf{Sidenote}
	 
	 So when actually implementing this, for the next assignemnt, We get 

	\begin{align}
	 	\delta^2_1 = \frac{\partial L_{BCE}}{\partial \mathbf{s}^2_1} &= \frac{\partial L_{BCE}}{\partial \hat{y}} \frac{\partial \hat{y}}{\mathbf{s}^2_1} \\
	 	&= \bigg( -y\frac{1}{\hat{y}}  + \frac{1-y}{1-\hat{y}} \bigg) \bigg( \sigma(\mathbf{s}^2_1)\cdot(1-\sigma(\mathbf{s}^2_1)) \bigg)\\
	 	&= \bigg( -y\frac{1}{\hat{y}}  + \frac{1-y}{1-\hat{y}} \bigg) \bigg(\hat{y}\cdot(1-\hat{y}) \bigg)\\
	 	&=  - \hat{y}\cdot(1-\hat{y})  \cdot \frac{y}{\hat{y}}  + \hat{y}\cdot(1-\hat{y}) \cdot \frac{1-y}{1-\hat{y}} \\
	 	&=  - (1-\hat{y})  \cdot y + \hat{y} \cdot (1-y)
	 \end{align}
	 
\newpage

\textbf{Q4 a) i)}

	We're going to use linearity of expectation.
	
	Let $I_i, i\in \{1\ldots |V|\}$ be the indicator variable for whether or not the word $i$ appears in Mary's sampling.
	
	Since draws are with replacement and the distribution is uniform, for a token \textit{not} to appear, all $n$ draws must have chosen another token. 
	
	The probability of not choosing token $i$ in a particular draw is of course $(|V|-1)/|V|$. The probability of not choosing token $i$ over $n$ samples is then
	
	\begin{align}
		\bigg(\frac{|V|-1}{|V|}\bigg)^n
	\end{align}
	
	However, We are interested in the opposite event - We want to know the probability of \textit{not} not choosing token $i$, which is given by
	
	\begin{align}
		1 - \bigg(\frac{|V|-1}{|V|}\bigg)^n
	\end{align}
	
	By linearity of expectation and the fact that the expected value of an indicator is simply the probability of it being 1, We get
	
	\begin{align}
		E\bigg(\sum^{|V|}_{i=1} = I_i \bigg) = \sum^{|V|}_{i=1} E(I_i) =  \sum^{|V|}_{i=1} P(I_i = 1) = |V|\cdot \bigg( 1 - \bigg(\frac{|V|-1}{|V|}\bigg)^n\bigg)
	\end{align}
	
\textbf{Q4 a) ii)}

	Given $n$ draws, what is the probability that all the words will appear in the sample? 
	
	All of my attempts have failed, and a whole lot of searching leads me to conclude that the probability is:
	
	\begin{align}
		\frac{|V|!}{|V|^n}S_2(n-1, |V|-1)
	\end{align}
	
	Where $S_2(n-1, |V|-1)$ is the Stirling number of the second kind. 
	
\textbf{Q4 b) i)}

	Alright, so, let $X$ be the number of samples before "work" and "hard" bigram appears.
	
	Let $ps$ (partial success) mean that the word "work" has been sampled. Then by law of total expectation We can say
	
	\begin{align}
		E(X) = (1+E(X|ps))\cdot P(ps) + (1+E(X|ps'))\cdot P(ps')
	\end{align}
	
	Where We the 1's in there come from the fact that a single sample has been taken.
	
	Now, given partial success We either fully succeed $(fs)$ or fail and go back to the beginning, so:
	
	\begin{align}
		E(X|ps) &= (1+E(X|ps, fs))\cdot P(fs|ps) + (1+E(X|ps, fs'))\cdot P(fs'|ps)\\
		&= (1)\cdot \frac{1}{|V|} + (1+E(X))\cdot \frac{|V|-1}{|V|}
	\end{align}
	
	Where given full success, expected number of draws to get a success is zero.  Given a failure We're back to $E(X)$.
	
	Of course, given $ps'$, the compliment of partial success i.e. partial failure, $E(X|ps') = E(X)$.
	
	Putting it all together We get:
	
	\begin{align}
		E(X) &= (1+E(X|ps))\cdot P(ps) + (1+E(X|ps'))\cdot P(ps')\\
		&= \bigg(1+\frac{1}{|V|} + (1+E(X))\cdot \frac{|V|-1}{|V|}\bigg)\cdot \frac{1}{|V|} + (1+E(X))\cdot \frac{|V| - 1}{|V|}\\
		&= \frac{1}{|V|}+\frac{1}{|V|^2} + (1+E(X))\cdot \frac{|V|-1}{|V|^2} + (1+E(X))\cdot \frac{|V|-1}{|V|}\\
		&= \frac{1}{|V|}+\frac{1}{|V|^2} + \frac{|V|-1}{|V|^2} + \frac{(|V|-1)E(X)}{|V|^2} + \frac{|V|-1}{|V|}+\frac{E(X)(|V|-1)}{|V|}
	\end{align}
	
	Now We just need to simplify this mess.
	
	\begin{align}
		&E(X) = \frac{1}{|V|}+\frac{1}{|V|^2} + \frac{|V|-1}{|V|^2} + \frac{(|V|-1)E(X)}{|V|^2} + \frac{|V|-1}{|V|}+\frac{E(X)(|V|-1)}{|V|}\\
		&E(X) - \frac{(|V|-1)E(X)}{|V|^2}-\frac{E(X)(|V|-1)}{|V|} = \frac{1}{|V|}+\frac{1}{|V|^2} + \frac{|V|-1}{|V|^2}  + \frac{|V|-1}{|V|}\\
		&E(X)\bigg(1 - \frac{(|V|-1)}{|V|^2}-\frac{(|V|-1)}{|V|}\bigg) = \frac{1}{|V|}+\frac{1}{|V|^2} + \frac{|V|-1}{|V|^2}  + \frac{|V|-1}{|V|}\\
		&E(X) = \frac{\frac{1}{|V|}+\frac{1}{|V|^2} + \frac{|V|-1}{|V|^2}  + \frac{|V|-1}{|V|}}{1 - \frac{(|V|-1)}{|V|^2}-\frac{(|V|-1)}{|V|}}
	\end{align}
	
	Beautiful. If You plug $|V|=2$ in there You get 6, which is the well known result of expected number of tosses to get 2 heads in a row. 
	
\textbf{Q4 b) ii)}

	Well, the probability of "work" not appearing in $n$ draws is
	
	\begin{align}
		\bigg( \frac{|V|-1}{|V|}\bigg)^n
	\end{align}
	
	And of course the probability of it appearing is
	
	\begin{align}
		1 - \bigg( \frac{|V|-1}{|V|}\bigg)^n
	\end{align}
	
	And Mary wants this probability to be above 0.95:

	\begin{align}
		1 - \bigg( \frac{|V|-1}{|V|}\bigg)^n &\ge 0.95\\
		- \bigg( \frac{|V|-1}{|V|}\bigg)^n &\ge -0.05\\
		\bigg( \frac{|V|-1}{|V|}\bigg)^n &\le 0.05\\
		\log\bigg(\bigg( \frac{|V|-1}{|V|}\bigg)^n\bigg) &\le \log(0.05)\\
		n\cdot\log\bigg( \frac{|V|-1}{|V|}\bigg) &\le \log(0.05)\\
		n &\ge \frac{\log(0.05)}{\log\big( \frac{|V|-1}{|V|}\big)}\\
	\end{align}
	
	Where the last sign flips since log of a quantity less than 1 is negative. 

\textbf{Q4 c) i)}

	The answer here is identical to the answer to question \textbf{b)i)}, by the fact that the tokens are all equally likely.
	
	
\textbf{Q4 c) ii)}

	Okay so We have
	
	\begin{align}
		h^0_t = f(w_0x_{t-1} + w_1 x_t + w_2h^0_{t-1} + b_0)
	\end{align}
	
	So, that's our first hidden state. $f$ is some arbitrary non-linearity (or maybe linearity, whatever, a function).
	
	$w_0$ multiplies previous input, $w_1$ multiplies current input, $w_2$ multiplies the previous hidden state and We have a bias term. 
	
	The output then is
	
	\begin{align}
		y_t = g(w_3h^0_t + b_1)
	\end{align}
	
	So $w_3$ is multiplying the hidden state, there's a bias  term and another function. 
	
	The goal is to output $x_{t-1}$ and $x_t$ are the same word.
	
	Okay so assume that  the two words are equal and the bias is 0:
	
	\begin{align}
		h^0_t &= f(w_0x_{t-1} + w_1 x_t + w_2h^0_{t-1} + b_0)\\
		&= f(w_0x_{t-1} + w_1 x_{t-1})\\
		&= f(x_{t-1}(w_0+w_1))
	\end{align}
	
	So clearly one way to go is to let $w_0$ be -1 and $w_1$ be 1, so that when they are added, We get zero in the case of equality. Then $f$ can just be like a check as to whether the input is 0, and if it is output 1, otherwise output 0.
	
	$g$ is then simply the identity function and $b_1$ is also 0.
	
	So, $w_0 = -1$, $w_1 = 1$, $w_2 = 0$, $w_3=1$, all biases are zero, $f$ is a boolean check for whether the input is $0$ and $g$ is the identity function.
	
\textbf{Q4 c) iii)}	

	So We simply use the second hidden layer as a counter. $w_4, w_5=1$, $b_1, b_2 = 0$ and $g$ and $h$ are still the identity function, and $f$ is the boolean 0 check.	

\textbf{Q4 c) iv)}		
	
	I think the non-uniform unigram language model has the greater probability of drawing two of the same token in a row. 
	
	This isn't a proof, but the reasoning is as follows: let $p_i$ be the probability of drawing the $i$'th token. Now suppose We alter the distribution such that $p_i' = p_i - x$. Now, We'll have to add that $x$ to some other $p_j$ to preserve a distribution, and since the sample are identical, sampling $p_j$ twice in a row will be more likely than if We had just left the probabilities alone.
	
	
\newpage

\textbf{Q5 a)}

	Well first let's re-derive this mess.
	
	\begin{align}
		\sum_{\mathbf{t}\in\mathcal{T}^N} \exp\bigg\{ \sum^N_{n=1} \text{score}\big( \langle t_{n-1}, t_n \rangle, \mathbf{w} \big) \bigg\}
	\end{align}	
	
	So let's start with inside the bracket comes from
	
	\begin{align}
		\text{score}\big(\mathbf{t}, \mathbf{w} \big) = \sum^N_{n=1} \text{score}\big( \langle t_{n-1}, t_n \rangle, \mathbf{w} \big)
	\end{align}
	
	So this is our simplifying assumption, and I suppose it's also what makes this a "Conditional Random Field". $\mathbf{t}$ stands for "tag" and it's the, well, tagging of the word sequence $\mathbf{w}$. So We're saying that the score for  the tag does not have to be calculated all in one go, but rather can be done in parts, where each part only depends on it's predecessor. Aight. Markov yay.
	
	$N$ is the length of the sentence by the way, so the length of both $\mathbf{t}$ and $\mathbf{w}$. 
	
	$\mathcal{T}$ is the set of all possible tags for any particular word. Since there are $N$ words, the total number of possible tags for $\mathbf{w}$ is then $\mathcal{T}^N$.
	
	So then the expression is simply calculating the normalizing constant, since it's summing over all possible tags $\mathbf{t}\in\mathcal{T}^N$.
	
	So, up next We get
	
	\begin{align}
		&\sum_{\mathbf{t}\in\mathcal{T}^N} \exp\bigg\{ \sum^N_{n=1} \text{score}\big( \langle t_{n-1}, t_n \rangle, \mathbf{w} \big) \bigg\}\\
		&= \sum_{\mathbf{t}_{1:n}\in\mathcal{T}^N} \prod^N_{n=1} \exp \big\{ \text{score}(\langle \mathbf{t}_{n-1}, \mathbf{t}_n \rangle, \mathbf{w}) \big\}
	\end{align}	
	
	Okay so what have We done here. For one, $\mathbf{t}$ now has a subscript. Sure.
	
	We've converted the inner sum to a product, which is fine - before We were taking the exponent of a sum, but product of a bunch of exponents will sum the exponents so no worries there. We then still sum over every possible tag. Sure. 
	
	\begin{align}
		&\sum_{\mathbf{t}\in\mathcal{T}^N} \exp\bigg\{ \sum^N_{n=1} \text{score}\big( \langle t_{n-1}, t_n \rangle, \mathbf{w} \big) \bigg\}\\
		&= \sum_{\mathbf{t}_{1:n}\in\mathcal{T}^N} \prod^N_{n=1} \exp \big\{ \text{score}(\langle \mathbf{t}_{n-1}, \mathbf{t}_n \rangle, \mathbf{w}) \big\}\\
		&= \sum_{\mathbf{t}_{1:N-1}\in\mathcal{T}^{N-1}} \sum_{t_N\in \mathcal{T}} \prod^N_{n=1} \exp \big\{ \text{score}(\langle \mathbf{t}_{n-1}, \mathbf{t}_n \rangle, \mathbf{w}) \big\}
	\end{align}	
	
	So, the first change is that the subscript for $\mathbf{t}$ now goes to $N-1$. So We're just taking out the last tag in the sequence of tags $\mathbf{t}_N$.
	
	Since that last tag could be anything, We sum over all the possible tags, so $t_N\in\mathcal{T}$. There are $|\mathcal{T}|$ choices.
	
	I mean really You could rewrite the first sum as $N$ sums each over $|\mathcal{T}|$ terms each. So We're just splitting one out. No worries.
	
	\begin{align}
		&\sum_{\mathbf{t}\in\mathcal{T}^N} \exp\bigg\{ \sum^N_{n=1} \text{score}\big( \langle t_{n-1}, t_n \rangle, \mathbf{w} \big) \bigg\}\\
		&= \sum_{\mathbf{t}_{1:n}\in\mathcal{T}^N} \prod^N_{n=1} \exp \big\{ \text{score}(\langle \mathbf{t}_{n-1}, \mathbf{t}_n \rangle, \mathbf{w}) \big\}\\
		&= \sum_{\mathbf{t}_{1:N-1}\in\mathcal{T}^{N-1}} \sum_{t_N\in \mathcal{T}} \prod^N_{n=1} \exp \big\{ \text{score}(\langle \mathbf{t}_{n-1}, \mathbf{t}_n \rangle, \mathbf{w}) \big\}\\
		&= \sum_{\mathbf{t}_{1:N-1}\in\mathcal{T}^{N-1}} \prod^{N-1}_{n=1} \exp \big\{ \text{score}(\langle \mathbf{t}_{n-1}, \mathbf{t}_n \rangle, \mathbf{w}) \big\} \times \sum_{t_N\in\mathcal{T}} \exp \big\{ \text{score}(\langle \mathbf{t}_{N-1}, \mathbf{t}_N \rangle, \mathbf{w}) \big\}
	\end{align}	
	
	Okay so if I had to explain this in words, how would I do it?
	
	Well, everything hinges on two parts - the first being that the score can be decomposed into partial computations. If there is no substructure, no simplification is possible. 
	
	The second I suppose is the distributative property I suppose. The key bit is that You can take stuff out of the product and sum it, and if You take the product of the stuff and the sum it all still works. Or rather, You have some operator $B$. $B$ is iterating over all possible versions of the tags. Then You also have some operator $A$, and You can take stuff out of $B$, combine it with $A$ and take output of $A$ and stick it back into $B$ and it all still works. Now You're reducing the exponential number of things You had to deal with. Sure.
	
	
	
	
	
	
	
	
\end{document} 