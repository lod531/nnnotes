\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{chngcntr}
\usepackage{float}
\usepackage{tabu}
\usepackage{bm}
\usepackage[lite]{amsrefs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}\usepackage{xcolor}
%\graphicspath{ {./img/} }


\counterwithin*{equation}{section}

\newcommand{\R}{\mathbb{R}}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{1}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}
    
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}


\begin{document}


\title{Optimization for Data Science}
\author{Bernd G$\ddot{\text{a}}$rtner, Martin Jaggi}
\date{}

\maketitle

\section{Theory of Convex Functions}

	\subsection{Mathematical Notation}
	
		Bold lower case symbols are used for vectors, and normal font is used for their coordinates e.g. $\mathbf{x} = (x_1, x_2\ldots x_d)\in \mathbb{R}^d$. Vectors are assumed to be column vectors unless transposed, so $\mathbf{x}$ is a column and $\mathbf{x}^\top$ is a row. Revolutionary. $\mathbf{x^\top y} = \sum^d_{i=1} x_i y_i$ for $\mathbf{x, y}\in\mathbb{R}^d$, i.e. the damn dot product.
		
		$\|\mathbf{x}\|$ is the Euclidian norm i.e. the length of a vector and the squared Euclidian norm is $\|\mathbf{x}\|^2 = \mathbf{x^\top x}$.
		
		Conventions here include that $\mathbb{N} = \{1, 2,\ldots \}$, so natural numbers do not include 0 and $\mathbb{R}_+ = \{x\in\mathbb{R}: x\ge 0 \}$, so the positive real numbers do include 0.
		
	\subsection{Cauchy-Schwarz Inequality}

		\begin{lemma}[Cauchy-Schwarz Inequality]
			Let $\mathbf{u, v}\in\mathbb{R}^d$, then
			
			\[ |\mathbf{u^\top v| \le \|u\|\|v\|}. \]
		\end{lemma}

		\begin{proof}
			The core of the dot product, to me anyhow, is that given that $\theta$ is the acute angle between vector $\mathbf{u, v}$, We have:
			
			\[ \|\mathbf{u^\top v \| = \|u\|\|v\|}\cos(\theta) \]
			
			Thankfully, We can assume the above. For an intuition for the above fact, see \href{https://www.youtube.com/watch?v=LyGKycYT2v0}{\color{blue}{here}}.
			
			The proof is then simply an observation that $\cos(\theta)$ is at most 1 (and at least -1), and so the inequality is satisfied in all cases.	
		\end{proof}
		
	\subsection{Spectral Norm}
	
		Spectral norm is a norm on matrices. Without further ado let's define it:
		
		\begin{definition}[Spectral Norm]
			For any matrix $A\in\mathbb{R}^{m\times d}$ let the spectral norm of A be
			
			\[ \|A\| = \max_{\mathbf{v}\in\mathbb{R}^d, \mathbf{v}\neq 0} \frac{\|A\mathbf{v}\|}{\|\mathbf{v}\|} = \max_{\mathbf{v}\in\mathbb{R}^d, \| \mathbf{v}\|=1} \|A\mathbf{v}\|  \]
		\end{definition}
		
		So, what does this mean - there is some direction that $A$ particularly likes, and vectors aligned with that direction get scaled by a whole lot. The spectral norm is an upper bound on how much the transformation by $A$ can change the length of $\mathbf{v}$. 
		
		Other note is that spectral norm is indeed a norm. What is a norm again?
		
		A norm is some operator that
		
		1. Satisfies the \textit{triangle inequality}. In our case this would be, for two matrices $A\in\mathbb{R}^{m\times n}, B\in\mathbb{R}^{n\times d}$, it must be true that $\|AB\| \le \|A\| + \|B\|$. This is pretty easy to understand in our case - remember that $AB$ can be thought of as sequential matrix multiplication instead of some complicated megamatrix. In the sequential case, the first matrix application can scale the input vector by at most $\|B\|$, and the second transformation by at most $\|A\|$, so the result of the sequential application cannot stretch the input vector by more than $\|A\|+\|B\|$, and We're done.
		
		2. A norm must be \textit{absolutely homogeneous}, which is to say $\|\lambda A\| \le |\lambda|\|A\|$ for some constant $\lambda$ and in our case some matrix $A$. Absolute as in absolute value of $\lambda$. This is obviously true for our spectral norm.
		
		3. Norm is only equal to 0 in case that the input is 0. In our case, $\|A\| = 0\iff A=0$. Again, makes sense, since We are looking at $\max$ in our definition, and for any non-zero matrix $A$ there will be some vector $\mathbf{v}$ such that the transformation will result in a non-zero vector.
		
	\subsection{Mean Value Theorem}
	
		\begin{theorem}[Mean Value Theorem]
			Let $a, b$ be real numbers such that $a<b$, and let $h:[a, b]\to\mathbb{R}$ be a continuous differentiable function on $(a, b)$ and let $h'$ be it's derivative, then there exists $c\in (a, b)$ such that
			
			
			\[ h'(c) = \frac{h(b)-h(a)}{b-a} \]
			
			\textnormal{To see this is true, think about drawing a line between $h(b)$ and $h(a)$. Now drag this line up and down on the cartesian plane. At some point as You are dragging it up and down, it will be \textit{just} tangent to the function $h$. At this point, which We called $c$, the slope of the function $h$ will be equivalent to the slope between the two points $(b, h(b)), (a, h(a))$}.
			
		\end{theorem}
			
	\subsection{Fundamental Theorem of Calculus}
	
		\begin{theorem}[Fundamental Theorem of Calculus]
			Let $a, b$ be real numbers such that $a<b$ and let $h:\mathbf{dom}(h)\to\mathbb{R}$ be a differentiable function in the interval $(a, b)$, let $h'$ be that derivative and let $h'$ be continuous on $[a, b]$, then
			
			\[ h(b) - h(a) = \int^a_b h'(t) dt. \]
			
			\textnormal{Tale as old as time.}
		\end{theorem}
		
	\subsection{Differentiability}
	
		\begin{definition}[Differentiability]
			Let $f:\mathbf{dom}(f)\to\mathbb{R}^m, \mathbf{dom}(f)\subset\mathbb{R}^d$, then $f$ is called \textnormal{differentiable} at $\mathbf{x}$ in the interior of $\mathbf{dom}(f)$ if there exists a matrix $A\in\mathbb{R}^{m\times d}$ and an error function $r:\mathbb{R}^d\to\mathbb{R}^m$ in some neighbourhood of $\mathbf{0}\in\mathbb{R}^d$ such that $\forall\;\mathbf{y}$ in some neighbourhood of $\mathbf{x}$:
			
			\[ f(\mathbf{y}) = f(\mathbf{x}) + A(\mathbf{y-x}) + r(\mathbf{y-x}) \]
			
			where
			
			\[ \lim_{\mathbf{v}\to 0} \frac{\|r(\mathbf{v})\|}{\|\mathbf{v}\|} \]
			
			It then also follows that A is unique and We call A the \textnormal{differential} or \textnormal{Jacobian} of f at $\mathbf{x}$, and more accurately A is a matrix of partial derivatives such that
			
			\[ Df(\mathbf{x})_{i, j} = \frac{\partial f(\mathbf{x})_i}{\partial \mathbf{x}_j} \]
			
			Finally, $f$ is called \textnormal{differentiable} if it is differentiable at all points in it's domain. 
		\end{definition}
		
		So, remarks: the idea is that $f$ is differentiable if it is approximated arbitrarily well by some linear function ($f(\mathbf{x}) + A(\mathbf{y-x})$) and that the error of this approximation is sublinear, i.e. as size of the difference $\mathbf{y-x}$ decreases linearly, the error term $r(\mathbf{y-x})$ decreases even faster.
		
		There are also some notation to be aware of - $\nabla f(\mathbf{x})$ is the gradient vector and it is a\textit{column}, and $A = Df(\mathbf{x}) = \nabla f(\mathbf{x})^\top$, so then We can write stuff like $f(\mathbf{y}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y-x}) + r(\mathbf{y-x})$
		
		\begin{lemma}[Chain Rule]
			Let $f:\mathbf{dom}(f)\to\mathbb{R}^m,\mathbf{dom}(f)\in\mathbb{R}^d$ and  $g:\mathbf{dom}(g)\to\mathbb{R}^d$, $g$ be differentiable at $\mathbf{x}\in\mathbf{dom}(g)$ and $f$ be differentiable at $g(\mathbf{x})$, then
			
			\[Df(g(\mathbf{x})) = D(f\circ g)(\mathbf{x}) = Df(g(\mathbf{x}))Dg(\mathbf{x}) \]
		\end{lemma}
		
		It kind of can't be a lot of other things. Don't think about how the information flows through the matrix - it \textit{does}, but 1 variable of $g$ affects all variables in $f$, and it's kind of a pain. 
		
\section{Convex Sets}

	\begin{definition}[Convex Set]
		A set $C\in\mathbb{R}^d$ is \textnormal{convex} if $\forall\;\mathbf{x, y}\in C$ it is true that $\lambda\mathbf{x} + (1-\lambda)\mathbf{y}\in C, \lambda\in [0,1]$.
	\end{definition}
	
	Not incredible. Idea is that a set is convex if for all two points in that set, the line between those two points is also in the set, so the shape of the set has no dents etc.
	
	\begin{observation}
		Let $C_i, i\in I$ be convex sets and $I$ be a potentially infinite index set, then $C = \cap_{i\in I} C_i $ is a convex set.
	\end{observation}
	
	\begin{proof}
		$\forall\;\mathbf{x, y}\in C$ that must mean that $\mathbf{x, y}\in \cap_{i\in I} C_i$, and by virtue of convexity the line between those points must also be in the intersection.
	\end{proof}
		
	\subsection{Mean Value Inequality}
	
		Is a relaxation of the mean value theorem. Really it's Lipschitz but from the perspective of mean value theorem:
		
		Suppose $f:\mathbf{dom}(f)\to\mathbb{R}$ is differentiable over the open interval $X\subset\mathbf{dom}(f)$, and suppose that $\forall\mathbf{x}\in X \implies |f'(\mathbf{x})| \le B$, then We can say
		
		\[ |f(\mathbf{y}) - f(\mathbf{x})| = |f'(c)(\mathbf{y-x}) \le B|\mathbf{y-x}| \]
		
		Which, recall, $f'(c)$ is just the slope of the the line between $f(\mathbf{y}) - f(\mathbf{x})$, and We know it exists by the Mean Value Theorem.
		
		So, really, (sort of by definition almost really), any function whose derivative is bounded by $B$ is $B$-Lipschitz over that whatever interval $X$. 
		
		So We went from mean theorem to Lipschitz, sort of. Ugh the notes do this:
		
		\[ |f'(c)| \le \lim_{\delta\to 0} \bigg| \frac{f(c+\delta) - f(c)}{\sigma} \bigg| \le B \]
		
		Which, like, yeah, the value of the derivative is bounded in a Lipchitz function. Shocking. I feel like this is a trivial observation but off We go to prove it (ah it's sorta nontrivial since We are not in the univariate case and have to deal with the Spectral norm):
		
		\begin{theorem}
			Let $f:\mathbf{dom}(f)\to\mathbb{R}$ be a continuous differentiable function over some  $X\subseteq\mathbf{dom}(f), B\in\mathbb{R}_+$, where $X$ is open and convex, then the following two statements are equivalent:
			
			\begin{align*}
				i)\;& \| f(\mathbf{y}) - f(\mathbf{x}) \| \le B\cdot \|\mathbf{y-x}\|, \qquad \; \forall\mathbf{x, y}\in X. \\
				ii)\;& |Df(\mathbf{x})\| \le B,\qquad  \forall\mathbf{x}\in X.
			\end{align*}
			
		\end{theorem}
		
		\begin{proof}
			First, let's assume $i)$ and prove $ii)$:
			
			\begin{align}
				\| f(\mathbf{y}) - f(\mathbf{x}) \|& \le B\cdot \|\mathbf{y-x}\| \\
			\end{align}
			
			Recall that We have 
			
			\begin{align*}
				f(\mathbf{y}) &= f(\mathbf{x}) + \nabla f\mathbf{(x)^\top(y-x)} + r(\mathbf{y-x}) \\
				f(\mathbf{y}) -  f(\mathbf{x}) &= \nabla f\mathbf{(x)^\top(y-x)} + r(\mathbf{y-x}) \\
			\end{align*}
			
			Using the above
			\begin{align*}
				\|f(\mathbf{y}) - f(\mathbf{x})|& \le B\cdot \|\mathbf{y-x}\| \\
				\| \nabla f\mathbf{(x)^\top(y-x)} + r(\mathbf{y-x}) \| &\le B\cdot \|\mathbf{y-x}\| \\
				\| \nabla f\mathbf{(x)^\top(x+v-x)} + r(\mathbf{x+v-x}) \| & \le B\cdot \| \mathbf{x+v-x} \|, \; (\mathbf{y = x+v}) \\
				\| \nabla f\mathbf{(x)^\top(v)} + r(\mathbf{v}) \| & \le B\cdot \| \mathbf{v} \|  \\
				\| \nabla f\mathbf{(x)^\top(v)} + r(\mathbf{v}) \| \le \| \nabla f\mathbf{(x)^\top(v)} \| + \| r(\mathbf{v})  \|  &\le B\cdot \| \mathbf{v} \|  \\
				\| \nabla f\mathbf{(x)^\top(v)} \| + \| r(\mathbf{v})  \|  &\le B\cdot \| \mathbf{v} \|  \\
				\frac{\| \nabla f\mathbf{(x)^\top(v)} \|}{\| \mathbf{v} \|} + \frac{\| r(\mathbf{v})  \|}{\| \mathbf{v} \|}  &\le \frac{B\cdot \| \mathbf{v} \|}{\| \mathbf{v} \|}  \\				
				\frac{\| \nabla f\mathbf{(x)^\top(v)} \|}{\| \mathbf{v} \|}  &\le B - \frac{\| r(\mathbf{v})  \|}{\| \mathbf{v} \|}  
			\end{align*}
			
			And now comes some shenanigans. Let $\mathbf{v} = t\cdot\mathbf{v}^*$ such that $\mathbf{v^*}$ is a unit vector and 
			
			\[ \frac{\| \nabla f\mathbf{(x)^\top(v^*)} \|}{\| \mathbf{v^*} \|} = \| \nabla f\mathbf{(x)} \| \]
			
			Then We just do 
			
			\begin{align*}
				\frac{\| \nabla f\mathbf{(x)^\top(v)} \|}{\| \mathbf{v} \|}  &\le B - \frac{\| r(\mathbf{v})  \|}{\| \mathbf{v} \|}  \\
				\frac{\| \nabla f\mathbf{(x)} ^\top t\mathbf{(v^*)} \|}{\| t  \mathbf{v^*} \|}  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
				\frac{ |t| \| \nabla f\mathbf{(x)^\top(v^*)} \|}{ |t| \| \mathbf{v^*} \|}  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
				\frac{  \| \nabla f\mathbf{(x)^\top(v^*)} \|}{  \| \mathbf{v^*} \|}  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
				 \| \nabla f\mathbf{(x)} \|  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
			\end{align*}
			
			and as $t\mathbf{v^*}\to\mathbf{0}$, We get our result.
			
			Now for  the other direction, let's create the following function:
			
			\[ g(t) = \mathbf{f(x) + t(y-x)} \]
			
			This simply gives us smooth means of traversal through the function, which will allow us to use the bounded differentials. We then define $h(t) = f(g(t))$, so We are just wrapping our traversal function with, well, the function of interest $f$. This allows us to say
			
			\begin{align*}
				\|f(\mathbf{y}) - f\mathbf{x})\| &= \| h(1) - h(0) \| \\
				&= \bigg\| \int^1_{t=0} h'(t) dt \bigg\| \\
				&= \bigg\| \int^1_{t=0} D(\mathbf{x} + t(\mathbf{y-x}))(\mathbf{y-x}) dt \bigg\| \\
				&\le  \int^1_{t=0}  \| D(\mathbf{x} + t(\mathbf{y-x}))(\mathbf{y-x}) \| dt\quad\text{Jensen's Inequality} \\
				&\le  \int^1_{t=0}  \| D(\mathbf{x} + t(\mathbf{y-x})) \| \| (\mathbf{y-x}) \| dt\quad\text{Spectral Norm} \\
				&\le  \int^1_{t=0}  B \| (\mathbf{y-x}) \| dt\quad\text{Bounded differentials} \\
				&\le   B \| (\mathbf{y-x}) \| 
			\end{align*}
		\end{proof}
		
\section{Convex Functions}

	\begin{definition}[Convex Function]
		A function $f:\mathbf{dom}(f)\to\mathbb{R}$ is a \textnormal{convex function} if i) $\mathbf{dom}(f)$ is a convex set and ii) if $\forall\;\mathbf{x, y}\in\mathbf{dom}(f)$ it is true that
		
		\[ f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})  \]
		
		for $\lambda\in [0, 1]$.
	\end{definition}
	
	A useful concept to go along with the convex function is the epigraph:
	
	\[ \mathbf{epi}(f) = \{ (\mathbf{x}, \alpha) \in\mathbb{R}^{d+1}: \mathbf{x}\in\mathbf{dom}(f), \alpha \ge f(\mathbf{x})\}. \]
		
	So just the set of all points above the graph of the function $f$.
	
	\begin{observation}
		Function $f:\mathbf{dom}(f)\to\mathbb{R}$ is convex if and only if $\mathbf{epi}(f)$ is a convex set.
	\end{observation}
	
	\begin{proof}
		First direction, We can assume convex $f$:
		
		\begin{align*}
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})
		\end{align*}
		
		We need to prove:
		
		\begin{align*}
			\forall (\mathbf{x}, \alpha_x),\; (\mathbf{y}, \alpha_y)\in\mathbf{epi}(f): \lambda(\mathbf{x}, \alpha_x)+(1-\lambda)(\mathbf{y}, \alpha_y) \in\mathbf{epi}(f)
		\end{align*}
		
		So We have the point $(\underbrace{\lambda\mathbf{x} + (1-\lambda)\mathbf{y}}_q, \underbrace{\lambda\alpha_x + (1-\lambda)\alpha_y}_w)$. Now We just need to assert that $q\in\mathbf{dom}(f), w\ge f(q)$ to satisfy the epigraph conditions. We know that $q\in\mathbf{dom}(f)$ by convexity of the domain of $f$, and for the second bit
		
		\begin{align*}
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda \alpha_x + (1-\lambda)\alpha_y\\
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})
		\end{align*}
		
		Which is true by convexity of $f$. Wee.
		
		Now for the other side - We can assume that $\mathbf{epi}(f)$ is a convex set, and We wish to prove that $f$ is convex, so We want
		
		\begin{align*}
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})
		\end{align*}
		
		This in words is quite trivial - take the two endpoints, they by definition are in the epigraph, then consider all points in between and observe that since they too are in the epigraph since the epigraph is convex they must be lie above the graph, concluding the proof.
	\end{proof}
	
	\begin{lemma}[Jensen's Inequality]
		For a convex function $f:\mathbf{dom}(f)\to\mathbb{R}$ and a random variable $X$ such that $X$ has a mean and $support(X)\in\mathbf{dom}(f)$ it is true that
		
		\[ E(f(X)) \le f(E(X))\]
	\end{lemma}
	
	\begin{proof}
		Let $\mu = E(X)$, and construct a tangent $a+bX$ to the convex function $X$ at $\mu$ such that $a + b\mu = f(\mu)$. By convexity of $f$ We have
		
		\begin{align*}
			f(X) &\ge a+bX\\
			E(f(X) &\ge E(a+bX)\\
			E(f(X) &\ge a+bE(X)\\
			E(f(X) &\ge a+b\mu\\
			E(f(X)) &\ge f(\mu)\\
			E(f(X)) &\ge f(E(X))
		\end{align*}
	\end{proof}
	
	\subsection{First Order Characterization of Convexity}

		\begin{lemma}
			Suppose $f$ is differentiable, then $f$ is also convex if and only if $\forall\mathbf{x, y}\in\mathbf{dom}(f)$
			
			\[ f(\mathbf{y}) \ge f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y-x})  \]
		\end{lemma}
		
		And intuitively this just means that the graph of $f$ is above the tangent to $f$ at any point.
		
		\begin{proof}
			First direction relies on rearranging convexity:
			
			\begin{align*}
				f(\lambda \mathbf{x} + (1-\lambda)(\mathbf{y})) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y}) \\
				f(\lambda \mathbf{x} + \mathbf{y}-\lambda\mathbf{y}) \le \lambda f(\mathbf{x}) + f(\mathbf{y}) -\lambda f(\mathbf{y}) \\
				f( \mathbf{y} + \lambda(\mathbf{x-y})) \le f(\mathbf{y}) + \lambda (f(\mathbf{x}) - f(\mathbf{y})) \\
				f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y}) \le  \lambda (f(\mathbf{x}) - f(\mathbf{y})) \\
				f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y}) \le  \lambda (f(\mathbf{x}) - f(\mathbf{y})) \\
				\frac{f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y})}{\lambda} \le f(\mathbf{x}) - f(\mathbf{y}) \\
				\frac{f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y})}{\lambda} + f(\mathbf{y}) \le f(\mathbf{x})  \\
				\frac{\nabla f(\mathbf{x})^\top \lambda(\mathbf{x-y}) + r(\lambda(\mathbf{x-y})) }{\lambda} + f(\mathbf{y}) \le f(\mathbf{x}) \\
				\frac{\nabla f(\mathbf{x})^\top \lambda(\mathbf{x-y})}{\lambda} + \frac{r(\lambda(\mathbf{x-y})) }{\lambda} + f(\mathbf{y}) \le f(\mathbf{x}) \\
				{\nabla f(\mathbf{x})^\top (\mathbf{x-y})} + f(\mathbf{y}) \le f(\mathbf{x})\quad\lim_{t\to 0}
			\end{align*}
			
			So two pieces to this one - reframing convexity as having one "fixed" point (in this case $\mathbf{y}$, and observing that 
			
			\[ f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y}) = \nabla f(\mathbf{y})^\top \lambda(\mathbf{x-y}) + r(\lambda(\mathbf{x-y}))  \]
			
			Since We are calculating the difference from $f(\mathbf{y})$ when our intput is shifted by $\lambda(\mathbf{x-y})$.
		\end{proof}
		
		\begin{proof}
			Now for the other direction, i.e. first order characterization implies convexity.
			
			First We establish some point $\mathbf{z}$ in between $\mathbf{x, y}$:
			
			\[ \mathbf{z} = \lambda \mathbf{x} + (1-\lambda)\mathbf{y} \]
			
			By first order characterization We then have
			
			\begin{align*}
				f(\mathbf{x}) &\ge f(\mathbf{z}) + \nabla f(\mathbf{z})^\top (\mathbf{x-z})\\
				f(\mathbf{y}) &\ge f(\mathbf{z}) + \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
			\end{align*}
			
			Then We start constructing our conclusion by multiplying the first line by $\lambda$ and the second by $(1-\lambda)$:
			
			\begin{align*}
				\lambda f(\mathbf{x}) &\ge \lambda f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z})\\
				(1-\lambda) f(\mathbf{y}) &\ge (1-\lambda) f(\mathbf{z}) + (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
			\end{align*}
			
			And then We add those two and simplify
			
			\begin{align*}
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				\lambda f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z}) + &(1-\lambda) f(\mathbf{z}) + (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				\lambda f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z}) + & f(\mathbf{z}) -\lambda f(\mathbf{z}) + (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z}) &+ (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				f(\mathbf{z}) +  \nabla f(\mathbf{z})^\top \bigg( \lambda (\mathbf{x-z}) &+ (1-\lambda)(\mathbf{y-z})\bigg)
			\end{align*}
			
			Now We just focus on those gradient terms:
			
			\begin{align*}
				 \lambda (\mathbf{x-z}) + (1-\lambda)(\mathbf{y-z}) &= \lambda\mathbf{x} - \lambda\mathbf{z} + \mathbf{y-z} - \lambda\mathbf{y}+\lambda\mathbf{z}\\
				&= \lambda\mathbf{x} + (-\lambda - 1 + \lambda)\mathbf{z} + \mathbf{y} - \lambda\mathbf{y}\\
				&= \lambda\mathbf{x} + (1-\lambda)\mathbf{y} -\mathbf{z} \\
				&= 0
			\end{align*}
			
			So there's really one bit to to this proof - take the tangent at the middle point $\mathbf{z}$ and compare it with the endpoints $f(\mathbf{x}), f(\mathbf{y})$, and from there it's addition and simplification.
		\end{proof}
		
		Only really useful if You have a derivative. I think in later chapters this is generalized with subgradients. Furthermore there are obviously convex functions like the absolute value operator that are not differentiable.
		
		\begin{lemma}
			An alternative first-order characterization of convexity is:
			
			\[ \nabla f(\mathbf{y}) - \nabla f(\mathbf{x}))^\top (\mathbf{y-x}) \ge 0 \]
		\end{lemma}
		
		A loose intuition for this is that whatever direction You go in $(\mathbf{y-x}$, the gradient will also point in that same direction.
		
		\begin{proof}
			By convexity We have these two statements
			
			\begin{align*}
				f(\mathbf{y}) \ge f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y-x})\\
				f(\mathbf{x}) \ge f(\mathbf{y}) + \nabla f(\mathbf{y})^\top (\mathbf{x-y})
			\end{align*}
			
			Adding those two and cancelling out $f(\mathbf{y}), f(\mathbf{x})$ gives
			
			\begin{align*}
				 0 &\ge \nabla f(\mathbf{x})^\top (\mathbf{y-x}) + \nabla f(\mathbf{y})^\top (\mathbf{x-y})\\
				 0 &\ge -\nabla f(\mathbf{x})^\top (\mathbf{x-y}) + \nabla f(\mathbf{y})^\top (\mathbf{x-y})\\
				 0 &\ge (\mathbf{x-y})( \nabla f(\mathbf{y}) -\nabla f(\mathbf{x}))^\top\\
				 0 &\le (\mathbf{y-x})( \nabla f(\mathbf{y}) -\nabla f(\mathbf{x}))^\top
			\end{align*}
			
			Which concludes this direction. All We needed to do was "look" at each point from the direction of the other point and combine the perspectives.
			
			Now for the other direction, We have
			
			\[ \nabla f(\mathbf{y}) - \nabla f(\mathbf{x}))^\top (\mathbf{y-x}) \ge 0 \]
			
			Let $\mathbf{y} = \mathbf{x} + t(\mathbf{y-x})$ to yield (note that the above holds for $t\in [0, 1]$)
			
			\begin{align*}
				\nabla f(\mathbf{x} + t(\mathbf{y-x})) - \nabla f(\mathbf{x}))^\top (\mathbf{x} + t(\mathbf{y-x}) - \mathbf{x}) &\ge 0 \\
				\nabla f(\mathbf{x} + t(\mathbf{y-x})) - \nabla f(\mathbf{x}))^\top t(\mathbf{y-x}) &\ge 0 \\
				\nabla f(\mathbf{x} + t(\mathbf{y-x})) - \nabla f(\mathbf{x}))^\top (\mathbf{y-x}) &\ge 0 \\
				\nabla f(\mathbf{x} + t(\mathbf{y-x}))^\top(\mathbf{y-x}) - \nabla f(\mathbf{x})^\top (\mathbf{y-x}) &\ge 0 \\
				\nabla f(\mathbf{x} + t(\mathbf{y-x}))^\top (\mathbf{y-x})  &\ge  \nabla f(\mathbf{x})^\top (\mathbf{y-x}) \\
			\end{align*}
			
			Now We define 
			
			\[ h(t) = f(\mathbf{x} + t(\mathbf{y-x})) \]
			
			and therefore 
			
			\[ h'(t) = \nabla  f(\mathbf{x} + t(\mathbf{y-x}))^\top(\mathbf{y-x}) \]
			
			Which We can use to rewrite
			
			\begin{align*}
				\nabla f(\mathbf{x} + t(\mathbf{y-x}))\top (\mathbf{y-x})  &\ge  \nabla f(\mathbf{x})^\top (\mathbf{y-x}) \\
				h'(t)  &\ge  \nabla f(\mathbf{x})^\top (\mathbf{y-x}) 
			\end{align*}
			
			And this still holds for all $t\in [0, 1]$.
			
			And now comes just random stuff plucked from the air: observe that by the Mean Value Theorem, it is true that there exists a $c$ such that $h'(c) = h(1) - h(0)$.
			
			\begin{align*}
				f(\mathbf{y}) = h(1) = h(0) + h(1) - h(0) = h(0) + h'(c)\\
				f(\mathbf{y}) \ge h(0) +\nabla f(\mathbf{x})^\top (\mathbf{y-x}) \\
				f(\mathbf{y}) \ge f(\mathbf{x}) +\nabla f(\mathbf{x})^\top (\mathbf{y-x}) \\
			\end{align*}
			
			Honestly no fun and seemingly unmotivated proof. 
		\end{proof}
		
	\subsection{Second-order characterization of convexity}
	
		\begin{lemma}
			If $f$ is twice differentiable on it's domain, namely
			
			\[ \nabla^2 f(\mathbf{x})_{i. j} =  \ \frac{\partial^2 f}{\partial \mathbf{x}_i \partial \mathbf{x}_j}  \]
			
			exists and $\nabla^2 f(\mathbf{x})$ is positive definite (or positive semidefinite), then $f$ is convex.
		\end{lemma}
		
		\begin{proof}
			Is omitted, at least for now.
		\end{proof}
		
	\subsection{Operations that preserve convexity}
	
		\begin{lemma}
			i) Multiplying by a positive constant, addition of convex functions while taking the intersection of their domains preserves convexity.
			
			ii) Let g be a linear function and f be a convex function, then $f\circ g$ is also convex.
		\end{lemma}

\section{Minimizing Convex Functions}

	\begin{definition}
		A function $f:\mathbf{dom}(f)\to\mathbb{R}$ has a local minimum $\mathbf{x}\in\mathbf{dom}(f)$ if there exists $\varepsilon >0$ such that
		
		\[ f(\mathbf{y}) \ge f(\mathbf{x})\qquad \forall\mathbf{y}\in\mathbf{dom}(f),\; \|\mathbf{y-x}\| < \varepsilon   \]
	\end{definition}
	
	In this case thing of $\varepsilon$ as a sort of radius for the local minimum. All points in the domain within that radius are greater than $f(\mathbf{x})$, so We have a neat little local dent (or plateau).
	
	\begin{lemma}
		Let $\mathbf{x^*}$ be a local minimum to a convex function $f:\mathbf{dom}(f)\to\mathbb{R}$, then       $\mathbf{x^*}$ is a global minimum meaning
		
		\[ f(\mathbf{x^*}) \le f(\mathbf{y})\quad  \forall \mathbf{y}\in\mathbf{dom}(f).\]
	\end{lemma}
	
	\begin{proof}
		Is simple - let $\mathbf{y}\in\mathbf{dom}(f)$ be such that $f(\mathbf{y}) < f(\mathbf{x^*})$. Let $\mathbf{y'} = \lambda\mathbf{y} + (1-\lambda)\mathbf{x^*}$. Then for $\lambda\in (0, 1)$ by convexity We have $f(\mathbf{y'}) < f(\mathbf{x^*})$. Choosing $\lambda$ small enough to violate $\|\mathbf{y' - x^*}\| < \varepsilon$ contradicts local minima assumption of $\mathbf{x^*}$.
	\end{proof}
	
	Then there's a bunch of caveats. Convex functions need not have a global minimum - $f(x) = x$ etc. Then even if it is bounded from below, asymptotic functions won't have a minimum e.g. $f(x) = e^x$.
	
	\begin{lemma}
		Suppose $f:\mathbf{dom}(f)\to\mathbb{R}$ is convex and differentiable, and let $\mathbf{x}\in\mathbf{dom}(f)$. If $\nabla f(\mathbf{x}) = \mathbf{0}$, then $\mathbf{x}$ is a global minimum.
	\end{lemma}
	
	\begin{proof}
		Assuming $\nabla f(\mathbf{x}) = \mathbf{0}$, with first order characterization We have
		
		\begin{align*}
			f(\mathbf{y}) &\ge f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y-x})\\
			&\ge f(\mathbf{x}) + \mathbf{0}\\
			&\ge f(\mathbf{x})
		\end{align*}
	\end{proof}
	
	\begin{lemma}
		Suppose $f$ is convex and differentiable and let $\mathbf{x}\in\mathbf{dom}(f)$ be a global minimum, then it is true that $\nabla f(\mathbf{x}) = \mathbf{0}$.
	\end{lemma}
	
	\subsection{Strictly Convex Functions}
	
		\begin{definition}
			A function $f:\mathbf{dom}\to\mathbb{R}$ is strictly convex if it is true $\forall\; \mathbf{x\neq y}\in\mathbf{dom}(f),\; \lambda\in (0,1)$ that
			
			\[ f(\lambda(\mathbf{x}) + (1-\lambda)\mathbf{y}) < \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) \]
		\end{definition}
		
		\begin{lemma}
			The second derivative matrix a.k.a. Hessian of a strictly convex function is positive definite (if it exists).
		\end{lemma}
		
		\begin{lemma}
			Suppose $f$ is strictly convex, then $f$ has a unique global minimum.
		\end{lemma}
		
		\begin{proof}
			Let $f:\mathbf{dom}(f)\to\mathbb{R}$ be a strictly convex function. Suppose there are two global minima $\mathbf{x\neq y}\in\mathbf{dom}(f)$. Then by taking a linear combination of those two minima, by strict convexity, We obtain an even lower point, violating the assumption that $\mathbf{x, y}$ were minima.
		\end{proof}
		
	\subsection{Constrained Optimization}
	
		\begin{definition}
			Let $f:\mathbf{x}\to\mathbb{R}$ be some function, $X\subseteq\mathbf{dom}(f)$ and $\mathbf{x}\in X$. If
			
			\[ f(\mathbf{y} \ge f(\mathbf{x})\quad\forall\mathbf{y}\in X \]
			
			Then $\mathbf{x}$ is a minimizer of $f$ over $X$.
		\end{definition}
		
		\begin{lemma}
			Let $f:\mathbf{dom}(f)\to\mathbb{R}$ be a convex differentiable function, $X\subseteq\mathbf{dom}(f)$, then $\mathbf{x^*}\in X$ is a minimizer of $f$ over $X$ if and only if
			
			\[ \nabla f(\mathbf{x^*})^\top (\mathbf{x - x^*}) \ge 0\quad \forall\mathbf{x}\in X. \]
		\end{lemma}
		
		Basically no direction exists such that the gradient is negative from our optimal point. Makes sense - if the above was not true, We could take a step against the direction of the gradient and obtain a smaller minimum.
		
\section{Existence of a minimizer}

	We care about this since most algorithms assume the existence of a minimizer. Here are some ways of going about proving such a minimizer exists:
	
	\subsection{Sublevel sets and Weierstrass Theorem}
	
		\begin{definition}
			Let $f:\mathbb{R}^d\to\mathbb{R}, \alpha\in\mathbb{R}$, the set
			
			\[ f^{\le\alpha} = \{\mathbf{x}\in\mathbb{R}^d : f(\mathbf{x} \le \alpha \}  \]
			
			is called the $\alpha$-sublevel set of $f$.
		\end{definition}
		
		So just the closed (due to the $\le$ condition) subset of the domain of $f$. Cool.
		
		\begin{theorem}
			Suppose $f:\mathbb{R}^d\to\mathbb{R}$ is a convex function with a bounded nonempty sublevel set $f^{\le\alpha}$, then $f$ has a global minimum.
		\end{theorem}
		
		\begin{proof}
			We know that the sublevel set is continuous since it is a subset of the convex domain of $f$. Furthermore We know it is bounded, so somewhere along that continuous line a minimum is achieved, and since all points outside of the sublevel set are larger, We've got a global minimum.
		\end{proof}
		
	\subsection{Recession cone and lineality space}
	
		The idea is that there is probably some "direction of recession" in our convex set $C$, and that is the reason the sublevel set is not bounded. Think of the case $f(x) = x$ - the line with slope -1 is a direction of recession. Formally:
		
		\begin{definition}
			Let $C$ be a convex set, then $\mathbf{y}\in \mathbf{R}^d$ is a direction of recession if for some $\mathbf{x}\in C$ and $\lambda \in \mathbb{R}_+ $ it holds that $\mathbf{x}+\lambda\mathbf{y}\in C$.
		\end{definition}
		
		Basically the set extends infinitely in that direction.
		
		\begin{lemma}
			For a closed convex set $C$ with $\mathbf{y}\in C$, then the following statements are equivalent:
			
			\begin{align*}
				i)&\; \exists\mathbf{x}\in C: \mathbf{x} + \lambda\mathbf{y} \in C,\;\forall\lambda\ge 0\\
				ii)&\; \forall\mathbf{x}\in C: \mathbf{x} + \lambda\mathbf{y} \in C,\;\forall\lambda\ge 0
			\end{align*}
		\end{lemma}
		
		\begin{proof}
			We only need to prove $i)\to ii)$. First assume 
			
			\[ \exists\mathbf{x}\in C: \mathbf{x} + \lambda\mathbf{y} \in C,\;\forall\lambda\ge 0\\ \]
			
			Now define $w_k = \mathbf{x} + k\lambda\mathbf{y}, k\in\mathbb{N}$. $w_k\in C$ by our assumption. 
			
			Now, We need to show that $\mathbf{x'}+\lambda\mathbf{y}\in C$ for any $\mathbf{x'}$. To this end suppose We have
			
			\begin{align*}
				\frac{1}{k}w_k + (1-\frac{1}{k})\mathbf{x'}\\
			\end{align*}
			
			And then let $k$ go to infinity.
			
			So what did We do? We took the direction of recession, stretched it out infinitely by some factor $k$, and then formulated a line between the infinite point and any other point $\mathbf{x'}$ using the same factor We used to stretch out the line. This way, when taking limits, the factors cancel and convexity gives us membership in the closed set.
		\end{proof}
		
		\begin{lemma}
			Let $C$ be a closed convex set and let $\mathbf{y}_1, \mathbf{y}_2\in C$ be directions of recession of $C$, then $\mathbf{y} = \lambda_1\mathbf{y}_1 + \lambda_2\mathbf{y}_2,\;lambda_1\lambda_2\in\mathbb{R}_+$ is also a direction of recession in $C$.
		\end{lemma}
		
		\begin{proof}
			Scale both $\mathbf{y}$ by $1/(\lambda_1 + \lambda_2)$, which allows us to assume without loss of generality that $\lambda_1 + \lambda_2 = 1$. Anyway, now the idea is to simply
			
			\begin{align*}
				\mathbf{x} + \lambda\mathbf{y} = \mathbf{x} + \lambda (\lambda_1\mathbf{y}_1 + \lambda_2\mathbf{y}_2) &= \mathbf{x} + \lambda \lambda_1\mathbf{y}_1 +  \lambda\lambda_2\mathbf{y}_2\\
				&=\lambda_1\mathbf{x} + \lambda_2\mathbf{x} + \lambda \lambda_1\mathbf{y}_1 +  \lambda\lambda_2\mathbf{y}_2\\
				&=\lambda_1(\mathbf{x} + \lambda\mathbf{y}_1) + \lambda_2(\mathbf{x}  +  \lambda\mathbf{y}_2)
			\end{align*}
			
			Since both endpoints are in the set, We are done. So the idea there was to split the new direction of recession into it's original components and since the result will be between  the two original directions, by convexity the new point is also in $C$. The scaling thing is there to make it neat.

		\end{proof}
		
		\begin{definition}
			Let $C\subseteq{R}^d$ be a closed convex set, then $\mathbf{y}\in C$ is a \textnormal{direction of constancy} if both $\mathbf{y, -y}$ are directions of recession.
		\end{definition}
		
		So, a direction of constancy has with it associated a linear subspace called \textit{lineality space L(C)}. The set shape here is something like cylinder for example. Anyway, lineality spaces are sort of straight lines, where as recession stuff is more like a cone, since it only deals with one direction (especially in convex cases, You're getting a cone). Remember that We are still just dealing with sets, though.
		
		\begin{lemma}
			Let $C\subseteq\mathbb{R}^d$ be a closed convex set and $\mathbf{y}_1, \mathbf{y}_2$ be directions of constancy, then $\mathbf{y} = \lambda_1\mathbf{y}_1 + \lambda_2\mathbf{y}_2, \lambda_1, \lambda_2\in\mathbb{R}$ is also a direction of constancy.
		\end{lemma}
		
		\begin{proof}
			Simply use above lemma and deal with positive and negative directions one at a time.
		\end{proof}
		
		So how do We use this stuff for functions? Well, We have our sublevel sets.  If the sublevel sets are well behaved, then We have a global minimum. 
		
		\begin{lemma}
			Let $f:\mathbb{R}^d\to\mathbb{R}$ be a convex function, then any two non-empty sublevel sets $f^{\le\alpha},f^{\le\alpha'}, \alpha\neq\alpha'$ share the same directions of recession.
		\end{lemma}
		
		\begin{proof}
			We first assume that a sublevel set $f^{\le\alpha}$ has a direction of recession $\mathbf{y}$ giving us
			
			\[ f(\mathbf{x} + \lambda\mathbf{y}) \le \alpha \]
			
			The claim is that We can actually state a stronger version of the above:
			
			\[ f(\mathbf{x} + \lambda\mathbf{y}) \le f(\mathbf{x}) \]
			
			To prove this We let $\mathbf{z} = \lambda\mathbf{y}$, $\mathbf{w}_k = \mathbf{x}+k\lambda\mathbf{y}\in f^{\le\alpha}$ We get
			
			\begin{align*}
				\mathbf{x}+\mathbf{z} = \bigg(1 - \frac{1}{k} \bigg)\mathbf{x} + \frac{1}{k}\mathbf{w}_k
			\end{align*}
			
			Which We can then use to say
			
			\begin{align*}
				f(\mathbf{x+z})  =  f\bigg(  \bigg(1 - \frac{1}{k} \bigg)\mathbf{x} + \frac{1}{k}\mathbf{w}_k \bigg) &\le \bigg( 1-\frac{1}{k} \bigg)f(\mathbf{x}) + \frac{1}{k}f(\mathbf{w}_k)\\
				&\le \bigg( 1-\frac{1}{k} \bigg)f(\mathbf{x}) + \frac{1}{k}f(\mathbf{w}_k)\\
				&\le \bigg( 1-\frac{1}{k} \bigg)f(\mathbf{x}) + \frac{1}{k}\alpha\\
				&\le f(\mathbf{x}) \quad \bigg( \lim_{k\to\inf}\bigg)
			\end{align*}
			
			The trick with directions of recession seems to be to allow an endpoint extend to infinity, and this allows for terms to vanish. In this case, We chose the endpoints such that We could use our definition of convexity. 
			
			Anywho, once We have the non-increasing term, it's simple. Since they non-empty sublevel sets, they must have some point in their intersection $\mathbf{x'}$, and by our non-increasing fact We can say $f(\mathbf{x'}+\lambda\mathbf{y})\le f(\mathbf{x'})$, and so it is in any sublevel set We'd like.
		\end{proof}
		
		\begin{definition}
			R(f) is the set of directions of recession, L(f) is the set of $f$.
		\end{definition}
		
		Having all of the above, We can make the following statement:
		
		\begin{lemma}
			Let $f\mathbb{R}^d\to\mathbb{R}$ be a convex function, then the following are equivalent:
			
			i) $\mathbf{y}\in\mathbb{R}^d$ is a direction of recession of f.
			
			ii) $f(\mathbf{x}+\lambda\mathbf{y})\le f(\mathbf{x}),\forall\mathbf{x}\in\mathbb{R}^d, \lambda\ge 0$.
			
			iii) $(\mathbf{y}, 0)\in\mathbb{R}^{d+1}$ is a "horizontal" direction of recession of the convex set $\mathbf{epi}(f)$.
		\end{lemma}
		
		The first two are clear.
		
		To understand the third one, first think of the epigraph. In our usual case of $f$ having a single output dimension, We got our graph on the cartesian plane, and the epigraph was everything above the graph. No big deal.
		
		Now, observe that $f$ in this case was one dimensional, and the epigraph is two dimensional since We need to add another dimension to the points to account for the output of the function $f$.
		
		Now think about directions \textit{in the epigraph}. We have a direction in the input space - $\mathbf{y}$. No worries there. In our 2D example, that's either left or right lol. By creating the vector $(\mathbf{y}, 0)\in\mathbb{R}^{d+1}$, We are saying that as You move along $\mathbf{y}$ in the input space, the output value does not change. In the epigraph, it is "horizontal". 
		
	\subsection{Coercive functions}
	
		Aren't very interesting in that they are too nice:
		
		\begin{definition}
			A $f\mathbb{R}^d\to\mathbb{R}$ convex function is \textnormal{coercive} if it's recession cone is trivial, i.e. $R(f) = \mathbf{0}$.
		\end{definition}
		
		So no matter which way You go, the function is increasing. 
		
		\begin{lemma}
			Let $f:\mathbb{R}^d$ be a coercive function, then every nonempty sublevel set $f^{\le\alpha}$ is bounded.
		\end{lemma}
		
		\begin{proof}
			The proof is a nightmare so We are just going to skip it. Probably not a nightmare if You've done real analysis though.
		\end{proof}
			
			The point is that We claim that coercive functions have bounded sublevel sets, and We know that bounded sublevel sets obviously have a global minimum, so:
		
		\begin{theorem}
			Let $f:\mathbb{R}^d\to\mathbb{R}$ be a coercive function, then $f$ has a global minimum.
		\end{theorem}
		
	\subsection{Weakly Coercive Functions}
	
		Are simply functions such that their recession cone and lineality space are the same set. We then come back to having coercive functions by taking the compliment of the lineality space of a weakly coercive function $f$.
		
		
		
		
		
		


\end{document} 