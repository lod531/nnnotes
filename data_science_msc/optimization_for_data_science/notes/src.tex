\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{chngcntr}
\usepackage{float}
\usepackage{tabu}
\usepackage{bm}
\usepackage[lite]{amsrefs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}\usepackage{xcolor}
%\graphicspath{ {./img/} }


\counterwithin*{equation}{section}

\newcommand{\R}{\mathbb{R}}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{1}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}
    
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}


\begin{document}


\title{Optimization for Data Science}
\author{Bernd G$\ddot{\text{a}}$rtner, Martin Jaggi}
\date{}

\maketitle

\section{Theory of Convex Functions}

	\subsection{Mathematical Notation}
	
		Bold lower case symbols are used for vectors, and normal font is used for their coordinates e.g. $\mathbf{x} = (x_1, x_2\ldots x_d)\in \mathbb{R}^d$. Vectors are assumed to be column vectors unless transposed, so $\mathbf{x}$ is a column and $\mathbf{x}^\top$ is a row. Revolutionary. $\mathbf{x^\top y} = \sum^d_{i=1} x_i y_i$ for $\mathbf{x, y}\in\mathbb{R}^d$, i.e. the damn dot product.
		
		$\|\mathbf{x}\|$ is the Euclidian norm i.e. the length of a vector and the squared Euclidian norm is $\|\mathbf{x}\|^2 = \mathbf{x^\top x}$.
		
		Conventions here include that $\mathbb{N} = \{1, 2,\ldots \}$, so natural numbers do not include 0 and $\mathbb{R}_+ = \{x\in\mathbb{R}: x\ge 0 \}$, so the positive real numbers do include 0.
		
	\subsection{Cauchy-Schwarz Inequality}

		\begin{lemma}[Cauchy-Schwarz Inequality]
			Let $\mathbf{u, v}\in\mathbb{R}^d$, then
			
			\[ |\mathbf{u^\top v| \le \|u\|\|v\|}. \]
		\end{lemma}

		\begin{proof}
			The core of the dot product, to me anyhow, is that given that $\theta$ is the acute angle between vector $\mathbf{u, v}$, We have:
			
			\[ \|\mathbf{u^\top v \| = \|u\|\|v\|}\cos(\theta) \]
			
			Thankfully, We can assume the above. For an intuition for the above fact, see \href{https://www.youtube.com/watch?v=LyGKycYT2v0}{\color{blue}{here}}.
			
			The proof is then simply an observation that $\cos(\theta)$ is at most 1 (and at least -1), and so the inequality is satisfied in all cases.	
		\end{proof}
		
	\subsection{Spectral Norm}
	
		Spectral norm is a norm on matrices. Without further ado let's define it:
		
		\begin{definition}[Spectral Norm]
			For any matrix $A\in\mathbb{R}^{m\times d}$ let the spectral norm of A be
			
			\[ \|A\| = \max_{\mathbf{v}\in\mathbb{R}^d, \mathbf{v}\neq 0} \frac{\|A\mathbf{v}\|}{\|\mathbf{v}\|} = \max_{\mathbf{v}\in\mathbb{R}^d, \| \mathbf{v}\|=1} \|A\mathbf{v}\|  \]
		\end{definition}
		
		So, what does this mean - there is some direction that $A$ particularly likes, and vectors aligned with that direction get scaled by a whole lot. The spectral norm is an upper bound on how much the transformation by $A$ can change the length of $\mathbf{v}$. 
		
		Other note is that spectral norm is indeed a norm. What is a norm again?
		
		A norm is some operator that
		
		1. Satisfies the \textit{triangle inequality}. In our case this would be, for two matrices $A\in\mathbb{R}^{m\times n}, B\in\mathbb{R}^{n\times d}$, it must be true that $\|AB\| \le \|A\| + \|B\|$. This is pretty easy to understand in our case - remember that $AB$ can be thought of as sequential matrix multiplication instead of some complicated megamatrix. In the sequential case, the first matrix application can scale the input vector by at most $\|B\|$, and the second transformation by at most $\|A\|$, so the result of the sequential application cannot stretch the input vector by more than $\|A\|+\|B\|$, and We're done.
		
		2. A norm must be \textit{absolutely homogeneous}, which is to say $\|\lambda A\| \le |\lambda|\|A\|$ for some constant $\lambda$ and in our case some matrix $A$. Absolute as in absolute value of $\lambda$. This is obviously true for our spectral norm.
		
		3. Norm is only equal to 0 in case that the input is 0. In our case, $\|A\| = 0\iff A=0$. Again, makes sense, since We are looking at $\max$ in our definition, and for any non-zero matrix $A$ there will be some vector $\mathbf{v}$ such that the transformation will result in a non-zero vector.
		
	\subsection{Mean Value Theorem}
	
		\begin{theorem}[Mean Value Theorem]
			Let $a, b$ be real numbers such that $a<b$, and let $h:[a, b]\to\mathbb{R}$ be a continuous differentiable function on $(a, b)$ and let $h'$ be it's derivative, then there exists $c\in (a, b)$ such that
			
			
			\[ h'(c) = \frac{h(b)-h(a)}{b-a} \]
			
			\textnormal{To see this is true, think about drawing a line between $h(b)$ and $h(a)$. Now drag this line up and down on the cartesian plane. At some point as You are dragging it up and down, it will be \textit{just} tangent to the function $h$. At this point, which We called $c$, the slope of the function $h$ will be equivalent to the slope between the two points $(b, h(b)), (a, h(a))$}.
			
		\end{theorem}
			
	\subsection{Fundamental Theorem of Calculus}
	
		\begin{theorem}[Fundamental Theorem of Calculus]
			Let $a, b$ be real numbers such that $a<b$ and let $h:\mathbf{dom}(h)\to\mathbb{R}$ be a differentiable function in the interval $(a, b)$, let $h'$ be that derivative and let $h'$ be continuous on $[a, b]$, then
			
			\[ h(b) - h(a) = \int^a_b h'(t) dt. \]
			
			\textnormal{Tale as old as time.}
		\end{theorem}
		
	\subsection{Differentiability}
	
		\begin{definition}[Differentiability]
			Let $f:\mathbf{dom}(f)\to\mathbb{R}^m, \mathbf{dom}(f)\subset\mathbb{R}^d$, then $f$ is called \textnormal{differentiable} at $\mathbf{x}$ in the interior of $\mathbf{dom}(f)$ if there exists a matrix $A\in\mathbb{R}^{m\times d}$ and an error function $r:\mathbb{R}^d\to\mathbb{R}^m$ in some neighbourhood of $\mathbf{0}\in\mathbb{R}^d$ such that $\forall\;\mathbf{y}$ in some neighbourhood of $\mathbf{x}$:
			
			\[ f(\mathbf{y}) = f(\mathbf{x}) + A(\mathbf{y-x}) + r(\mathbf{y-x}) \]
			
			where
			
			\[ \lim_{\mathbf{v}\to 0} \frac{\|r(\mathbf{v})\|}{\|\mathbf{v}\|} \]
			
			It then also follows that A is unique and We call A the \textnormal{differential} or \textnormal{Jacobian} of f at $\mathbf{x}$, and more accurately A is a matrix of partial derivatives such that
			
			\[ Df(\mathbf{x})_{i, j} = \frac{\partial f(\mathbf{x})_i}{\partial \mathbf{x}_j} \]
			
			Finally, $f$ is called \textnormal{differentiable} if it is differentiable at all points in it's domain. 
		\end{definition}
		
		So, remarks: the idea is that $f$ is differentiable if it is approximated arbitrarily well by some linear function ($f(\mathbf{x}) + A(\mathbf{y-x})$) and that the error of this approximation is sublinear, i.e. as size of the difference $\mathbf{y-x}$ decreases linearly, the error term $r(\mathbf{y-x})$ decreases even faster.
		
		There are also some notation to be aware of - $\nabla f(\mathbf{x})$ is the gradient vector and it is a\textit{column}, and $A = Df(\mathbf{x}) = \nabla f(\mathbf{x})^\top$, so then We can write stuff like $f(\mathbf{y}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y-x}) + r(\mathbf{y-x})$
		
		\begin{lemma}[Chain Rule]
			Let $f:\mathbf{dom}(f)\to\mathbb{R}^m,\mathbf{dom}(f)\in\mathbb{R}^d$ and  $g:\mathbf{dom}(g)\to\mathbb{R}^d$, $g$ be differentiable at $\mathbf{x}\in\mathbf{dom}(g)$ and $f$ be differentiable at $g(\mathbf{x})$, then
			
			\[Df(g(\mathbf{x})) = D(f\circ g)(\mathbf{x}) = Df(g(\mathbf{x}))Dg(\mathbf{x}) \]
		\end{lemma}
		
		It kind of can't be a lot of other things. Don't think about how the information flows through the matrix - it \textit{does}, but 1 variable of $g$ affects all variables in $f$, and it's kind of a pain. 
		
\section{Convex Sets}

	\begin{definition}[Convex Set]
		A set $C\in\mathbb{R}^d$ is \textnormal{convex} if $\forall\;\mathbf{x, y}\in C$ it is true that $\lambda\mathbf{x} + (1-\lambda)\mathbf{y}\in C, \lambda\in [0,1]$.
	\end{definition}
	
	Not incredible. Idea is that a set is convex if for all two points in that set, the line between those two points is also in the set, so the shape of the set has no dents etc.
	
	\begin{observation}
		Let $C_i, i\in I$ be convex sets and $I$ be a potentially infinite index set, then $C = \cap_{i\in I} C_i $ is a convex set.
	\end{observation}
	
	\begin{proof}
		$\forall\;\mathbf{x, y}\in C$ that must mean that $\mathbf{x, y}\in \cap_{i\in I} C_i$, and by virtue of convexity the line between those points must also be in the intersection.
	\end{proof}
		
	\subsection{Mean Value Inequality}
	
		Is a relaxation of the mean value theorem. Really it's Lipschitz but from the perspective of mean value theorem:
		
		Suppose $f:\mathbf{dom}(f)\to\mathbb{R}$ is differentiable over the open interval $X\subset\mathbf{dom}(f)$, and suppose that $\forall\mathbf{x}\in X \implies |f'(\mathbf{x})| \le B$, then We can say
		
		\[ |f(\mathbf{y}) - f(\mathbf{x})| = |f'(c)(\mathbf{y-x}) \le B|\mathbf{y-x}| \]
		
		Which, recall, $f'(c)$ is just the slope of the the line between $f(\mathbf{y}) - f(\mathbf{x})$, and We know it exists by the Mean Value Theorem.
		
		So, really, (sort of by definition almost really), any function whose derivative is bounded by $B$ is $B$-Lipschitz over that whatever interval $X$. 
		
		So We went from mean theorem to Lipschitz, sort of. Ugh the notes do this:
		
		\[ |f'(c)| \le \lim_{\delta\to 0} \bigg| \frac{f(c+\delta) - f(c)}{\sigma} \bigg| \le B \]
		
		Which, like, yeah, the value of the derivative is bounded in a Lipchitz function. Shocking. I feel like this is a trivial observation but off We go to prove it (ah it's sorta nontrivial since We are not in the univariate case and have to deal with the Spectral norm):
		
		\begin{theorem}
			Let $f:\mathbf{dom}(f)\to\mathbb{R}$ be a continuous differentiable function over some  $X\subseteq\mathbf{dom}(f), B\in\mathbb{R}_+$, where $X$ is open and convex, then the following two statements are equivalent:
			
			\begin{align*}
				i)\;& \| f(\mathbf{y}) - f(\mathbf{x}) \| \le B\cdot \|\mathbf{y-x}\|, \qquad \; \forall\mathbf{x, y}\in X. \\
				ii)\;& |Df(\mathbf{x})\| \le B,\qquad  \forall\mathbf{x}\in X.
			\end{align*}
			
		\end{theorem}
		
		\begin{proof}
			First, let's assume $i)$ and prove $ii)$:
			
			\begin{align}
				\| f(\mathbf{y}) - f(\mathbf{x}) \|& \le B\cdot \|\mathbf{y-x}\| \\
			\end{align}
			
			Recall that We have 
			
			\begin{align*}
				f(\mathbf{y}) &= f(\mathbf{x}) + \nabla f\mathbf{(x)^\top(y-x)} + r(\mathbf{y-x}) \\
				f(\mathbf{y}) -  f(\mathbf{x}) &= \nabla f\mathbf{(x)^\top(y-x)} + r(\mathbf{y-x}) \\
			\end{align*}
			
			Using the above
			\begin{align*}
				\|f(\mathbf{y}) - f(\mathbf{x})|& \le B\cdot \|\mathbf{y-x}\| \\
				\| \nabla f\mathbf{(x)^\top(y-x)} + r(\mathbf{y-x}) \| &\le B\cdot \|\mathbf{y-x}\| \\
				\| \nabla f\mathbf{(x)^\top(x+v-x)} + r(\mathbf{x+v-x}) \| & \le B\cdot \| \mathbf{x+v-x} \|, \; (\mathbf{y = x+v}) \\
				\| \nabla f\mathbf{(x)^\top(v)} + r(\mathbf{v}) \| & \le B\cdot \| \mathbf{v} \|  \\
				\| \nabla f\mathbf{(x)^\top(v)} + r(\mathbf{v}) \| \le \| \nabla f\mathbf{(x)^\top(v)} \| + \| r(\mathbf{v})  \|  &\le B\cdot \| \mathbf{v} \|  \\
				\| \nabla f\mathbf{(x)^\top(v)} \| + \| r(\mathbf{v})  \|  &\le B\cdot \| \mathbf{v} \|  \\
				\frac{\| \nabla f\mathbf{(x)^\top(v)} \|}{\| \mathbf{v} \|} + \frac{\| r(\mathbf{v})  \|}{\| \mathbf{v} \|}  &\le \frac{B\cdot \| \mathbf{v} \|}{\| \mathbf{v} \|}  \\				
				\frac{\| \nabla f\mathbf{(x)^\top(v)} \|}{\| \mathbf{v} \|}  &\le B - \frac{\| r(\mathbf{v})  \|}{\| \mathbf{v} \|}  
			\end{align*}
			
			And now comes some shenanigans. Let $\mathbf{v} = t\cdot\mathbf{v}^*$ such that $\mathbf{v^*}$ is a unit vector and 
			
			\[ \frac{\| \nabla f\mathbf{(x)^\top(v^*)} \|}{\| \mathbf{v^*} \|} = \| \nabla f\mathbf{(x)} \| \]
			
			Then We just do 
			
			\begin{align*}
				\frac{\| \nabla f\mathbf{(x)^\top(v)} \|}{\| \mathbf{v} \|}  &\le B - \frac{\| r(\mathbf{v})  \|}{\| \mathbf{v} \|}  \\
				\frac{\| \nabla f\mathbf{(x)} ^\top t\mathbf{(v^*)} \|}{\| t  \mathbf{v^*} \|}  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
				\frac{ |t| \| \nabla f\mathbf{(x)^\top(v^*)} \|}{ |t| \| \mathbf{v^*} \|}  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
				\frac{  \| \nabla f\mathbf{(x)^\top(v^*)} \|}{  \| \mathbf{v^*} \|}  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
				 \| \nabla f\mathbf{(x)} \|  &\le B - \frac{\|  r(t\mathbf{v^*})  \|}{\| t\mathbf{v^*} \|}  \\
			\end{align*}
			
			and as $t\mathbf{v^*}\to\mathbf{0}$, We get our result.
			
			Now for  the other direction, let's create the following function:
			
			\[ g(t) = \mathbf{f(x) + t(y-x)} \]
			
			This simply gives us smooth means of traversal through the function, which will allow us to use the bounded differentials. We then define $h(t) = f(g(t))$, so We are just wrapping our traversal function with, well, the function of interest $f$. This allows us to say
			
			\begin{align*}
				\|f(\mathbf{y}) - f\mathbf{x})\| &= \| h(1) - h(0) \| \\
				&= \bigg\| \int^1_{t=0} h'(t) dt \bigg\| \\
				&= \bigg\| \int^1_{t=0} D(\mathbf{x} + t(\mathbf{y-x}))(\mathbf{y-x}) dt \bigg\| \\
				&\le  \int^1_{t=0}  \| D(\mathbf{x} + t(\mathbf{y-x}))(\mathbf{y-x}) \| dt\quad\text{Jensen's Inequality} \\
				&\le  \int^1_{t=0}  \| D(\mathbf{x} + t(\mathbf{y-x})) \| \| (\mathbf{y-x}) \| dt\quad\text{Spectral Norm} \\
				&\le  \int^1_{t=0}  B \| (\mathbf{y-x}) \| dt\quad\text{Bounded differentials} \\
				&\le   B \| (\mathbf{y-x}) \| 
			\end{align*}
		\end{proof}
		
\section{Convex Functions}

	\begin{definition}[Convex Function]
		A function $f:\mathbf{dom}(f)\to\mathbb{R}$ is a \textnormal{convex function} if i) $\mathbf{dom}(f)$ is a convex set and ii) if $\forall\;\mathbf{x, y}\in\mathbf{dom}(f)$ it is true that
		
		\[ f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})  \]
		
		for $\lambda\in [0, 1]$.
	\end{definition}
	
	A useful concept to go along with the convex function is the epigraph:
	
	\[ \mathbf{epi}(f) = \{ (\mathbf{x}, \alpha) \in\mathbb{R}^{d+1}: \mathbf{x}\in\mathbf{dom}(f), \alpha \ge f(\mathbf{x})\}. \]
		
	So just the set of all points above the graph of the function $f$.
	
	\begin{observation}
		Function $f:\mathbf{dom}(f)\to\mathbb{R}$ is convex if and only if $\mathbf{epi}(f)$ is a convex set.
	\end{observation}
	
	\begin{proof}
		First direction, We can assume convex $f$:
		
		\begin{align*}
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})
		\end{align*}
		
		We need to prove:
		
		\begin{align*}
			\forall (\mathbf{x}, \alpha_x),\; (\mathbf{y}, \alpha_y)\in\mathbf{epi}(f): \lambda(\mathbf{x}, \alpha_x)+(1-\lambda)(\mathbf{y}, \alpha_y) \in\mathbf{epi}(f)
		\end{align*}
		
		So We have the point $(\underbrace{\lambda\mathbf{x} + (1-\lambda)\mathbf{y}}_q, \underbrace{\lambda\alpha_x + (1-\lambda)\alpha_y}_w)$. Now We just need to assert that $q\in\mathbf{dom}(f), w\ge f(q)$ to satisfy the epigraph conditions. We know that $q\in\mathbf{dom}(f)$ by convexity of the domain of $f$, and for the second bit
		
		\begin{align*}
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda \alpha_x + (1-\lambda)\alpha_y\\
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})
		\end{align*}
		
		Which is true by convexity of $f$. Wee.
		
		Now for the other side - We can assume that $\mathbf{epi}(f)$ is a convex set, and We wish to prove that $f$ is convex, so We want
		
		\begin{align*}
			f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y})
		\end{align*}
		
		This in words is quite trivial - take the two endpoints, they by definition are in the epigraph, then consider all points in between and observe that since they too are in the epigraph since the epigraph is convex they must be lie above the graph, concluding the proof.
	\end{proof}
	
	\begin{lemma}[Jensen's Inequality]
		For a convex function $f:\mathbf{dom}(f)\to\mathbb{R}$ and a random variable $X$ such that $X$ has a mean and $support(X)\in\mathbf{dom}(f)$ it is true that
		
		\[ E(f(X)) \le f(E(X))\]
	\end{lemma}
	
	\begin{proof}
		Let $\mu = E(X)$, and construct a tangent $a+bX$ to the convex function $X$ at $\mu$ such that $a + b\mu = f(\mu)$. By convexity of $f$ We have
		
		\begin{align*}
			f(X) &\ge a+bX\\
			E(f(X) &\ge E(a+bX)\\
			E(f(X) &\ge a+bE(X)\\
			E(f(X) &\ge a+b\mu\\
			E(f(X)) &\ge f(\mu)\\
			E(f(X)) &\ge f(E(X))
		\end{align*}
	\end{proof}
	
	\subsection{First Order Characterization of Convexity}

		\begin{lemma}
			Suppose $f$ is differentiable, then $f$ is also convex if and only if $\forall\mathbf{x, y}\in\mathbf{dom}(f)$
			
			\[ f(\mathbf{y}) \ge f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y-x})  \]
		\end{lemma}
		
		And intuitively this just means that the graph of $f$ is above the tangent to $f$ at any point.
		
		\begin{proof}
			First direction relies on rearranging convexity:
			
			\begin{align*}
				f(\lambda \mathbf{x} + (1-\lambda)(\mathbf{y})) \le \lambda f(\mathbf{x}) + (1-\lambda)f(\mathbf{y}) \\
				f(\lambda \mathbf{x} + \mathbf{y}-\lambda\mathbf{y}) \le \lambda f(\mathbf{x}) + f(\mathbf{y}) -\lambda f(\mathbf{y}) \\
				f( \mathbf{y} + \lambda(\mathbf{x-y})) \le f(\mathbf{y}) + \lambda (f(\mathbf{x}) - f(\mathbf{y})) \\
				f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y}) \le  \lambda (f(\mathbf{x}) - f(\mathbf{y})) \\
				f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y}) \le  \lambda (f(\mathbf{x}) - f(\mathbf{y})) \\
				\frac{f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y})}{\lambda} \le f(\mathbf{x}) - f(\mathbf{y}) \\
				\frac{f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y})}{\lambda} + f(\mathbf{y}) \le f(\mathbf{x})  \\
				\frac{\nabla f(\mathbf{x})^\top \lambda(\mathbf{x-y}) + r(\lambda(\mathbf{x-y})) }{\lambda} + f(\mathbf{y}) \le f(\mathbf{x}) \\
				\frac{\nabla f(\mathbf{x})^\top \lambda(\mathbf{x-y})}{\lambda} + \frac{r(\lambda(\mathbf{x-y})) }{\lambda} + f(\mathbf{y}) \le f(\mathbf{x}) \\
				{\nabla f(\mathbf{x})^\top (\mathbf{x-y})} + f(\mathbf{y}) \le f(\mathbf{x})\quad\lim_{t\to 0}
			\end{align*}
			
			So two pieces to this one - reframing convexity as having one "fixed" point (in this case $\mathbf{y}$, and observing that 
			
			\[ f( \mathbf{y} + \lambda(\mathbf{x-y})) -  f(\mathbf{y}) = \nabla f(\mathbf{y})^\top \lambda(\mathbf{x-y}) + r(\lambda(\mathbf{x-y}))  \]
			
			Since We are calculating the difference from $f(\mathbf{y})$ when our intput is shifted by $\lambda(\mathbf{x-y})$.
		\end{proof}
		
		\begin{proof}
			Now for the other direction, i.e. first order characterization implies convexity.
			
			First We establish some point $\mathbf{z}$ in between $\mathbf{x, y}$:
			
			\[ \mathbf{z} = \lambda \mathbf{x} + (1-\lambda)\mathbf{y} \]
			
			By first order characterization We then have
			
			\begin{align*}
				f(\mathbf{x}) &\ge f(\mathbf{z}) + \nabla f(\mathbf{z})^\top (\mathbf{x-z})\\
				f(\mathbf{y}) &\ge f(\mathbf{z}) + \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
			\end{align*}
			
			Then We start constructing our conclusion by multiplying the first line by $\lambda$ and the second by $(1-\lambda)$:
			
			\begin{align*}
				\lambda f(\mathbf{x}) &\ge \lambda f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z})\\
				(1-\lambda) f(\mathbf{y}) &\ge (1-\lambda) f(\mathbf{z}) + (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
			\end{align*}
			
			And then We add those two and simplify
			
			\begin{align*}
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				\lambda f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z}) + &(1-\lambda) f(\mathbf{z}) + (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				\lambda f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z}) + & f(\mathbf{z}) -\lambda f(\mathbf{z}) + (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				f(\mathbf{z}) + \lambda \nabla f(\mathbf{z})^\top (\mathbf{x-z}) &+ (1-\lambda) \nabla f(\mathbf{z})^\top (\mathbf{y-z})\\
				\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) &\ge \\ 
				f(\mathbf{z}) +  \nabla f(\mathbf{z})^\top \bigg( \lambda (\mathbf{x-z}) &+ (1-\lambda)(\mathbf{y-z})\bigg)
			\end{align*}
			
			Now We just focus on those gradient terms:
			
			\begin{align*}
				 \lambda (\mathbf{x-z}) + (1-\lambda)(\mathbf{y-z}) &= \lambda\mathbf{x} - \lambda\mathbf{z} + \mathbf{y-z} - \lambda\mathbf{y}+\lambda\mathbf{z}\\
				&= \lambda\mathbf{x} + (-\lambda - 1 + \lambda)\mathbf{z} + \mathbf{y} - \lambda\mathbf{y}\\
				&= \lambda\mathbf{x} + (1-\lambda)\mathbf{y} -\mathbf{z} \\
				&= 0
			\end{align*}
			
			So there's really one bit to to this proof - take the tangent at the middle point $\mathbf{z}$ and compare it with the endpoints $f(\mathbf{x}), f(\mathbf{y})$, and from there it's addition and simplification.
		\end{proof}
		
		Only really useful if You have a derivative. I think in later chapters this is generalized with subgradients. Furthermore there are obviously convex functions like the absolute value operator that are not differentiable.
		
		
		
		
		
		
		
		
		
		
		
		


\end{document} 