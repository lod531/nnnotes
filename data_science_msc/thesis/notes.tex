\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{chngcntr}
\usepackage{float}
\usepackage{tabu}
\usepackage{bm}
\usepackage[lite]{amsrefs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{bbm}
%\graphicspath{ {./img/} }

\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{tikz}
\usetikzlibrary{matrix}


\newcommand{\shrug}[1][]{%
\begin{tikzpicture}[baseline,x=0.8\ht\strutbox,y=0.8\ht\strutbox,line width=0.125ex,#1]
\def\arm{(-2.5,0.95) to (-2,0.95) (-1.9,1) to (-1.5,0) (-1.35,0) to (-0.8,0)};
\draw \arm;
\draw[xscale=-1] \arm;
\def\headpart{(0.6,0) arc[start angle=-40, end angle=40,x radius=0.6,y radius=0.8]};
\draw \headpart;
\draw[xscale=-1] \headpart;
\def\eye{(-0.075,0.15) .. controls (0.02,0) .. (0.075,-0.15)};
\draw[shift={(-0.3,0.8)}] \eye;
\draw[shift={(0,0.85)}] \eye;
% draw mouth
\draw (-0.1,0.2) to [out=15,in=-100] (0.4,0.95); 
\end{tikzpicture}}




\counterwithin*{equation}{section}

\newcommand{\R}{\mathbb{R}}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{1}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}
    
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

%\setlength{\parindent}{0pt}

\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}%
                     =}

\usepackage[]{algorithm2e}


\newcommand*\OR{\ |\ }

\newcommand{\KL}{\mathrm{KL}}
\newcommand{\uniform}{u}
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\model}{p_{\vtheta}}
\newcommand{\context}{\boldsymbol{c}}
\newcommand{\Count}{\#}

\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}

\begin{document}


\title{Thesis Stuff}
\author{}
\date{}

\maketitle

\section{Regularizing Neural Networks by Penalizing Confident Output Distributions}

	\subsection{Aight}
	
		So We have entropy
		
		\begin{align}
			H(p_\theta(y|x)) = -\sum_{y'\in\mathcal{Y}} p_\theta(y'|x)\log(p_\theta(y'|x))
		\end{align}
		
		It's an expected value of the information obtained by observing some label $y'$, nbd. 
		
		Negative log happens to (not just happens I bet there are deep reasons for this) satisfy all the properties We'd want for information - larger for smaller quantities, composes well, is never negative, is monotonous, events with probability 1 have no information etc.
		
		So then We have negative log likelihood for the loss:
		
		\begin{align}
			L(\theta) = -\sum^n_{i=1} \log(p_\theta(\hat{y}_i|x_i))
		\end{align}
		
		Where $y_i$ is the observed label for data point $i$.
		
		So now for each data point We add to the loss:
		
		\begin{align}
			L(\theta) = -\sum^n_{i=1} \log(p_\theta(\hat{y}_i|x_i)) + H(p_\theta(\hat{y}_i'|x_i))
		\end{align}
		
		The entropy term is always positive, and We have that minus in front of everything so now it's always negative. The loss can then be minimized by increasing the value of the strictly negative term, as in increasing the entropy of the output distribution.
		
		Now for the derivative of entropy I suppose, with respect to the logits. Note that the logit vector is what goes \textit{into} the softmax. Let softmax be $\theta$, $C$ be the number of classes and $z_i$ be the $i$'th input logit, then
		
		\begin{align}
			\frac{\partial H(p_\theta)}{\partial z_i} &= \frac{\partial }{\partial z_i} -\sum_{y'\in\mathcal{Y}} p_\theta(y'|x)\log(p_\theta(y'|x))\\
		&= \frac{\partial }{\partial z_i} -\sum_{j}^{C} \frac{e^{z_j}}{\sum e^{z_k}}\log\bigg(\frac{e^{z_j}}{\sum e^{z_k}}\bigg)\\
		&= \frac{\partial }{\partial z_i} -\sum_{j}^{C} S_j\log(S_j)
		\end{align}
		
		This can be split into two cases: $i=j$ and $i\neq j$.
		
		We have the derivative of the softmax - $D_jS_i = S_i(1-S_j)$ in case $i=j$ and $-S_jS_i$ in $i\neq j$.
		
		Anyway, let's look at $i=j$ in the overall equation.
		
		So
		
		\begin{align}
			\frac{\partial }{\partial z_i} - \frac{e^{z_i}}{\sum e^{z_k}}\log\bigg(\frac{e^{z_i}}{\sum e^{z_k}}\bigg)
		\end{align}
		
		By the product rule We get
		
		\begin{align}
			u &= \frac{e^{z_i}}{\sum e^{z_k}}\\
			\frac{\partial u}{\partial z_i} &= S_i(1-S_i)\\
			v &= \log\bigg(\frac{e^{z_i}}{\sum e^{z_k}}\bigg)\\
			\frac{\partial v}{\partial z_i} &= S_i(1-S_i) \cdot \frac{1}{S_i} = \frac{1-S_i}{S_i}
		\end{align}
		
		So here We used the derivative of softmax, sure, and We also used the chain rule to get derivative of $\partial v/\partial x\; log(x)$ where $x=S_i$, and $S_i$ is the i'th output of the softmax.
		
		So then by the product rule We get 
		
		\begin{align}
			uv' + u'v &= S_i\cdot \frac{1-S_i}{S_i} + S_i(1-S_i)\cdot \log(S_i)\\
			&= 1-S_i + S_i(1-S_i)\cdot \log(S_i)\\
			&= 1-S_i + S_i-S_i^2\cdot \log(S_i)\\
			&= 1 - S_i^2\cdot \log(S_i)
		\end{align}
		
		Alternative approach - backprop, as a quick sidenote.
		
		So $S_i = P_\theta(Y_i|X)$ so 
		
		\begin{align}
			H(p_\theta(y|x)) &= -\sum_{y'\in\mathcal{Y}} p_\theta(y'|x)\log(p_\theta(y'|x))\\
			&= -\sum_{j} S_j\log(S_j)
		\end{align}
		
		Then We have
		
		\begin{align}
			\frac{\partial H(P_\theta(\hat{y}_i|x_i))}{\partial S_j} = -(1+\log(S_j))
		\end{align}
		
		You'll need to sum across all outputs since any individual logit will affect them all.
		
		Alright christ
		
		\begin{align}
			\frac{\partial H(P_\theta(\hat{y}_i|x_i))}{\partial z_i} &= \sum_j \frac{\partial H(P_\theta(\hat{y}_i|x_i))}{\partial S_j}\times \frac{\partial S_j}{z_i}\\
			&= \sum_j -(1+\log(S_j))\times \frac{\partial S_j}{z_i}\\
			&= -(1+\log(S_i))\times \frac{\partial S_i}{z_i} + \sum_{j\neq i} -(1+\log(S_j))\times \frac{\partial S_j}{z_i}	\\
			&= -(1+\log(S_i))\times S_i(1-S_i) + \sum_{j\neq i} -(1+\log(S_j))\times (-S_jS_i)\\
			&= S_i\big(-(1+\log(S_i))\times (1-S_i) + \sum_{j\neq i} (1+\log(S_j))S_j\big)\\
		\end{align}
		
		So hmm.

		Alright christ
		
		\begin{align}
			&= -(1+\log(S_i))\times (1-S_i) + \sum_{j\neq i} (1+\log(S_j))S_j\\
			&= -(1-S_i) - (1-S_i)\log(S_i) + \sum_{j\neq i} (1+\log(S_j))S_j\\
			&= -1+S_i - \log(S_i) + S_i\log(S_i) + \sum_{j\neq i} (1+\log(S_j))S_j\\
			&= -1 - \log(S_i) + S_i(1+log(S_i)) + \sum_{j\neq i} (1+\log(S_j))S_j\\
			&= -1 - \log(S_i)  + \sum_{j} (1+\log(S_j))S_j\\
		\end{align}
		
		So it's pretty close aside from that pesky 1.
		
		I ought to try to simplify this:
		
		\begin{align}
			H(p_\theta(y|x)) &= -\sum_{y'\in\mathcal{Y}} p_\theta(y'|x)\log(p_\theta(y'|x))\\
			&= -S_1\log(S_1) -S_2\log(S_2) -S_3\log(S_3)\ldots 
		\end{align}
		
		And then I do thing We have 
		
		\begin{align}
			\frac{\partial H(P_\theta(y_i|x_i))}{\partial z_i} &= \sum_j \frac{\partial H(P_\theta(y_i|x_i))}{\partial S_j}\times \frac{\partial S_j}{z_i}\\
		\end{align}
		
		Then focus on the individual
		
		\begin{align}
			\frac{\partial H(P_\theta(y_i|x_i))}{\partial S_j}\times \frac{\partial S_j}{z_i}\\
		\end{align}
		
		Then individual parts:
		
		\begin{align}
			\frac{\partial H(P_\theta(y_i|x_i))}{\partial S_j} &= \frac{\partial}{\partial S_j} -S_1\log(S_1) -S_2\log(S_2) -S_3\log(S_3)\ldots\\
			&= -1-\log(S_j)
		\end{align}
		
		Then $\partial S_j/\partial z_i$. If $i=j$ then it's $S_i(1-S_i)$, otherwise $-S_jS_i$.
		
		So then.
		
		\begin{align}
			\frac{\partial H(P_\theta(y_i|x_i))}{\partial z_i} &= \sum_j \frac{\partial H(P_\theta(y_i|x_i))}{\partial S_j}\times \frac{\partial S_j}{z_i}\\
			&= (-1-\log(S_i))(S_i(1-S_i)) + \sum_{j\neq i} -1-\log(S_j)\times (-S_jS_i)\\
		\end{align}
		
		And now the idea is to simplify everything down
		
		\begin{align}
			&= (-1-\log(S_i))(S_i(1-S_i)) + \sum_{j\neq i} (-1-\log(S_j))\times (-S_jS_i)\\
			&= S_i\bigg((-1-\log(S_i))(1-S_i) + \sum_{j\neq i} (-1-\log(S_j))\times -S_j\bigg)\\
			&= S_i\bigg(-1-\log(S_i) + S_i + S_i\log(S_i) + \sum_{j\neq i} (-1-\log(S_j))\times -S_j\bigg)\\
			&= S_i\bigg(-1-\log(S_i) + S_i(1 +\log(S_i)) + \sum_{j\neq i} (-1-\log(S_j))\times -S_j\bigg)\\
			&= S_i\bigg(-1-\log(S_i) + S_i(1 +\log(S_i)) + \sum_{j\neq i} (1+\log(S_j))\times S_j\bigg)\\
			&= S_i\bigg(-1-\log(S_i) + \sum_{j} (1+\log(S_j))\times S_j\bigg)\\
			&= S_i\bigg(-1-\log(S_i) + \sum_{j} S_i +\sum_{j}S_j\log(S_j))\bigg)\\
			&= S_i\bigg(-1-\log(S_i) + \sum_{j} S_i +H(P_\theta(y_i|x_i)\bigg)\\
			&= S_i\bigg(-1-\log(S_i) + 1 +H(P_\theta(y_i|x_i)\bigg)\\
			&= S_i\bigg(-\log(S_i)+H(P_\theta(y_i|x_i)\bigg)\\
			&= P_\theta(y_i|x_i)\bigg(-\log(P_\theta(y_i|x_i)+H(P_\theta(y_i|x_i)\bigg)
		\end{align}
		
		ayyy
		
	\subsection{Smoothing as KL penalty}
	
		First We have \href{https://arxiv.org/pdf/1512.00567.pdf}{Rethinking the Inception Architecture for Computer Vision by Szegedy et al}
		
		So this is label smoothing as defined by them:
		
		You have the ground truth distribution of $q(k|x)$, where $k$ is the label and $x$ is the observed data point. 
		
		So then they observe that if You're optimizing the log likelihood, You'll end up modeling $q(k|x)$ as a Dirac delta $\delta_{k, y}$ such that $\delta_{k, y} = 1$ if $k=y$ and 0 otherwise. Sure thing, the truth and nothing but the truth.
		
		And so in this sense log likelihood is great - as it optimizes the network You'll get Your delta distribution.
		
		Then the problem is that this yields overconfidence and overfitting. To solve this, instead of making the assumption that the ground truth $q$ distribution is a delta, they instead make it noisy:
		
		\begin{align}
			q'(k|x) = (1-\epsilon)\delta_{k, x} + \epsilon u(k)
		\end{align}
		
		And they take $u$ to be the uniform distribution, so You get 
		
		\begin{align}
			q'(k|x) = (1-\epsilon)\delta_{k, x} + \epsilon/K
		\end{align}
		
		How do You use this in a loss then?
		
		Well they had cross entropy
		
		\begin{align}
			\ell = -\sum^K_{k=1} \log(p(k))q(k)
		\end{align}
		
		Where $q$ is the true distribution of the labels given data and $p$ is the distribution approximated by the model, i.e. the output of the softmax.
		
		With the original choice of $q(k)$ as the true distribution, that sum reduces to just a single term. 
		
		Now You stick in Your new probability $q'(k)$ and suddenly all the predictions factor into the loss. Neat.
		
		You can expand that loss using the new distribution as follows:
		
		\begin{align}
			\ell &= -\sum^K_{k=1} \log(p(k))q(k)\\
			&= -\sum^K_{k=1} \log(p(k))((1-\epsilon)\delta_{k, x} + \epsilon u(k))\\
			&= \bigg(-\sum^K_{k=1} \log(p(k))(1-\epsilon)\delta_{k, x}\bigg) -\bigg(\sum^K_{k=1} \log(p(k)) \epsilon u(k)\bigg)\\
			&= (1-\epsilon)\bigg(-\sum^K_{k=1} \log(p(k))\delta_{k, x}\bigg) +\epsilon \bigg(-\sum^K_{k=1} \log(p(k))u(k) \bigg)\\
		\end{align}
		
		Then since cross entropy is
		
		\begin{align}
			CE(P, Q) = -\sum_{x\in\mathcal{X}} P(x)\cdot log(Q(x))
		\end{align}
		
		This is kind of awkward since for some reason in the Szegedy paper they take $P$ to be the estimate and $Q$ to be the truth, and my notation is backwards lol.
		
		Then We can say
		
		\begin{align}
			\ell &= (1-\epsilon)\bigg(-\sum^K_{k=1} \log(p(k))\delta_{k, x}\bigg) +\epsilon \bigg(-\sum^K_{k=1} \log(p(k))u(k) \bigg)\\
			&= (1-\epsilon)CE(Q, P) +\epsilon CE(U, P)
		\end{align}
		
		Alright and so since You changed the underlying distribution in the cross entropy into a mixture of two distributions then the loss splits into two cross entropy terms. 
		
		Then they observe well hey, as per appendix You can split that second term into
		
		\begin{align}
			\ell &= (1-\epsilon)CE(Q, P) +\epsilon CE(U, P)\\
			&= (1-\epsilon)CE(Q, P) +\epsilon \big( Ent(U) + KL(U, P) \big)
		\end{align}
		
		And then observe that hey, entropy of $U$ is a constant so the loss doesn't care about it one way or the other, and so You can toss it, which means You end up with 
		
		\begin{align}
			\ell &= (1-\epsilon)CE(Q, P) +\epsilon \big( Ent(U) + KL(U, P) \big)\\
			&= (1-\epsilon)CE(Q, P) +\epsilon KL(U, P)
		\end{align}
		
		Finally they mention that when $U$ is the uniform distribution, $CE(U, P)$ ends up being a measure of how dissimilar $P$ is to the uniform, and I guess the idea is that entropy of $P$ also captures this? As in, the larger the entropy of $P$, the closer it is to the uniform, so We can just sub that in. Sure.
		
	\subsection{KL and entropy penalty}
	
		Kay so now go all the way back and recall that the idea for this paper is to take the log likelihood and add on to that entropy:
		
		\begin{align}
			L(\theta) = -\sum^n_{i=1} \log(p_\theta(\hat{y}_i|x_i)) + H(p_\theta(\hat{y}_i'|x_i))
		\end{align}
		
		So they say in the paper that "When the prior
label distribution is uniform, label smoothing is equivalent to adding the KL divergence between the
uniform distribution $u$ and the network's predicted distribution $p_\theta$ to the negative log-likelihood" which We just derived.

		\begin{align}
			\mathcal{L}(\theta) = -\sum\log(p_\theta(y|x)) - KL(u|p_\theta)
		\end{align}
		
		Everyone's got their own notation lol.
		
		Then their point is that hey, if You reverse the KL divergence term, You get confidence penalty which is what they propose:
		
		\begin{align}
			\mathcal{L}(\theta) &= -\sum\log(p_\theta(y|x)) - KL(p_\theta | u)\\
			&= -\sum_y \log(p_\theta(y|x)) - \sum p_\theta(y) \log\bigg(\frac{p_\theta(y)}{u(y)} \bigg)\\
			&= -\sum_y \log(p_\theta(y|x)) - \sum p_\theta(y) \log(p_\theta(y)) + \sum p_\theta(y) \log(u(y))\\
			&= -\sum_y \log(p_\theta(y|x)) - \sum p_\theta(y) \log(p_\theta(y)) + \log(u(y))\bigg(\sum p_\theta(y)\bigg) \\
			&= -\sum_y \log(p_\theta(y|x)) - \sum p_\theta(y) \log(p_\theta(y)) + \log(u(y))\cdot 1\\
			&= -\sum_y \log(p_\theta(y|x)) - \sum p_\theta(y) \log(p_\theta(y))\\
			&= -\sum_y \log(p_\theta(y|x)) - H(p_\theta)
		\end{align}
		
		And We lose that last uniform term since it's just a constant so who cares.


\newpage		
\section{Jurafsky and Martin - Speech and Language Processing}

	\subsection{N-Grams}
	
		So in an $N$-gram model We predict what the next word will be given $N-1$ previous words.


		\textbf{Disfluencies} - I suppose You could call them things that break the normal flow of a sentence - whether it's starting a word and reapeating it, or filler words. Basically if You transcribe a spoken sentence into a written one, anything that does not belong in the written translation is a disfluency it seems.
		
		\textbf{Lemma} - words that share a root and meaning, I suppose.
		
		\textbf{Wordform} - an instanciation of the lemma?
		
		\textbf{Word type} - distinct identity assigned to each word based on tokenization / lemma-ization? And tokens just being counts of words.
		
		So word token - token in sentence/corpus, word type - unique identity of word. You count the number of word types in a corpus to get the dictionary of the corpus.		
		
		A complaint: they claim that to get the number of bigrams $(w, X)$ where $w$ is fixed We just need to count the number of instances of $w$.
		
		I feel like this is wrong because if $w$ is the last word in a sentence, then it won't be a part of a bigram.
		
		So You want to know the probability of "I am", and the words "I" and "am" occur only once and form the bigram "I am" so naive porbability is one. Now add a bunch of sentences that end in "I", somehow. Those new endings should not affect the bigram count, but under their notation, they will.
		
		Padding with an end token solves this.
		
		\textbf{OOV} - Out of Vocabulary words, and percetange of OOV words in test set is the OOV rate.
		
		The way they model these in the book is weird - mask out OOV words from the training set, as in OOV words are in the training set lol, and just replace them with UNKs and train on that.
		
		\textbf{Extrinsic} evaluation is hard - it's evaluation on an actual task, something in the real world, like talking to Your phone. They give example of a thorough test that takes days to compute. I guess the point is that it's representative but expensive.
		
		\textbf{Intrinsic} is the fast proxy
		
		\subsubsection{Laplace Smoothing}
		
			Aight so the idea is to add 1 count to all occurrences.
			
			In the unigram model, this corresponds to adding 1 to all nominators and $V$ to all denominators, where $V$ is the size of the vocabulary.
			
			Another view is the adjusted count. The idea is that this is easier to compare with the original vanilla counts.
			
			We take each original count, $c_i$, and just add 1 to it by Laplace smoothing. Sure.
			
			But now that We've added 1 to everything, We have to multiply every count by this new normalizing factor.
			
			So, OG Laplace smoothing ($w_i$ is the random variable for word at position $i$, $c$ is some word, $c_i$ is the count of that word in the training corpus, $N$ is the number of words in the corpus, $V$ is the number of words in the vocabulary):
			
			\begin{align}
				P(w_i = c) = \frac{c_i + 1}{N + V}
			\end{align}
			
			Or, You can look at as if though You have some normalizing constant applied to the updated counts, dictated by the kind of update You do:
			
			\begin{align}
				P(w_i = c) = (c_i+1)\cdot\frac{1}{N + V}
			\end{align}
			
			A \textbf{discount factor} is the ratio of smoothed to unsmoothed counts:
			
			\begin{align}
				d_{c_i} = \frac{c^*_i}{c_i}
			\end{align}
		
			So now Laplace smoothing for bigrams.
			
			Allegedly it is done like so:
			
			\begin{align}
				P^*_{Laplace}(w_n|w_{n-1}) = \frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V}
			\end{align}
		
			Which I guess makes sense if You think again about how much additional mass has smoothing introduced - given some fixed $w_{n-1}$, We now have $V$ options for $w_n$, and each of those has just gained a +1 in terms of count.
			
			Hah, so then depending on the case, adding 1 to all counts might be too dramatic, so they introduced add$-\delta$ smoothing, where You add less than 1.
			
		\subsubsection{Good-Turing Smoothing}
		
			Kay so N-grams that occur once are called \textbf{singletons} or, and get this, \textbf{hapax legomenon}.
			
			First We have $N_c$, the number of N-grams that occur $c$ times. Bit wonky reusing $c$ here, but sure - $c$ is now a number, not a word.
			
			So if there are 10 different N-grams that occur 8 times each, then $N_8 = 10$.
			
			Good Turing then has two main equations. The first one takes care of the case of N-grams that just don't occur, since We don't have a count for them. The equation for those is:
			
			\begin{align}
				P(\text{N-gram not in training set}) = \frac{N_1}{N}
			\end{align}
	
			Where $N_1$ is the number of N-grams that appear once, and $N$ is the total number of tokens in the corpus.
			
			For non-zero cases, You use this formula to get the updated count:
						
			\begin{align}
				c^* = (c+1)\frac{N_{c+1}}{N_c}
			\end{align}
			
			And then the probability is just $c^*/N$.
			
			But so how was this derived?
			
			Take Your training set, which have in total $c$ unique N-grams.
			
			Now take one of the N-grams out, so Your vocabulary now has $c-1$ words. You can of course do this $c$ times, resulting in $c$ corpuses, each having a dictionary of $c-1$ N-grams.
			
			So now the question is, what fraction of words are held out in training? 
			
			Well the slides for this are shit, and can be found here: 
			
			\url{https://youtu.be/GwP8gKa-ij8}, instead let's consult the paper, which can be found here: 
			
			\url{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=476512} .
			
			So looking at the paper then, they still have $N_c$, except they call it $N_r$.
			
		\subsubsection{On the Estimation of 'Small' Probabilities
by Leaving-One-Out}

			We have $K$ classes, $N(k)$ is the number of times class $k$ occurs in the corpus, e.g. if "I am" occurs three times, then $N(\text{"I am"}) = 3$.
			
			In the simplest case then, the probability of some event $k$ is $N(k)/N$, where $N$ is the total number of tokens in the corpus.
			
			The problem then of course is that there exist $k$ such that $N(k)=0$ etc. etc.
			
			So now We generalize - first We start to think about equivalence classes of events $k$. If two events $k$, $j$ occur the same number of times, as in $N(k)=N(j)$, then they are in the same equivalence class, and events in the same equivalence class should be assigned the same probability. This is with a view towards the idea that We're going to change the probabilities of events in the same equivalence class.
			
			So, now that We have these equivalence classes which are specified by the number of times an N-gram occurs, We talk about how large each equivalence class is - how many N-grams occur just once will be the size of the equivalence class of count 1, or something like that.
			
			So then that's what $n_r$ is, it's how large is an equivalence class:
			
			\begin{align}
				n_r = \sum_{k:N(k)=r}1
			\end{align}
			
			In the stanford lecture and the book this was $N_c$.
			
			Kay so now one observation is that the number of tokens can be expressed as:
			
			\begin{align}
				N = \sum^R_{r=1} r\cdot n_r
			\end{align}
			
			Where $R$ is the number of times the most frequent N-gram occurs in the corpus. 
			
			We also have the 'ole probability constraint:
			
			\begin{align}
				\sum^R_{r=1} r\cdot p(n_r) = 1
			\end{align}
			
			Which is wrong and it's good that it's wrong lol.
			
			So this is a different way of thinking about this, and it's actually a fundamentally shitty one in terms of what We're going to ultimately be doing.
			
			So suppose We have a bigram "I am", how do I know it's probability in this scheme?
			
			Well, as part of training, We calculate probabilities for each equivalence class, and then I guess We have to find out which equivalence class "I am" belongs to in the training set, and then We return the probability of that equivalence class. The reason it's kind of dumb in the long term is that this is a static distribution, as in it does not take any kind of context into account, as in the distribution of the equivalence classes is the same no matter what We condition on in the test set.
			
			So, at first We had $K$ outcomes, which were N-grams, then We said I guess fuck that and now We bin the N-grams by frequency and get equivalence classes, and now those equivalence classes are the events and they get probabilities. Which means
			
			\begin{align}
				\sum^R_{r=1} p_r\cdot n_r = 1
			\end{align}
			
			Now they introduce cross-validation. So We split the data into a training set and a holdout set. We use the training set to figure out the equivalence classes, as in in the training set "I am" occurs 15 times, so it belongs to the $n_15$ equivalence class.
			
			Then they have this $C_r$ number, and it's $C_r$, and $C_r$ is the number of observations of N-grams that belong to $n_r$ in the holdout set. So for example if You take $n_2$, in there You might have 10 N-grams, since each of them occurred 2 times, like "we meet" and "go there" or whatever, then You look at the holdout set and You count how many "we meet" tokens there are, how many "go there" tokens there are etc. and then You sum all of those counts up to get $C_r$. I guess You would expect it to be around $r\times n_r$?
			
			And so now Your goal is to find $p_r$ such that the probability of the holdout set is maximized, I guess they don't hold to the descent convention.
			
			But so You know that if We consider some $n_r$, that $n_r$ occurred $C_r$ times in the holdout, so We have
			
			\begin{align}
				P(\text{holdout set}) = \prod^{N_H}_{i=1} P(H_i = k)
			\end{align}
			
			Where $N_H$ is the number of tokens in the holdout set, $H_i=h_i$ is the event that the $i$'th token in the holdout set turned out to be some N-gram $h_i$, which We observed.
			
			\begin{align}
				P(\text{holdout set}) &= \prod^{N_H}_{i=1} P(H_i = k)\\
				&= \prod^{R}_{r=0} \prod^{C_r}_{i=1}  p_r\\
				&= \prod^{R}_{r=0} p_r^{C_r}
			\end{align}
			
			I know it looks a little weird - the second product operator is just there to get $p_r^{C_r}$
			
			How did We get there - first observe that if We iterate over $C_r$, We get all the terms:
			
			\begin{align}
				\sum^R_{r=0} C_r = N_H
			\end{align}
			
			Which makes sense - $C_1$ is the number of tokens in the holdout set which occurred once in the training set, $C_2$ is the number of tokens in the holdout set which occurred twice in the training set and so on until every token in the holdout set is counted. Note that the summation starts at $r=0$ to account for tokens that appear not at all in the training set but are seen in the holdout set.
			
			But so then We have $C_r$ number of tokens which belong to equivalence class $n_r$, and We know that all of those tokens have the same probability, so We just get $C_r$ $p_r$ terms multiplied together to get the joint probability assuming independence stuff.
			
			Then take the log to get the sum: 
			
			\begin{align}
				P(\text{holdout set}) &= \prod^{N_H}_{i=1} P(H_i = k)\\
				&= \prod^{R}_{r=0} \prod^{C_r}_{i=1}  p_r\\
				&= \prod^{R}_{r=0} p_r^{C_r}\\
				\ln(P(\text{holdout set})) &= \ln\bigg( \prod^{R}_{r=0} p_r^{C_r} \bigg)\\
				&= \ln\bigg( \prod^{R}_{r=0} p_r^{C_r} \bigg)\\
				&= \sum^{R}_{r=0} \ln(p_r^{C_r}) \\
				&= \sum^{R}_{r=0} C_r\ln(p_r) 
			\end{align}
			
			And so then We maximize this function, wee.
			
			The paper goes on to add the lagrange multipliers and taking it from there, which actually doesn't seem that instructive, so let's go back to the Stanford lecture:
			
		\subsubsection{Good-Turing Smooth Stanford Lecture}
		
			Can be found at \url{https://youtu.be/GwP8gKa-ij8}.
			
			Okay so the Stanford guy got His slides from Dan Klein, and here are Klein's slides: \href{https://people.eecs.berkeley.edu/~klein/cs288/sp10/slides/SP10%20cs288%20lecture%203%20--%20language%20models%20II%20(6PP).pdf}{(click me)}.
			
			The intuition for Good Turing comes from cross validation. We can't estimate the probability of words We haven't seen, but We can take words We have out of the corpus to simulate the case, and then We just use this simulation to get an approximation.
			
			So You start with some corpus with lots of tokens.
			
			The total training set has $c$ unique N-grams, and associated $n_k$ partitions. 
			
			Now take one word out at a time to get $c$ sets, each with a vocabulary size of $c-1$. 
			
			Depending on the word, it belonged to some particular partition $n_k$ in the original full training set.
			
			So now I guess You take the union of the $c$ hold-one-out training sets You have and think about missing counts?
			
			So, how many $n_1$ terms are missing from the union of the set? Well, for each item in $n_1$ was taken out once, and they all occurred once in the corpus, so $n_1$ words were missing all together.
			
			What about $n_2$? Well, each of those was also missing once, and each time We took a word out of the vocabulary We took out 2 words out of the corpus since the words We are taking out belonged to the $n_2$ partition, so in total $2\cdot n_2$ words.
			
			Going back to the $n_1$ case, this question can be reformulated - how many tokens are seen 0 times in the training set? Well, for a token to have been erased from the training set, it had to occur once in the original set. There were $n_1$ such tokens.
			
			So now what about the partitions in the unified cross validation set? "How many held-out tokens are seen $k$ times in the training set?". Well, in order for a held-out token to have been seen $k$ in the set with that token removed, it had to have been present $k+1$ times in the original set. There are $n_{k+1}$ such tokens by definition, so the number held out tokens that are seen $k$ times .....
			
			This doesn't square.
			
			I think the actual set up is that they are just literally removing \textit{a} token at a time.
			
		\subsubsection{Good Turing Let Me Go}
		
			Kay You have Your training set lol, and it has $c$ N-grams.
			
			Now create $c$ other sets by taking out one N-gram at a time.
			
			Now how many held-out n-grams are never seen in training? It's $n_1$ n-grams since if We remove one token from the corpus and because of this the token never occurs again, it belonged to the $n_1$ partition, and there are $n_1$ tokens belonging to the $n_1$ partition.
			
			Now how many held-out n-grams were seen once in the training set? Well, I took a token and I yoinked it out. It occurred twice originally, now it occurs once. There are $(k+1)n_{k+1}$ such tokens in the corpus, if You think about it. Consider some particular token that occurs twice, like "I am", and it happens twice in the corpus. How many times is it seen when it's held out? Two times it's taken out, each time it's taken out one of the "I am" tokens remains.
			
			Now suppose there are two such tokens, as in two tokens occur twice. Then over all You get each token seen twice, so $2\times 2$. In general then, if You have $n_2$ number of tokens seen twice (which is just the definition of $n_2$ lol) then You'll see them $2\times n_2$, each time a token is held out it'll be seen once.
			
			In general then, each token which occurs $k$ times in the original set will be observed $k-1$ times in the holdout set, and this will happen with $n_k$ tokens (since each token belonging to $n_k$ occurs $k$ times, and each time We're going to take one instance out, leaving $k-1$ times the token occurs).
			
			"How many held-out tokens are seen $k$ times in training?" - given a token is held out and it occurs $k$ times in the training set then it occurred $k+1$ times in the not-held out set and this scenario is going to happen $n_{k+1}$ times.
			
			I don't understand why We are doing what We are doing and where We are going.
			
			Only thing that I can make sense of is to calculate counts from the leave-one-out training set from the perspective of the original set. As in You've left a word out on purpose to see what it looks like in teh wild, and now You're using that to estimate some counts/probabilities.
			
			So You leave a word out, and now it occurs $k$ times in the holdout set You just generated. That means in the original set it occurred $(k-1)n_k$ times. Sure.
			
			So now what. 
			
			We are predicting the distribution of the holdout set(s) in terms of words in the original corpus?
			
			Kay so if We leave a word out and generate a bunch of corpora, and We take those new corpora to model the real world, We expect that in there the words that will occur with frequency $k$, as in words that belong to the partition $n_k$ in our original dataset occurred $(k+1)n_{k+1}$ times.
			
			And then since We are looking at words that occurred $k$ times, there are ah fuck 
			
			Okay so We will expect $n_k$ words to be of frequency $k$ in the future datasets as an absolute statement.
			
			And then You get the expected count for each individual words out of the $n_k$ words that will have frequency $k$ by dividing by $n_k$. What a mess.
			
		\subsubsection{Okay so here is a Good-Turing Smoothing derivation}
		
			Start with some corpus with $N$ tokens in it. The tokens may be n-grams for instance.
			
			Call the complete corpus of all the tokens set $C$. 
			
			Now generate holdout corpora by taking one of the $N$ tokens out at a time, which will yield $N$ holdout sets, each with $N-1$ tokens in them.
			
			Now think about the complete corpus - in that corpus, there may be 10 tokens that only occur once, as in if You removed that particular token, then that type of token does not appear in the corpus again, for example if "I am" is in there as an n-gram just once, then removing that token mean "I am" is no longer in the set at all anywhere else.
			
			You can count the number of tokens that occur only once, and that will yield a number. Call that number $n_1$. Likewise, there will be a bunch of tokens that occur twice, like if "to go" is in the corpora twice. Count the number of tokens that appear twice (which means You are counting \textit{types} of tokens), and You'll get $n_2$, the number of tokens in the corpus that occur twice. So on for $n_k$, the number of tokens that occur $k$ times.
			
			The idea then is that in the real world We will need to figure out what to do with tokens We've never seen before.
			
			We obviously can't just simply reason about the unseen token - We've never seen it before, know nothing of it's potential frequency.
			
			So the idea then is to take the complete corpus and drop out one token at a time to generate those holdout corpora, and come up for probabilities in the complete set in terms of tokens.
			
			First are the tokens that won't appear at all in the holdout when held out - there will be $n_1$ of those, since for a token to be unseen after removal, it had to have had occurred just once in the complete corpus.
			
			Now as a probability, We have $N$ total tokens, and out of that total $N$, $n_1$ occurred just once. So to get a probability, We do $n_1/KN$.
			
			Now for the general case, if We observe some token $w$ times in a holdout corpus, then it had to have had occurred $w+1$ times in the complete corpus. There are then by definition $n_{w+1}$ such tokens in the complete corpus. 
			
			To get the probability of some token which belongs to the $n_w$ partition, We just do 
			
			\begin{align}
				\frac{(w+1)n_{w+1}}{N}
			\end{align}
			
			Since out of all $N$ tokens, $(w+1)n_{w+1}$ will occur with frequency $w$ in the holdout sets, which try to keep in mind that the holdout sets are trying to model our goal distribution.
			
			Since that equation there gives You the fraction of tokens that will occur $n_w$ times, then the expected count can be derived as follows: first, We will have $n_w$ types of tokens that occur $w$ times, as in "I am" may occur $w$ times, "me too" can occur $w$ times and so on.
			
			We want probabilities for types of tokens in the complete corpus in terms of counts of tokens in the holdout corpora.
			
			Anyway to get counts We just multiply the fraction We expect to have frequency $w$:
			
			\begin{align}
				\frac{(w+1)n_{w+1}}{N}\times N = (w+1)n_{w+1}
			\end{align}
			
			But then there are $n_w$ such tokens, by kind of definition, so We get
			
			\begin{align}
				\frac{(w+1)n_{w+1}}{n_w}
			\end{align}
			
			As expected new count of tokens that occur $w$ times lol.
			
			I think the notation here is shit. You need pseudocounts. Which is what these are. This new expected count is a psuedocount, which it kind of has to be since We carved out space for tokens with frequency 0, so even though the no-shit-Sherlock answer to "how often do You expect a token which occurs $w$ times to occur?" is $w$, now it's this thing above due to our meddling, hence a pseudocount. Or at least it's a pseudocount from the perspective of the holdout set.
			
		\subsubsection{One Last Good Time, I Swear}
		
			\href{http://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec11.pdf}{Cornell lecture here (click me)}.
		
			Simulate the problem ahead of time by doing what precisely.
			
			Well, holding stuff out, but how to do it such that it's representative? 
			
			Well it only makes sense to think about the distribution of something given that it's been held out, otherwise it's kinda the same as empirical observation.
			
			I am struggling to envision what sample space We are dividing up here. 
			
			We hold out one token, then group the resulting holdout sets by the partitioning of the held out tokens, and look at the resulting distribution.
			
			Okay there are two main parts here I think.
			
			First is the partition assumption.
			
			Second is the idea that if You observe a holdout set in the wild, that the counts of that holdout set are approximately equal to the complete set.
			
			So, let $n_r$ be frequency of frequencies in the complete set, let $\hat{n}_r$ be f.o.f. in the holdout set.
			
			Now hold out a partition.
			
			If the partition is missing, then it was $n_1$ in the complete set. $n_1/N$ is the fraction of tokens, in terms of complete-set terms.
			
			If, given that a token of this partition has been held out, it occurs $k$ times in the holdout set, then it occurred $k+1$ times in the original set, and there are $(k+1)\times n_{k+1}$ such tokens in the original set. As a fraction it's $(k+1)\times n_{k+1}/N$.
			
			It's bad intuition. The idea is then to say sure, We have come this far, but now that We have observed the actual holdout set, even though the "source" probability might be $(k+1)\times n_{k+1}/N$, We will now take this and divide it among the $n_k$ token types that We have observed. 
			
			Rather, call $\hat{n}_k$ the number of items that have occurred $k$ times in the holdout set, where We got to the holdout set by holding out one token from each type that belonged to $n_{k+1}$ in the complete set. Basically $\hat{n}_k\approx n_{k+1}$.
			
			So then the final equation is given by
			
			\begin{align}
				\frac{(k+1)\times n_{k+1}}{N\times \hat{n}_{k}}
			\end{align}
			
			And then You say okay well that's approximately
			
			\begin{align}
				\frac{(k+1)\times n_{k+1}}{N\times n_{k}}
			\end{align}
			
			Is the best I have. 
			
		\subsubsection{Issues in Good-Turing}
		
			Jeez the more I stare at Good-Turing the shittier it seems.
			
			So for one, it's not all that principled. It starts with this neat "hold one out" idea, and then just makes assumptions until You get a distribution out.
			
			It's also ill defined since the adjusted count for the most frequent partition depends on an even more frequent partition, which does not exist.
			
			And then to solve this they just throw all caution to the wind and fit a log-linear model and use that to fill in blanks, plus some random hacks like "oh of frequency is above 5 don't even worry lol.
			
		
		\subsubsection{Backoff and Interpolation}
		
			Backoff is using (n-1)-gram counts if n-gram counts are zero.
			
			Interpolation is taking weighted sums of various n-gram probabilities such that the weights sum to 1.
			
			You can an also have conditioned interpolation, where the weight factors change depending on context.
			
		\subsubsection{Katz Backoff}
		
			First equation is:
			
			\begin{align}
				P_{\text{Katz}}(w_n|w^{n-1}_{n-N+1} = P^*(w_n|w^{n-1}_{n-N+1})
			\end{align}
			
			if the n-gram count of the N-gram in question is non-zero, and otherwise We back off to
			
			\begin{align}
				P_{\text{Katz}}(w_n|w^{n-1}_{n-N+1}) = \alpha(w^{n-1}_{n-N+1})P_{\text{katz}} (w_n|w^{n-1}_{n-N+2})
			\end{align}
			
			So lots of little bits going on here.
			
			First is $P^*$. The reason this is some new distribution and not just straight forward $P$ which would result from n-gram counts, is that We are giving mass to things that $P$ assigns zero mass to. That's kind of the whole point - We encountered an n-gram with zero count, which our naive approximation would give 0 mass to, so We use something else that gives non-zero mass, which means We have to take mass away from other stuff to keep everything summing to 1.
			
			Next is the backoff case - to see this a little clearer maybe let's write
			
			\begin{align}
				P_{\text{Katz}}(w|w^{a}_{b}) = \alpha(w^{a}_{b})P_{\text{katz}} (w|w^{a}_{b+1})
			\end{align}
			
			Basically just dropping a context word once a turn. Wild.
	
\newpage		
\section{Working through what's already there}

	\begin{align}
	   \sum_{\context} \Big( {\color{blue} \KL\left(\tilde{p}(\cdot 			\mid \context) \mid\mid \model(\cdot \mid \context)					\right)} + {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)}\Big)
	\end{align}
	
	So $c$ is the context, I suppose N-gram wise.
	
	Recall
	
	\begin{align}
			KL(P, Q) &= \sum_{x\in\mathcal{X}} P(x)\cdot\big(\log(P(X)) - \log(Q(X)) \big)\\
			&= \sum_{x\in\mathcal{X}} P(x)\cdot\log\bigg(\frac{P(X)}{Q(X)}\bigg)
	\end{align}
	
	So the first divergence is between $\tilde{p}(\cdot 			\mid \context)$ and $\model(\cdot \mid \context)$, where I can only presume the first distribution is the true distribution and the second is the model distribution. 
	
	And then of course the true distribution just puts all the mass on the actually observed label so You just get negative log likelihood.
	
	Then We have
	
	\begin{align}
		\gamma(\context) = \frac{\lambda}{\#(\context)}
	\end{align}
	
	Okay so this is a weird mix of stuff He's up to here.
	
	He starts with the end.
	
	The equation is label smoothing with that $\gamma(\context)$ weighting factor, and He picks it such that minimum for the equation yields additive smoothing.
	
	So anyway then He goes on to say well let's look at the probability of a word given a context:
	
	\begin{align}
		\model(w|\context) \propto \frac{\# (\context \circ w)}{\#(\context)} + \frac{\lambda}{\#(\context) }
	\end{align}
	
	I kind of want to derive that though. 
	
	And derive it I did - see appendix.

\newpage
\section{Jelinek-Mercer}
	
	Idea is to interpolate between N-gram models.
	
	First We have the standard MLE estimate for unigram:
	
	\begin{align}
		p_{mle}(w_i) = \frac{c(w_i)}{\sum_{w_i} c(w_i)}
	\end{align}
	
	Or I guess to keep the notation consistent:
	
	\begin{align}
		p_{mle}(w_i) = \frac{\#(w_i)}{\sum_{w_i} \#(w_i)}
	\end{align}
	
	Then for the interpolated bigram estimate We have
	
	\begin{align}
		p_{inter}(w_i|w_{i-1}) = \lambda p_{mle}(w_i|w_{i-1}) + (1-\lambda)p_{mle}(w_i)
	\end{align}
			
	So a convex combination of the two maximum likelihood estimates.
	
	And then recursively for an n-gram model:
	
	\begin{align}
		p_{inter}(w_i|w_{i-n+1}) = \lambda_{w^{i-1}_{i-n+1}} p_{mle}(w_i|w^{i-1}_{i-n+1}) + (1-\lambda_{w^{i-1}_{i-n+1}})p_{inter}(w_i|w^{i-1}_{i-n+2})
	\end{align}
			
	It's so nice and simple lol.
	
	So first We have $\lambda_{w^{i-1}_{i-n+1}}$, so this lambda is conditioned on the previous $n-1$ words, so We can distinguish between $n^{|V|}$ different $\lambda$ terms, oof. 
	
	But so given one of those $\lambda$ terms We take the convex combination of the maximum likelihood estimate taking all the context into account, plus the interpolated probability with the context window shrunk by 1. 
	
	The recursion stops at either the unigram model, which can be smoothed using additive smoothing or something, or You can stop at a 0-th order model, which is just the uniform distribution over words.
	
	And then how choose the $\lambda$ terms and what does it all mean.
	
	Well depending on the $\lambda$ You weigh different n-gram models differently. I suppose the ones with the most frequency for this particular $w_i$ should get more weight, since they'll be based on more data.
	
	Either way, too many $\lambda$ terms to tune. Bucket by
	
	\begin{align}
		\sum_{w_i} c(w^i_{i-n+1})
	\end{align}
	
	That doesn't really illuminate it for me so \href{https://aclanthology.org/P96-1041.pdf}{this} seems better.
	
	The paper says partitioning is done in accordance to $c(w^{i-1}_{i-n+1})$, which is just the number of times that that context has appeared. Contexts that have appeared the same number of times get the same $\lambda$. Aight. 
	
	So now what. 
	
	\subsection{So now what}
	
		Well in ze previous derivation We started with a loss and went boiled it down to an expression in terms of counts.
		
		Now We have counts, and I suppose We are looking to go the other way. 
		
		I guess I should try doing that on the existing examples to get a handle on things:
		
	\subsection{From count to log likelihood}
	
		Okay so We start with the n-gram formulation:
		
		\begin{align}
			P(w_i) = \frac{\#(w_i)}{\sum_{w_i} \#(w_i)}
		\end{align}
		
		Then suppose that
		
		\begin{align}
			-\lambda = \sum_{w_i} \#(w_i)
		\end{align}
		
		Why is this right?
		
		Well, in both previous derivations, We used that
		
		\begin{align}
			P(w_i) = \frac{\circ}{-\lambda}
		\end{align}
		
		Where $\circ$ is unknown for the time being. I guess the idea is that at some point that langrange multiplier will be on the left side of the equation, multiplying the desired probability, and We'll have to divide by it to get it outta there. 
		
		Well, is the $\circ$ unknown? Kinda feels known lol. If You have the n-gram formulation, then You have $\circ$.
		
		So it's really 
	
	 	\begin{align}
			P(w_i) = \frac{\#(w_i)}{-\lambda}
		\end{align}
		
		Amazing progress.
		
		Aight so then We do know that $\lambda$ will be on it's own and that the whole thing will be equal to zero so
		
		\begin{align}
			P(w_i) &= \frac{\#(w_i)}{-\lambda}\\
			\frac{\#(w_i)}{P(w_i)} + \lambda &= 0\\
		\end{align}
		
		And then You know that $\lambda$ will be be multiplying a simple constraint but does that actually give You anything?
		
		You take the integral, You get 
		
		\begin{align}
			\#(w_i)\ln(P(w_i)) + \lambda P(w_i)
		\end{align}
		
		and then You know You got one of these for each derivative so then You sum and split out the constraint for clarity
		
		\begin{align}
			\bigg(\sum^m_{i=1} \#(w_i)\ln(P(w_i)) \bigg) + \lambda\bigg(\sum_{w_i} P(w_i)-1\bigg)
		\end{align}
		
		Then I guess You divide by the count of a particular occurence to get individual occurences and look for $ln(P(w_i))$ terms for that KL between truth and model.
		
	\subsection{Additive Smoothing}
	
		I think I'll omit the context 
		
		\begin{align}
			P(w_i|\context) = \frac{\#(w_i, \context) + \alpha}{\sum^{|V|}_{i=1} \#(w_i, \context) + |V|\alpha }
		\end{align}
			
		Where $|V|$ is the size of the vocabulary.
		
		So then let
		
		\begin{align}
			-\lambda = \sum^{|V|}_{i=1} \#(w_i, \context) + |V|\alpha
		\end{align}

		So then
		
		\begin{align}
			P(w_i|\context) &= \frac{\#(w_i, \context) + \alpha}{\sum^{|V|}_{i=1} \#(w_i, \context) + |V|\alpha }\\
			P(w_i|\context) &= \frac{\#(w_i, \context) + \alpha}{-\lambda}\\
			\frac{\#(w_i, \context) + \alpha}{P(w_i|\context)} + \lambda &= 0\\
		\end{align}
			
		Then by the integral We get
		
		\begin{align}
			\bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) + \alpha)\ln(P(w_i|\context)\bigg) + \lambda\bigg(\sum^{|V|}_{i=1} P(w_i|\context) \bigg)
		\end{align}
		
		Then We can ignore the constraint
		
		\begin{align}
			&\bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) + \alpha)\ln(P(w_i|\context)\bigg) + \lambda\bigg(\sum^{|V|}_{i=1} P(w_i|\context) \bigg)\\
			&\bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) + \alpha)\ln(P(w_i|\context)\bigg) \\
			& \bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) )\ln(P(w_i|\context) + \alpha\ln(P(w_i|\context)\bigg) 
		\end{align}
		
		And that is the total log probability. Then to focus on a particular word We focus on a particular $i$:
		
		\begin{align}
			& \bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) )\ln(P(w_i|\context) + \alpha\ln(P(w_i|\context)\bigg) \\
			& (\#(w_i, \context) )\ln(P(w_i|\context) + \alpha\ln(P(w_i|\context)
		\end{align}
		
	 	If that is the log likelihood then that count term is counting duplicates so
		
		\begin{align}
			& (\#(w_i, \context) )\ln(P(w_i|\context) + \alpha\ln(P(w_i|\context)\\
			& \ln(P(w_i|\context) + \frac{\alpha}{(\#(w_i, \context) )}\ln(P(w_i|\context)\\
		\end{align}
			
		Then add a constant which I guess We have floating around lol
			
		\begin{align}
			& \ln(P(w_i|\context) + \frac{\alpha}{(\#(w_i, \context) )}\ln(P(w_i|\context)\\
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\\
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{1}{P(w_i|\context)}\bigg) + \ln\bigg( \frac{\alpha}{(\#(w_i, \context) )} \bigg)\\
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{\alpha/(\#(w_i, \context) )}{P(w_i|\context)}\bigg) 
		\end{align}	
			
		So now the first term is then of course cross entropy or KL w.r.t. original distribution and the second, well, the second is dodgier.
		
		Depending on the value of $\alpha$ it's the uniform distribution? 	 
			
		What went wrong - did something go wrong? 
		
		Yeah, there should be a summation across all labels in the loss for any particular label. 
		
		Okay so here 
			
		\begin{align}
			\bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) + \alpha)\ln(P(w_i|\context)\bigg) + \lambda\bigg(\sum^{|V|}_{i=1} P(w_i|\context) \bigg)
		\end{align}
			
		Hmm. I feel like this is where We should have the fact that there is the same number of all-word terms for each word. 
		
		I guess the information is in there, still, it's just that We lost what $\alpha$ was? 'cause it wasn't just random. 
		
		\begin{align}
			\sum_{y_i\in\mathcal{Y}} \#(y_i, \context) +\#(\context)\gamma(\context)u(y) &= \lambda
		\end{align}
		
		Is what it was, and
		
		\begin{align}
			-\lambda = \sum^{|V|}_{i=1} \#(w_i, \context) + |V|\alpha
		\end{align}
		
		Whoop I think I found the error.
		
		So We had
		
		\begin{align}
			& \bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) )\ln(P(w_i|\context) + \alpha\ln(P(w_i|\context)\bigg) 
		\end{align}
		
		Okay so that was not the mistake.
		
		So what happens if We set 
		
		\begin{align}
			\gamma(\context) &= \frac{|V|\alpha}{\#(\context)}\\
			\gamma(\context)\#(\context) &= |V|\alpha\\
			\frac{\gamma(\context)\#(\context)}{|V|} &= \alpha
		\end{align}
		
		Plugging that in
		
		\begin{align}
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{\alpha/(\#(w_i, \context) )}{P(w_i|\context)}\bigg) \\
			& \ln(P(w_i|\context) - \frac{\frac{\gamma(\context)\#(\context)}{|V|}}{(\#(w_i, \context) )}\ln\bigg(\frac{\frac{\gamma(\context)\#(\context)}{|V|}/(\#(w_i, \context) )}{P(w_i|\context)}\bigg) \\
			& \ln(P(w_i|\context) - \frac{\gamma(\context)\#(\context)}{|V|(\#(w_i, \context) )}\ln\bigg(\frac{\gamma(\context)\#(\context)/|V|(\#(w_i, \context) )}{P(w_i|\context)}\bigg) 
		\end{align}
		
		Okay so I think something definitely gone wrong since I don't think I'm wait a god damn minute
		
		I'm getting the uniform distribution.... Over data items as opposed to labels? 
		
		\begin{align}
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{\alpha/(\#(w_i, \context) )}{P(w_i|\context)}\bigg) 
		\end{align}	
		
		Okay now let's think about summing this over all data items. Each data item..
		
		This is weird because it's only focusing on the correct label. 
		
		Can We scrounge together a uniform distribution if We look at the summation over all data items? 
		
		Yeah okay
		
		If We sum over all samples of a particular word, We'll get
		
		Okay so the idea at the edge of my mind is that the reason each term is normalized by $(\#(w_i, \context)$ is so that when We add together duplicates, they'll get the same share of the uniform distribution, regardless of how many duplicates there are. 
		
		That checks out. 
		
		Maybe fix it here? Instead of doing that
		
		\begin{align}
			& \ln(P(w_i|\context) + \frac{\alpha}{(\#(w_i, \context) )}\ln(P(w_i|\context)\\
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\\
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{1}{P(w_i|\context)}\bigg) + \ln\bigg( \frac{\alpha}{(\#(w_i, \context) )} \bigg)\\
			& \ln(P(w_i|\context) - \frac{\alpha}{(\#(w_i, \context) )}\ln\bigg(\frac{\alpha/(\#(w_i, \context) )}{P(w_i|\context)}\bigg) 
		\end{align}	
		
		Instead do
		
		\begin{align}
			& \ln(P(w_i|\context) + \frac{\alpha}{(\#(w_i, \context) )}\ln(P(w_i|\context)	 \\
			& \ln(P(w_i|\context) + \alpha\frac{1}{(\#(w_i, \context) )}\ln(P(w_i|\context)	 \\
			& \ln(P(w_i|\context) - \alpha\frac{1}{(\#(w_i, \context) )}\ln(\frac{1}{P(w_i|\context}) \\
			& \ln(P(w_i|\context) - \alpha\frac{1}{(\#(w_i, \context) )}\ln(\frac{1}{P(w_i|\context}) + \alpha\frac{1}{(\#(w_i, \context) )}\ln(1/\#(w_i, \context)) \\
			& \ln(P(w_i|\context) - \alpha\frac{1}{(\#(w_i, \context) )}\ln\bigg(\frac{1/(\#(w_i, \context) )}{P(w_i|\context}\bigg)
		\end{align}
		
		Sorta, but it doesn't address missing labels. Not a fan. 
		
		\subsubsection{Mistakes}
		
			Alright so is this line correct:
		
			\begin{align}
				\bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) + \alpha)\ln(P(w_i|\context)\bigg) + \lambda\bigg(\sum^{|V|}_{i=1} P(w_i|\context) - 1 \bigg)
			\end{align}
		
			In the original derivation it was
		
			\begin{align}
				&\bigg(\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot  \ln\bigg(\frac{1}{\model(y_i|\context)} 	\bigg)\bigg) \\
				&+ \#(\context)\gamma(\context)\bigg(\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context))\bigg) \\
				&+ \bigg(\lambda\sum_{y_l\in\mathcal{Y}} \model(y_l|\context) -1\bigg) 
			\end{align}	
		
			The signs are wonky, because I messed up the $\lambda$ sign. 
		
			\begin{align}
				&\bigg( \sum^{|V|}_{i=1} -(\#(w_i, \context) + \alpha)\ln(P(w_i|\context)\bigg) + \lambda\bigg(\sum^{|V|}_{i=1} P(w_i|\context) - 1 \bigg)\\
				&\bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) + \alpha)\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\bigg) + \lambda\bigg(\sum^{|V|}_{i=1} P(w_i|\context) - 1 \bigg)
			\end{align}
			
			Is what it would be if it were correct, I think lol. 
			
			Then drop the constraint 'cause who cares
			
			\begin{align}
				&\bigg( \sum^{|V|}_{i=1} (\#(w_i, \context) + \alpha)\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\bigg)\\
				&\bigg( \sum^{|V|}_{i=1} \#(w_i, \context)\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\bigg) + \bigg( \sum^{|V|}_{i=1} \alpha\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\bigg)
			\end{align}
			
			And now I suppose We try to do splitting by datapoint? 
			
			One datapoint gets one cross entropy term, which is okay.
			
			Then We need to think about $\alpha$ lol. Are there any constraint for what it should be set as? It just has to be above zero, so We can introduce any constant We'd like, for example
			
			\begin{align}
				\alpha = \frac{1}{|V|}\#(\context)\beta
			\end{align}
			
			Then
		
			\begin{align}
				&\sum^{|V|}_{i=1} \alpha\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\bigg)\\
				&\sum^{|V|}_{i=1} \frac{1}{|V|}\#(\context)\beta\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\bigg)\\
			\end{align}
			
			And now You need to think carefully about how to distribute this. You also need to add a constant lol The constant is:
		
			\begin{align}
				&\sum^{|V|}_{i=1} \frac{1}{|V|}\#(\context)\beta\ln\bigg(\frac{1}{P(w_i|\context)}\bigg)\bigg)\\
				&\bigg(\sum^{|V|}_{i=1} -\frac{1}{|V|}\#(\context)\beta\ln(P(w_i|\context))\bigg) + \sum^{|V|}_{i=1} \frac{1}{|V|}\#(\context)\beta\ln\bigg(\frac{1}{|V|}\bigg)\\
				&\bigg(\sum^{|V|}_{i=1} -\frac{1}{|V|}\#(\context)\beta\ln\bigg(\frac{1/|V|}{P(w_i|\context)}\bigg)\bigg)\\
			\end{align}
			
			And so now You use the count term to distribute this across all data items, split out the $\beta$ and for each data item then We get
			
			\begin{align}
				& \ln\bigg(\frac{1}{P(w_i|\context)}\bigg) + \beta\sum^{|V|}_{i=1} \frac{1}{|V|}\ln\bigg(\frac{1/|V|}{P(w_i|\context)}\bigg)\\
			\end{align}
		
			I think that's that. 
			
		
		\subsection{Deriving it I suppose}
		
			So, We had
			
			\begin{align}
				p_{inter}(w_i|w_{i-n+1}) = \lambda_{w^{i-1}_{i-n+1}} p_{mle}(w_i|w^{i-1}_{i-n+1}) + (1-\lambda_{w^{i-1}_{i-n+1}})p_{inter}(w_i|w^{i-1}_{i-n+2})
			\end{align}
		
			I guess I should just start with an interpolated bigram and see how I get on.
			
			I mean, it's just a sum of two MLE estimates:
			
			\begin{align}
				p_{inter}(w_i|w_{i-1}) = \alpha p_{mle}(w_i|w_{i-1}) + (1-\alpha)p_{mle}(w_i)
			\end{align}
		
			I feel like You can't just make the claim that KL divergence works here, but it's worth a shot. The objective then is:
			
			\begin{align}
				\sum^2_{n=1} \alpha_n \KL (\tilde{p}(\cdot | \context_{<n}) \mid\mid \model(\cdot | \context_{<2}))
			\end{align}
		
			Which kind of makes You go "hmm".
			
			I suppose the trivial thing to do would be to just train separate n-gram models and ensamble them. I guess they are not looking for that though lol.
			
			So for the ground truth We take the unigram and bigram observed samples, and in both cases We try to make our distribution more like those. Aight.
			
			Does that make sense to do? On the one hand We have count of contexts over count of instances, on the other yeah just some model. It might have access to the previous 2 tokens okay
			
			So given an $n$ gram model, it the divergences between short n-gram counts that act as smoothing here I suppose. Aight. 
			
			So then to peel away the KL for a particular observed token $(w_j, \context_{<n})$:
			
			\begin{align}
				& \alpha_n \KL (\tilde{p}(\cdot | \context_{<n}) \mid\mid \model(\cdot | \context_{<2}))\\
				& \alpha\sum^{|V|}_{i=1} \tilde{p}(w_i | \context_{<n})\ln\bigg( \frac{\tilde{p}(w_i | \context_{<n})}{\model(w_i | \context_{<2})} \bigg)\\
				& \alpha\ln\bigg( \frac{1}{\model(w_i | \context_{<2})} \bigg)
			\end{align}
			
			So the point again is for a particular observed token, only one head word matches, the rest have probability zero. 
			
			Summing over the different n-grams and the entire corpus then yields:
			
			\begin{align}
				&\sum^2_{n=1} \alpha_n \KL (\tilde{p}(\cdot | \context_{<n}) \mid\mid \model(\cdot | \context_{<2})) \\
				&= \sum^2_{n=1} \sum^{|V|}_{i=1} (\#(w_i, \context_{<n})\alpha_n \KL(\tilde{p}(\cdot | \context_{<n}) \mid\mid \model(\cdot | \context_{<2}))\\
				&= \sum^2_{n=1} \sum^{|V|}_{i=1} (\#(w_i, \context_{<n})\alpha_n \ln\bigg( \frac{1}{\model(w_i | \context_{<2})} \bigg)
			\end{align}
		
			Okay so We sum over both n-gram cases, and then sum over all words in the vocabulary. That's fine.
			
			Then for each word in the vocabulary We'll have the number of times it occurs with the appropriate length context. Does that make sense? Well as We sum over all the words, We'll go over each token once this way, so that squares. 
			
			Then the appropriate $\alpha$ for the $n$ count and We're good, I think.
			
			Should the indexing be simplified? Seems like maybe so, or at least I think it makes sense to move the outer sum in.
			
		
			
			\begin{align}
				&= \sum^2_{n=1} \sum^{|V|}_{i=1} (\#(w_i, \context_{<n})\alpha_n \ln\bigg( \frac{1}{\model(w_i | \context_{<2})} \bigg)\\
				&= \sum^{|V|}_{i=1}  \sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n \ln\bigg( \frac{1}{\model(w_i | \context_{<2})} \bigg)
			\end{align}
			
			So then I suppose it's on to Lagrange
			
			\begin{align}
				\mathcal{L} &= \bigg(\sum^{|V|}_{i=1}  \sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n \ln\bigg( \frac{1}{\model(w_i | \context_{<2})} \bigg)\bigg) + \lambda\bigg(\sum^{|V|}_{j=1} \model(w_i|\context{<2})-1 \bigg)\\
				\mathcal{L} &= \bigg(\sum^{|V|}_{i=1}  \sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n \ln( \model(w_i | \context_{<2}))\bigg) + \lambda\bigg(\sum^{|V|}_{j=1} \model(w_i|\context{<2})-1 \bigg)
			\end{align}
			
			Then derivative w.r.t. some $\model(w_m | \context_{<2})$
			
			\begin{align}
				\sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n \frac{1}{\model(w_m | \context_{<2})} + \lambda &= 0\\
				\sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n \frac{1}{\model(w_m | \context_{<2})}  &= -\lambda\\
				\frac{\sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n}{\model(w_m | \context_{<2})}  &= -\lambda\\
				\frac{\model(w_m | \context_{<2})}{\sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n}  &= -\frac{1}{\lambda}\\
				\model(w_m | \context_{<2})  &= -\frac{\sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n}{\lambda}
			\end{align}
			
			Then back to the constraint:
			
			\begin{align}
				\sum^{|V|}_{i=1} \model(w_i|\context_{<2}) &= 1\\
				\sum^{|V|}_{i=1} -\frac{\sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n}{\lambda} &= 1\\
				 -\frac{ \sum^{|V|}_{i=1} \sum^2_{n=1} -(\#(w_i, \context_{<n})\alpha_n}{\lambda} &= 1\\
				  \sum^{|V|}_{i=1} \sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n &= \lambda
			\end{align}
			
			Then back again:
		
			\begin{align}
				\model(w_m | \context_{<2})  &= \frac{\sum^2_{n=1}  (\#(w_i, \context_{<n})\alpha_n}{\lambda}\\
				\model(w_m | \context_{<2})  &= \frac{\sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^{|V|}_{i=1} \sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}
			\end{align}
			
			And then it's just simplification I suppose.
			
			\begin{align}
				\model(w_m | \context_{<2})  &= \frac{\sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^{|V|}_{i=1} \sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}\\
				&= \frac{\sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^2_{n=1} \sum^{|V|}_{i=1} (\#(w_i, \context_{<n})\alpha_n}\\
				&= \frac{\sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^2_{n=1} (\#(\context_{<n})\alpha_n}		
			\end{align}
		
			Yep, that's a weighted sum lol. 
			
			I suppose I ought to sanity check this
		
			\begin{align}
				&= \frac{\sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^2_{n=1} (\#(\context_{<n})\alpha_n}\\
				&= \sum^{|V|}_{i=1}\frac{\sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^2_{n=1} (\#(\context_{<n})\alpha_n}\\
				&= \frac{ \sum^{|V|}_{i=1} \sum^2_{n=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^2_{n=1} (\#(\context_{<n})\alpha_n}\\
				&= \frac{  \sum^2_{n=1} \sum^{|V|}_{i=1} (\#(w_i, \context_{<n})\alpha_n}{\sum^2_{n=1} (\#(\context_{<n})\alpha_n}\\
				&= \frac{  \sum^2_{n=1} (\#(\context_{<n})\alpha_n}{\sum^2_{n=1} (\#(\context_{<n})\alpha_n} = 1
			\end{align}
		
			Yeet.
			
			Oh yeah I'd like a justification for the starting line - which I guess is not difficult to do. 
			
		\subsection{General and hopefully pretty case}
		
			So We consider all tokens up to size $N$, and our model is aware of a fixed size context $\context_N$
			
			\begin{align}
				\sum^N_{n=1} \alpha_n \KL (\tilde{p}(\cdot | \context_{<n}) \mid\mid \model(\cdot | \context_{<N}))
			\end{align}
			
			Consider a particular token has been observed: $(w_j, \context)$. Removing the KL notation:
			
			\begin{align}
				\KL (\tilde{p}(\cdot | \context_{<n}) \mid\mid \model(\cdot | \context_{<N})) &= \sum^{|V|}_{i=1} \tilde{p}(w_i | \context_{<n})\ln\bigg(\frac{\tilde{p}(w_i | \context_{<n})}{\model(w_i| \context_{<N})}\bigg)\\
				&= \sum^{|V|}_{i=1} \mathbbm{1}_{\{w_i = w_j\}} \ln\bigg(\frac{\mathbbm{1}_{\{w_i = w_j\}}}{\model(w_i| \context_{<N})}\bigg)\\
				&= \ln\bigg(\frac{1}{\model(w_j| \context_{<N})}\bigg)\\
				&= -\ln(\model(w_j| \context_{<N}))		
			\end{align}
			
			Summing over the KL terms yields
			
			\begin{align}
				\sum^N_{n=1}  -\alpha_n \ln(\model(w_j| \context_{<N}))		
			\end{align}
						
			Then summing over the entire vocabulary and using the count function to account for duplicates:
		
			\begin{align}
				\sum^{|V|}_{i=1} \sum^N_{n=1}  -(\#(w_i, \context_{<n})\alpha_n \ln(\model(w_j| \context_{<N}))		
			\end{align}
			
			Then for the Lagrange multiplier:
			
			\begin{align}
				\mathcal{L} = \bigg(\sum^{|V|}_{i=1} \sum^N_{n=1}  -(\#(w_i, \context_{<n})\alpha_n \ln(\model(w_j| \context_{<N}))	\bigg) + \lambda\sum^{|V|}_{i=1} \model(w_i|\context_{<N}) - 1	
			\end{align}
			
			Then taking the derivative w.r.t. a particular $\model(w_m|\context_{<N})$:
			
			\begin{align}
				\bigg(\sum^N_{n=1}  -(\#(w_m, \context_{<n})\alpha_n \frac{1}{\model(w_m| \context_{<N})}	\bigg) + \lambda &= 0\\
				\sum^N_{n=1} (\#(w_m, \context_{<n})\alpha_n \frac{1}{\model(w_m| \context_{<N})}  &= \lambda	\\
				\frac{\sum^N_{n=1} \#(w_m, \context_{<n})\alpha_n}{\model(w_m| \context_{<N})}  &= \lambda\\
				\frac{\model(w_m| \context_{<N})}{\sum^N_{n=1} \#(w_m, \context_{<n})\alpha_n}  &= \frac{1}{\lambda}\\
				\model(w_m| \context_{<N})  &= \frac{\sum^N_{n=1} \#(w_m, \context_{<n})\alpha_n}{\lambda}
			\end{align}
			
			Then using the above and our constraint:
			
			\begin{align}
				\sum^{|V|}_{i=1} \model(w_i|\context_{<N}) &= 1\\
				\sum^{|V|}_{i=1} \frac{\sum^N_{n=1} \#(w_m, \context_{<n})\alpha_n}{\lambda} &= 1\\
				\sum^{|V|}_{i=1} \sum^N_{n=1} \#(w_m, \context_{<n})\alpha_n &= \lambda
			\end{align}
			
			Plugging this $\lambda$ into the previous expression for $\model(w_m| \context_{<N})$:
			
			\begin{align}
				\model(w_m| \context_{<N})  &= \frac{\sum^N_{n=1} \#(w_m, \context_{<n})\alpha_n}{\lambda}\\
				&= \frac{\sum^N_{n=1} (\#(w_m, \context_{<n})\alpha_n}{\sum^{|V|}_{i=1} \sum^N_{n=1} \#(w_m, \context_{<n})\alpha_n}
			\end{align}
			
			Which looks about right.
			
\newpage		
\section{Katz-Backoff}

	Okay so real slowly:
	
	If the count is non-null, We have
	
	\begin{align}
		P_{katz}(w_i|w^{i-1}_{i-N+1}) = P^*(w_n|w^{n-1}_{n-N+1})
	\end{align}
	
	All that says it's not just straight probability, which, like, no kidding.
	
	If the count \textit{is} zero then
	
	\begin{align}
		P_{katz}(w_i|w^{i-1}_{i-N+1}) = \alpha(w^{n-1}_{n-N+1})P_{katz}(w_n|w^{n-1}_{n-N+2})
	\end{align}
	
	So if the count is zero We get some customed tailored coefficient and a probability which takes into account one less context word. 
	
	The same awkwardness exists in this formulation as before - it's in terms of absolute terms instead of variables. 
	
	\begin{align}
		P_{katz}(w_i|w^{i-1}_{i-N+1}) &= \alpha(w^{n-1}_{n-N+1})P_{katz}(w_n|w^{n-1}_{n-N+2})\\
		P_{katz}(w_i|w^{a}_{b}) &= \alpha(w^{a}_{b})P_{katz}(w_n|w^{a}_{b+1})
	\end{align}	
	
	The idea then is to define a function to keep track of the available probability mass:
	
	\begin{align}
		\beta(w^{n-1}_{n-N+1}) = 1 - \sum_{w_n:\#(w^n_{n-N+1})>0} P^*(w_n|w^{n-1}_{n-N+1})
	\end{align}
	
	So the available mass given a context is the total mass, sure, minus the sum of probabilities of words with non-zero count. One thing that bothers me is that each individual word gets it's own probability mass. I guess We are just looking for the probability of $UNK$, so, sure.
	
	There's a problem that I'm having which is I'm not sure what it is that We are counting. Like, tokens, sure but so here is my problem:
	
	It sounds like We are giving probability mass to each unknown new word, but the number of such words is infinite, so We're inventing an infinite amount of mass which like no.
	
	Okay so I think this is a fixed size vocabulary? Yep. And You just count words that don't have a history. 
	
	Then You take that available mass and distribute it - that's the $\alpha^{n-1}_{n-N+1}$, so
	
	\begin{align}
		\alpha^{n-1}_{n-N+1} = \frac{\beta(w^{n-1}_{n-N+1})}{\sum_{w_i:\#(w^n_{n-N+1})=0} P_{katz}(w_n|w^{n-1}_{n-N+2})}
	\end{align}
		
	So that's fine - the denominator is just the sum of Katz probabilities of tokens with zero count
		
	Instead of calculating the sum of the probability mass of the tokens with zero count, You can take the total probability mass (1), and subtract all the stuff that has a non-zero count to leave You with the mass for the zero count terms:
		
	\begin{align}
		\alpha^{n-1}_{n-N+1} &= \frac{\beta(w^{n-1}_{n-N+1})}{\sum_{w_i:\#(w^n_{n-N+1})\color{red}{=0}} P_{katz}(w_n|w^{n-1}_{n-N+2})}\\
		&= \frac{\beta(w^{n-1}_{n-N+1})}{1-\sum_{w_i:\#(w^n_{n-N+1})\color{red}{>}0} P_{katz}(w_n|w^{n-1}_{n-N+2})}
	\end{align}	
		
	Seems fair.
	
	So now it's just $P^*$ and how to calculate that. 
	
	\subsection{Another attempt due to bad notation lol}
	
		Source can be found \href{https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf}{here}.
		
		So, first We define modified counts:
		
		\begin{align}
			\#_{katz}(w^n_{n-N+1}) = d_r\cdot \#(w^n_{n-N+1})
		\end{align}
		
		So in the case of a non-zero count, We take the true count $\#_{katz}(w^n_{n-N+1})$ and discount it by some factor $d_r$, which $d_r$ depends on the number of times the token in question has occurred.
		
		If the count of a particular token is zero, then
		
		\begin{align}
			\#_{katz}(w^n_{n-N+1}) = \alpha(w^n_{n-N+1})p_{katz}(w^n_{n-N+2})
		\end{align}
		
		So We have that $\alpha$ term that will split the probability among tokens with zero count, and We have that $p_{katz}$ that We'll need to define. 
		
		The $\alpha$ here is also a bit different - in the previous formulation We thought about probabilities, but now We are thinking about counts. All the same in the end I guess, but so now the point of the $\alpha$ is to ensure that the overall count remains unchanged - the number of tokens is the same when using the Katz counting scheme.
		
		\begin{align}
			\alpha(w^n_{n-N+1}) = \frac{1 - \sum_{w_n:\#(w^n_{n-N+1})>0} p_{katz}(w_n|w^{n-1}_{n-N+1})}{\sum_{w_n:\#(w^n_{n-N+1})=0} p_{katz}(w^n|w^{n-1}_{n-N+2})}
		\end{align}
		
		What a mess.
		
		Okay so on top We have the total probability mass minus the mass term for non zero counts so then We just have the mass reserved for zero count words, sure.
		
		Now if We just gave each word a constant weight, I think We'd just recover Good-Turing.
		
		But instead We are going to slice up that mass in accordance to the weighting dictated by the $n-1$-gram model.
		
		We know what those weights are going to be so We calculate up a normalizer. With You so far.
		
		So now for the actual probability of a zero-count token, You'll do
		
		\begin{align}
			&p_{katz}(w^n|w^{n-1}_{n-N+1}) \\
			&= \frac{1 - \sum_{w_n:\#(w^n_{n-N+1})>0} p_{katz}(w_n|w^{n-1}_{n-N+1})}{\sum_{w_n:\#(w^n_{n-N+1})=0} p_{katz}(w^n|w^{n-1}_{n-N+2})} \times p_{katz}(w^n|w^{n-1}_{n-N+2})
		\end{align}
		
		kay. And now also observe that
		
		\begin{align}
			p_{katz}(w^n|w^{n-1}_{n-N+1}) = \frac{\#_{katz}(w^n_{n-N+1})}{\sum_{w\in|V|} \#(w_i, w^{n-1}_{n-N+1})}
		\end{align}
		
		So then in total, for an unobserved token We have 
		
		\begin{align}
			&p_{katz}(w^n|w^{n-1}_{n-N+1}) \\
			&= \frac{\frac{1 - \sum_{w_n:\#(w^n_{n-N+1})>0} p_{katz}(w_n|w^{n-1}_{n-N+1})}{\sum_{w_n:\#(w^n_{n-N+1})=0} p_{katz}(w^n|w^{n-1}_{n-N+2})} \times p_{katz}(w^n|w^{n-1}_{n-N+2})}{\sum_{w\in|V|} \#(w_i, w^{n-1}_{n-N+1})}
		\end{align}
		
		Isn't it beautiful?
		
		Okay so I think that their formulation is actually wrong. Let's see if I can prove this:
		
	\subsection{Mistakes?}
		
		Kay so the point is that obviously this must hold:
		
		\begin{align}
			\sum_{w_i\in V} P(w_i|w^n_{n-N+1}) = 1
		\end{align}
		
		Now We can split this into two cases - zero and non-zero.
		
		In the non-zero cases We have
		
		\begin{align}
			P_{katz}(w_i|w^n_{n-N+1}) &= \frac{\#_{katz}(w_i, w^n_{n-N+1})}{\#(w^n_{n-N+1})}\\
			\sum_{w_i:\#(w_i, w^n_{n-N+1}>0)}P_{katz}(w_i|w^n_{n-N+1}) &= \sum_{w_i:\#(w_i, w^n_{n-N+1}>0)}\frac{\#_{katz}(w_i, w^n_{n-N+1})}{\#(w^n_{n-N+1})}
		\end{align}
		
		Sure yeah.
		
		In the zero case We have 
		
		\begin{align}
			&= \frac{\frac{1 - \sum_{w_n:\#(w^n_{n-N+1})>0} p_{katz}(w_n|w^{n-1}_{n-N+1})}{\sum_{w_n:\#(w^n_{n-N+1})=0} p_{katz}(w^n|w^{n-1}_{n-N+2})} \times p_{katz}(w^n|w^{n-1}_{n-N+2})}{\sum_{w\in|V|} \#(w_i, w^{n-1}_{n-N+1})}
		\end{align}
		
		Everything is divided by the total count so let's multiply it out 
		
		\begin{align}
			\sum_{w_i:\#(w_i, w^n_{n-N+1}>0)}P_{katz}(w_i|w^n_{n-N+1}) &= \sum_{w_i:\#(w_i, w^n_{n-N+1}>0)}\frac{\#_{katz}(w_i, w^n_{n-N+1})}{\#(w^n_{n-N+1})}\\
			\#(w^n_{n-N+1})P_{katz}(w_i|w^n_{n-N+1}) &= \#_{katz}(w_i, w^n_{n-N+1})
		\end{align}
		
		Dropped the sum term 'cause it's taking up tons of space.
		
		So now if We sum over those terms like We are supposed to We'll get the sum of the Katz counts, sure.
		
		If We do the same for the zero count case, as in sum over all of them
		
		\begin{align}
			&\frac{1 - \sum_{w_n:\#(w^n_{n-N+1})>0} p_{katz}(w_n|w^{n-1}_{n-N+1})}{\sum_{w_n:\#(w^n_{n-N+1})=0} p_{katz}(w^n|w^{n-1}_{n-N+2})} \times p_{katz}(w^n|w^{n-1}_{n-N+2})
		\end{align}
		
		Yeah when You sum the above over all tokens that have zero count You'll just get 
		
		\begin{align}
			1 - \sum_{w_n:\#(w^n_{n-N+1})>0} p_{katz}(w_n|w^{n-1}_{n-N+1})
		\end{align}
		
		By definition, which ain't no God damn count, it's a probability, so this paper fucked it.
		
	\subsection{Bad-Turing lol}
	
		So I guess I ought to do the Turing bit, since it's going to be used by Katz anyway, so.
		
		We're back to this paper by the by \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=476512&tag=1}{link}.
		
		So $N$ observed token instances, $K$ types.
		
		Partition by $n_r$ such that
		
		\begin{align}
			n_r &= \sum_{k:N(k)=r} 1\\
			\sum^{R}_{r=0} n_r  r &= N\\
			\sum^R_{r=0} n_r p_r &= 1
		\end{align}
		
		So $n_r$ is just the number of types that have occurred $r$ times, summing over the partitions and multiplying by their frequencies gets You the total count which makes sense, and each member of a partition gets some probability mass, with each partition having $n_r$ members so yeah, sure.
		
		Then divide a corpus into holdout and training sets.
		
		The training part sets up the partition - this is where We count how much of each token occurs, thereby partitioning the tokens. Training is the set up. Alright.
		
		The first thing the holdout set is used or is to calculate $C_r$ - $C_2$ for example is the number of words in the holdout set that belong to $n_2$ in the training set. 
		
		So as far as I can see it's the holdout set that dictates what We are trying to maximize as an objective:
		
		\begin{align}
			G(p_0, p_1\ldots p_R) &= \prod^R_{r=1} p_r^{C_r}\\
			\ln(G(p_0, p_1\ldots p_R)) &= \sum^R_{r=1} C_r\ln(p_r)\\
			F(p_0, p_1\ldots p_R) &= \sum^R_{r=1} C_r\ln(p_r)
		\end{align}
		
		Okay and then You want the probabilities to sum to one so:
		
		\begin{align}
			\mathcal{L} &= \bigg(\sum^R_{r=1} C_r\ln(p_r)\bigg) + \lambda \bigg(\bigg(\sum^R_{r=0} n_r p_r\bigg)-1\bigg)
		\end{align}
		
		So a bit of confusion is that $n_r$ thing, but I mean what else could You do? It's the training set that dictates the partitions, so $n_r$ kind of has to be in there.
		
		So then We take the derivative w.r.t. some particular $p_j$ I suppose:
		
		\begin{align}
			\frac{\partial}{\partial p_j} \sum^R_{r=1} C_r\ln(p_r) &= \frac{C_j}{p_j}
		\end{align}
		
		So that's fine and
		
		\begin{align}
			\frac{\partial}{\partial p_j} \lambda \bigg(\bigg(\sum^R_{r=0} n_r p_r\bigg)-1\bigg) &= \lambda n_j
		\end{align}
		
		Okay cool so We have $R$ equations of the form
		
		\begin{align}
			\frac{C_j}{p_j} + \lambda n_j &= 0\\
			\frac{C_j}{p_j} &= - \lambda n_j \\
			\frac{p_j}{C_j} &= - \frac{1}{\lambda n_j} \\
			p_j &= - \frac{C_j}{\lambda n_j} 
		\end{align}
		
		Then
		
		\begin{align}
			\sum^R_{r=1} n_r p_r &= 1\\
			-\sum^R_{r=1} n_r \frac{C_r}{\lambda n_r} &= 1\\
			-\sum^R_{r=1} \frac{C_r}{\lambda} &= 1\\
			-\sum^R_{r=1} C_r &= \lambda
		\end{align}
		
		So then the optimal value for the probability is:
		
		\begin{align}
			p_j &= - \frac{C_j}{\lambda n_j}\\
			&= - \frac{C_j}{n_j \bigg(-\sum^R_{r=0} C_r\bigg)}\\
			&= \frac{C_j}{n_j \bigg(\sum^R_{r=0} C_r\bigg)} \\
			&= \frac{C_j}{n_j}\frac{1}{\sum^R_{r=0} C_r} 
		\end{align}
		
		So that's the setup.
		
		Now the idea is to do Leave One Out Cross Validation?
		
		You're going to generate a whole lot of holdout sets, for some set $C$, it'll have one item, and the training will therefore have $N-1$ tokens, and though the original count in the whole set for this particular holdout item was $r+1$, after being held out it's now $r$.
		
		So then in total, across holdout sets, it'll be $(r+1)n_{r+1}$ number of tokens that will belong to the $p_r$ partition in the training set.
		
		\begin{align}
			\ln\bigg(\prod^{R-1}_{r=0} p_r^{n_{r+1}(r+1)} \bigg) &= \sum_{r=0}^{R-1} (r+1)n_{r+1}\ln(p_r)
		\end{align}
		
		So points of confusion - the indexing now starts at $r=0$ because there will be tokens in the holdout set that occur 0 times in the training set. No worries though, right? You'd right $p_0=0$, but there's no like law about that. The objective is dictated by the holdout set, and there are things in the holdout set that are not in the training set, and now You just get to optimize and see what ya get.
		
		But so then the whole shebang is:
		
		\begin{align}
			\mathcal{L} &= \bigg(\sum_{r=0}^{R-1} (r+1)n_{r+1}\ln(p_r)\bigg) +  \lambda \bigg(\bigg(\sum^R_{r=0} n_r p_r\bigg)-1\bigg)
		\end{align}
		
		Why does the constraint start at 0? I see why $p_0$ exists, and I suppose $n_0$ can then be thought of as the number types that have probability $p_0$.
		
		Okay so I think this is that thing again of different training sets and different holdout sets playing together. 
		
		If You have some dictionary, then $n_0$ is just the number of words that do not appear in the observed corpus.
		
		Like We had $\sum_{r=1}^R n_r r = N$ but if You include the words that did not occur in the corpus then yeah of course Your sum will be at least $N$.
		
		This makes sense in the objective equation - it's got $p_0$ in there since if held out item can occur 0 times, and there will be $n_1$ such occurences, so You assign to $p_0$ in whatever way You have to to maximize the likelihood.
		
		But then also the probability mass of an equivalence class is distributed evenly to it's members, so the constraint has $n_0$ from the perspective of the vocabulary.
		
		
		
		Then the derivatives:
		
		\begin{align}
			\frac{\partial}{\partial p_j} \sum_{r=0}^{R-1} (r+1)n_{r+1}\ln(p_r) = \frac{(j+1)n_{j+1}}{p_j}
		\end{align}
		
		and then
		
		\begin{align}
			\frac{\partial}{\partial p_j} \lambda \bigg(\bigg(\sum^R_{r=0} n_r p_r\bigg)-1\bigg) &= \lambda n_j
		\end{align}
		
		So putting those together:
		
		\begin{align}
			\frac{(j+1)n_{j+1}}{p_j} + \lambda n_j &= 0\\
			\frac{(j+1)n_{j+1}}{p_j}  &= - \lambda n_j\\
			\frac{p_j}{(j+1)n_{j+1}}  &= - \frac{1}{\lambda n_j}\\
			p_j  &= - \frac{(j+1)n_{j+1}}{\lambda n_j}
		\end{align}
		
		Then the constraint:
		
		\begin{align}
			\sum^R_{r=0} n_r p_r &= 1\\
			-\sum^R_{r=0} n_r \frac{(r+1)n_{r+1}}{\lambda n_r} &= 1\\
			-\sum^R_{r=0}  (r+1)n_{r+1} &= \lambda \\
		\end{align}
		
		Then back again:
		
		\begin{align}
			p_j  &= - \frac{(j+1)n_{j+1}}{\lambda n_j}\\
			p_j  &= - \frac{(j+1)n_{j+1}}{\bigg( -\sum^R_{r=0}  (r+1)n_{r+1} \bigg) n_j}\\
			p_j  &= \frac{(j+1)n_{j+1}}{\bigg(\sum^R_{r=0}  (r+1)n_{r+1} \bigg) n_j}\\
			p_j  &= \frac{(j+1)n_{j+1}}{N n_j}\\
			p_j  &= \frac{(j+1)n_{j+1}}{n_j}\frac{1}{N }
		\end{align}
		
		
		Something's gone amiss.
		
		So it's true that 
		
		\begin{align}
			p_j  &= - \frac{(j+1)n_{j+1}}{\lambda n_j}\\
			&= - \frac{1}{\lambda}\frac{(j+1)n_{j+1}}{n_j}
		\end{align}
		
		And then We stick that into the constraint
		
		\begin{align}
			\sum^R_{r=0} n_r p_r &= 1\\
			-\sum^R_{r=0} n_r \frac{1}{\lambda}\frac{(r+1)n_{r+1}}{n_r} &= 1\\
			-\sum^R_{r=0} n_r \frac{(r+1)n_{r+1}}{n_r} &= \lambda\\
		\end{align}
		
		Okay I think this may have to do with the fact that We have constraints for $R-1$ of the tokens in the lagrange formulation.
		
		\subsubsection{Mistakes}
		
			Alright so We have 
		
			\begin{align}
				\frac{\partial}{\partial p_j} \sum_{r=0}^{R-1} (r+1)n_{r+1}\ln(p_r) = \frac{(j+1)n_{j+1}}{p_j}
			\end{align}
			
			and then
			
			\begin{align}
				\frac{\partial}{\partial p_j} \lambda \bigg(\bigg(\sum^R_{r=0} n_r p_r\bigg)-1\bigg) &= \lambda n_j
			\end{align}
			
			Where the first derivative gives us expressions for $p_0\ldots p_{R-1}$, and the second derivative is true across all bits.
			
			We do still have this
			
			\begin{align}
				p_j  &= - \frac{(j+1)n_{j+1}}{\lambda n_j}\\
				&= - \frac{1}{\lambda}\frac{(j+1)n_{j+1}}{n_j}
			\end{align}
			
			But only for $j=0\ldots R-1$. We also have
			
			\begin{align}
				\lambda n_R = 0
			\end{align}
			
			Which, uhhhhh lol.
			
			I guess they step around this by making the function a function of $p_0\ldots p_{R-1}$.
			
			But so then
		
			\begin{align}
				\sum^R_{r=0} n_r p_r &= 1\\
				n_Rp_R + \sum^{R-1}_{r=0} n_r p_r &= 1\\
				\sum^{R-1}_{r=0} n_r p_r &= 1-n_R p_R\\
				-\sum^{R-1}_{r=0} n_r \frac{1}{\lambda}\frac{(r+1)n_{r+1}}{n_r} &= 1-n_R p_R\\
				-\sum^{R-1}_{r=0} (r+1)n_{r+1} &= \lambda(1-n_R p_R)\\
				-N &= \lambda(1-n_R p_R)\\
				- &= \lambda\\
			\end{align}
		
			Yeah and so then 
		
			\begin{align}
				p_j  &= - \frac{(j+1)n_{j+1}}{\lambda n_j}\\
				&= - \frac{1}{\lambda}\frac{(j+1)n_{j+1}}{n_j}\\
				&= \frac{1-n_R p_R}{N}\frac{(j+1)n_{j+1}}{n_j}
			\end{align}
			
			
		\subsubsection{Using standard notation}
			
			Consider a set of $N$ tokens, with $K$ different types of tokens present in the corpus. Now create $N$ pairs of holdout and training sets by holding out one token at a time. The joint log likelihood of the holdout sets is
			
			\begin{align}
				\ln\bigg(\prod^{R-1}_{r=0} p_r^{n_{r+1}(r+1)} \bigg) &= \sum_{r=0}^{R-1} (r+1)n_{r+1}\ln(p_r)
			\end{align}
		
			Using notation from \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=476512&tag=1}{this} paper, let $r+1$ be the number of times a token occurred in the full size corpus, then if that type of token is in the holdout set, it must only occur $r$ times in the training set. Therefore, from the perspective of the training set, the held out token belongs to the $p_r$ partition, and there will be $n_r(r+1)$ such cases among all the holdout sets, where $n_r$ is the number of tokens which appear with frequency $r$.
			
			The normalizing constraint is:
			
			\begin{align}
				\sum^R_{r=0} n_r p_r &= 1
			\end{align}
			
			And it's important to note that $p_0n_0$ make an appearance since the likelihood of the holdout sets will include tokens that never occur from the perspective of the associated training set, namely tokens that occur only once in the complete corpus. 
		
			\begin{align}
				\mathcal{L}(p_0\ldots p_{R-1}) &= \bigg( \sum_{r=0}^{R-1} (r+1)n_{r+1}\ln(p_r)\bigg) - \lambda\bigg(\bigg(\sum^R_{r=0} n_r p_r)\bigg) -1 \bigg)
			\end{align}
			
			Then taking the derivative
			
			\begin{align}
				\frac{\partial}{\partial p_j} \sum_{r=0}^{R-1} (r+1)n_{r+1}\ln(p_r) = \frac{(j+1)n_{j+1}}{p_j}
			\end{align}
			
			and
			
			\begin{align}
				\frac{\partial}{\partial p_j} \lambda \bigg(\bigg(\sum^R_{r=0} n_r p_r\bigg)-1\bigg) &= \lambda n_j
			\end{align}
			
			Putting those back together yields
			
			\begin{align}
				\frac{(j+1)n_{j+1}}{p_j} - \lambda n_j &= 0\\
				p_j  &= -\frac{1}{\lambda}\frac{(j+1)n_{j+1}}{n_j}\qquad \text{For }j\in\{0\ldots R-1\}
			\end{align}
			
			Then using plugging the above into the constraint (keeping track of the fact that We only have equations for $p_0\ldots p_{R-1}$:
			
			\begin{align}
				\sum^R_{r=0} n_r p_r &= 1\\
				n_Rp_R + \sum^{R-1}_{r=0} n_r p_r &= 1\\
				\sum^{R-1}_{r=0} n_r p_r &= 1-n_R p_R\\
				-\sum^{R-1}_{r=0} n_r \frac{1}{\lambda}\frac{(r+1)n_{r+1}}{n_r} &= 1-n_R p_R\\
				-\sum^{R-1}_{r=0} (r+1)n_{r+1} &= \lambda(1-n_R p_R)\\
				-N &= \lambda(1-n_R p_R)\\	
				-\frac{N}{(1-n_R p_R)} &= \lambda		
			\end{align}
			
			Using this yields our probabilities:
			
			\begin{align}
				p_j  &= -\frac{1}{\lambda}\frac{(j+1)n_{j+1}}{n_j}\\
				&= \frac{(1-n_R p_R)}{N}\frac{(j+1)n_{j+1}}{n_j} \qquad \text{For }j\in\{0\ldots R-1\}
			\end{align}
			
			Since $n_Rp_R$ is the number of words that occur frequently times the probability of a word on the frequent partition, it's usually close to 0. Ignoring the term yields the Good-Turing estimate:
			
			\begin{align}
				p_j  &= \frac{1}{N}\frac{(j+1)n_{j+1}}{n_j} \qquad \text{For }j\in\{0\ldots R-1\}
			\end{align}
		
		\subsubsection{Mistakes??}
		
			Is there a way to get rid of that $n_Rp_R$ term?
			
			Well why is it there.
			
			I feel like this is double counting somehow. 
			
			Okay so We are expressing the probability of the held out set in terms of partitions. 
			
			In a particular example We yoink out some token, there were $r+1$ tokens to begin with, now there are $r$ in the training set, so the probability of this token is $p_r$. I'm okay with that I think.
			
			What about the most frequent tokens?
			
			In that case $r+1=R$ so the term in the log likelihood sum is $Rn_R\ln(p_{R-1})$.
			
			
	\subsection{Katz!}
	
		Parts of Katz:
		
		Discounting according to Good-Turing to save some probability mass. Call the saved mass $S$.
		
		Good-Turing divides the mass evenly among the classes, Katz does it unevenly, in accordance to a smaller n-gram model.
		
		Trouble is that those weights won't sum to one, so, We normalize the probabilities.
		
		So it's just a wonky Good-Turing.
		
		Which means I need a KL version for Good-Turing. 
		
		
	\subsection{Not Katz lol, KL-Turing}
	
		How does that even work.
		
		Well if I could factor the Good-Turing thing into a straight forward maximum likelihood plus some offset that would be great lol
		
		\begin{align}
			p_j^{GT}  &= \frac{1}{N}\frac{(j+1)n_{j+1}}{n_j} \qquad \text{For }j\in\{0\ldots R-1\}
		\end{align}
		
		That there is the probability of a particular type belonging to partition $j$.
		
		In that notation in standard maximum likelihood You'd get
		
		\begin{align}
			p^{MLE}_j = \frac{j}{N}
		\end{align}
		
		Hmm lol
		
		\begin{align}
			p_j^{GT}  &= \frac{1}{N}\frac{(j+1)n_{j+1}}{n_j}\\
			&= \frac{j}{N}\frac{ n_{j+1}}{n_j}+\frac{1}{N}\frac{n_{j+1}}{n_j}
		\end{align}
		
		Okay so We do have the MLE estimate in there lol, it's just that it's scaled on a per-item basis, so I can't just like put a multiplicative term in front a KL term lol.
		
		Taking the KL between the model and the true distribution will result in $j/N$.
		
		So, recall, what We'll have is a KL thing per observed token.
		
		And for each token in the MLE case We had just the KL between the observed and the model distribution I think.
		
		What do We need to get to this magical Good-Turing place in terms of KL divergence.
		
		Uhh You could do something weird like 
		
		\begin{align}
			p_j^{GT}  &= \frac{1}{N}\frac{(j+1)n_{j+1}}{n_j}\\
			&= \frac{1}{N}\frac{jn_{j+1} + n_{j+1}}{n_j}\\
		\end{align}
		
		So at first let's try to specify the MLE objective in terms of partitioned probabilities.
		
		\subsubsection{Partition MLE}
		
			So We have our $1\ldots N$ samples, $1\ldots K$ type of token (just assume We are focusing on a particular context), We partition the $K$ tokens according to how many times each has appeared, and We let $n_r$ be the number of tokens that appear $r$ times and $p_r$ be the probability of a token that appears $r$ times. Let also $R$ be the maximum frequency of tokens - no token occurs more times than some $R$.
			
			Yeah everywhere just assume some particular context $\context$ lol.
			
			Suppose some ordering of tokens? $w^r_i$ maybe? So We would have something like $w^r_i,\;i\in[n_r]$ so $w^r$ means it's a token belonging to the $r$'th partition and $w^r_i$ means it's the $i$'th token in the $r$'th partition.
			
			Let also $V$ be the vocabulary.
			
			So then We can express the MLE KL yoke as
			
			\begin{align}
				\sum^R_{r=1} \sum^{n_r}_{i=1} \KL( \tilde{p}(\cdot|w^r_i,\context)\mid\mid \model(\cdot | w^r_i,\context))
			\end{align}
			
			Then We simplify the KL term:
			
			\begin{align}
			\KL( \tilde{p}(\cdot|w^r_i,\context)\mid\mid \model(\cdot | w^r_i,\context)) &= \sum_{w'\in V} \tilde{p}( w' |w^r_i,\context) \ln\bigg(\frac{\tilde{p}( w' |w^r_i,\context)}{\model( w' |w^r_i,\context)} \bigg)\\
			&= \ln\bigg(\frac{1}{\model(w^r_i|\context)} \bigg)
			\end{align}
			
			So then 
		
			\begin{align}
				\sum^R_{r=1} \sum^{n_r}_{i=1} \KL( \tilde{p}(\cdot|w^r_i,\context)\mid\mid \model(\cdot | w^r_i,\context)) &= \sum^R_{r=1} \sum^{n_r}_{i=1} \ln\bigg(\frac{1}{\model(w^r_i|\context)} \bigg)
			\end{align}
		
			Then the constraint. I suppose let's do the one from the paper:
			
			We had
			
			\begin{align}
				\sum^R_{r=0} n_r p_r &= 1\\
				\sum^R_{r=0} \sum^{n_r}_{i=1} \model(w^r_i) &= 1
			\end{align}
			
			Plugging it in:
			
			\begin{align}
				\mathcal{L} = \bigg(\sum^R_{r=1} \sum^{n_r}_{i=1} \ln\bigg(\frac{1}{\model(w^r_i|\context)} \bigg)\bigg) - \lambda\bigg(\sum^R_{r=0}n_r p_r\bigg) -1\bigg)
			\end{align}
			
			Let's just carry this one out I suppose:
			
			\begin{align}
				\frac{\partial }{\partial \model(w^a_b|\context)} \sum^R_{r=1} \sum^{n_r}_{i=1} \ln\bigg(\frac{1}{\model(w^r_i|\context)} \bigg) &= \frac{\partial }{\partial \model(w^a_b|\context)} \sum^R_{r=1} \sum^{n_r}_{i=1} \ln(\model(w^r_i|\context)^{-1} )\\
				&= \frac{\partial }{\partial \model(w^a_b|\context)} \sum^R_{r=1} \sum^{n_r}_{i=1} -\ln(\model(w^r_i|\context) )\\
				&= -\frac{1}{\model(w^a_b|\context)}
			\end{align}
					
			Things to note: it's only the unseen items that will get heterogenous assignments in Katz, so maybe those can just be split out or something.
			
			\begin{align}
				\frac{\partial }{\partial \model(w^a_b|\context)} - \lambda\bigg(\sum^R_{r=0}n_r p_r\bigg) -1\bigg) &= 
			\end{align}
			
			Well here's a problem. I think We need to also do summation here. This notation's kinda clunky.
			
			There's also stuff about $n_0$ here. 
			
			There will not be any KL terms for unseen examples because We have no observed distribution. 
			
			I feel like the problem here is that We have different variables in the function We are trying to optimize and the constraint - the constraint depends on all probabilities, the function only probabilities associated with observed words.
			
			Maybe cutting off at certain $R$ is a workaround.
			
			Yeah in the paper they just limited themselves to well I mean yeah it's not wrong You set the other probabilities and the last is dictated by the constraint right.
			
			Okay so We just take the derivative w.r.t. observed stuff.
			
			Anyway, here's wonderwall:
			
			\begin{align}
				\frac{\partial }{\partial \model(w^a_b|\context)} -\bigg(\lambda  \bigg( \sum^R_{r=0} \sum^{n_r}_{i=1} p^r_i \bigg)-1\bigg) &= \lambda
			\end{align}
			
			lol
			
			So the above derivative exists only for $a\in\{1\ldots R\}$ is the important bit.
			
			Alright then putting the derivatives together:
			
			\begin{align}
				-\frac{1}{\model(w^a_b|\context)} - \lambda &= 0\\
				\model(w^a_b|\context)  &= -\frac{1}{\lambda}\qquad\text{for }a\in\{1\ldots R\}
			\end{align}
			
			I mean yeah each occurrence is getting some set probability mass.
			
			\begin{align}
				\sum^R_{r=0} \sum^{n_r}_{i=1} p^r_i &= 1\\
				\bigg(\sum^{n_0}_{j=1}p^0_j\bigg) + \sum^R_{r=1} \sum^{n_r}_{i=1} p^r_i &= 1\\
				\bigg(\sum^{n_0}_{j=1}p^0_j\bigg) + \sum^R_{r=1} \sum^{n_r}_{i=1} -\frac{1}{\lambda} &= 1\\
				\sum^R_{r=1} \sum^{n_r}_{i=1} -\frac{1}{\lambda} &= 1 - \bigg(\sum^{n_0}_{j=1}p^0_j\bigg)\\
				-\frac{N}{\lambda} &= 1 - \bigg(\sum^{n_0}_{j=1}p^0_j\bigg)\\
				-\frac{\lambda}{N} &= \frac{1 - \bigg(\sum^{n_0}_{j=1}p^0_j\bigg)}{1}\\
				\lambda &= -N\frac{1 - \bigg(\sum^{n_0}_{j=1}p^0_j\bigg)}{1}\\
			\end{align}
			
			So then
			
			\begin{align}
				\model(w^a_b|\context)  &= -\frac{1}{\lambda}\qquad\text{for }a\in\{1\ldots R\}\\
				&= -\bigg( -N\frac{1 - \bigg(\sum^{n_0}_{j=1}p^0_j\bigg)}{1} \bigg)^{-1}\\
				&= \frac{1}{N \bigg( 1 - \bigg(\sum^{n_0}_{j=1}p^0_j\bigg)\bigg)}
			\end{align}
			
			So basically You get to decide how much mass You allocate to these unseen examples. It's a hyperparameter.
			
			There was an $r$ term missing the whole time lol.
			
			
		\subsubsection{Partition Good-Turing}
		
			This depends on grouping partitions, I think.
			
			So I think You'll actually not want to keep everything as individual probabilities. 
			
			But We need a per-term KL loss right.
			
			Kay so We go over the dataset one item at a time, and We pretend that this item has been held out. So let's say that in the full set some observed word was occurred $r+1$ times. Okay. 
			
			This is making things more complicated - before We just had what is the correct word, but now We also have which partition the correct word is in in the full set. 
			
			But so ok We have some true word $w$, it occurred $a+1$ times in the full set. Now We want to write a KL term for it.
			
			We have the true distribution $\tilde{p}$ which will select the correct from the model distribution.
			
			So I guess it's the model distribution term that changes? Our prediction for word $w^{a+1}$ is $p^a$, right?
			
			So the surface stays the same: 
			
			\begin{align}
				\sum^R_{r=1} \sum^{n_r}_{i=1} \KL( \tilde{p}(\cdot|w^r_i,\context)\mid\mid \model(\cdot | w^r_i,\context))
			\end{align}
			
			Then given that We are observing word $w^{a+1}$, this becomes
			
			\begin{align}
				\KL( \tilde{p}(\cdot|w^{a+1}_i,\context)\mid\mid \model(\cdot | w^{a+1}_i,\context)) &= \sum_{w'\in V} \tilde{p}( w' |w^{a+1}_i,\context) \ln\bigg(\frac{\tilde{p}( w' |w^{a+1}_i,\context)}{\model( w' |w^{a+1}_i,\context)} \bigg)\\
				&= \ln\bigg(\frac{1}{\model(w^a|\context)} \bigg)
			\end{align}
			
			or You can go one down instead of one up:
			
			\begin{align}
				\KL( \tilde{p}(\cdot|w^{a}_i,\context)\mid\mid \model(\cdot | w^{a}_i,\context)) &= \sum_{w'\in V} \tilde{p}( w' |w^{a}_i,\context) \ln\bigg(\frac{\tilde{p}( w' |w^{a}_i,\context)}{\model( w' |w^{a}_i,\context)} \bigg)\\
				&= \ln\bigg(\frac{1}{\model(w^{a-1}|\context)} \bigg)
			\end{align}
			
			Maybe also simplify this notation by saying
			
			\begin{align}
				\model(w^{a-1}|\context) = \model^{a-1}
			\end{align}
			
			The constraint is also partitioned:
			
			\begin{align}
				\sum^{R}_{r=0} n_r \model^r &= 1
			\end{align}
			
			So then
			
			Now to streamline the likelihood
			
			\begin{align}
				\sum^R_{r=1} \sum^{n_r}_{i=1} r\cdot \KL( \tilde{p}(\cdot|w^{r}_i,\context)\mid\mid \model(\cdot | w^{r}_i,\context)) &= \sum^R_{r=1} \sum^{n_r}_{i=1} r\cdot \KL( \tilde{p}(\cdot|w^{r}_i,\context)\mid\mid \model(\cdot | w^{r}_i,\context))\\
				&= \sum^R_{r=1} n_r r\cdot \KL( \tilde{p}(\cdot|w^{r}_i,\context)\mid\mid \model(\cdot | w^{r}_i,\context))\\
				&= \sum^R_{r=1} n_r r\cdot \ln\bigg(\frac{1}{\model(w^{r-1}|\context)} \bigg)
			\end{align}
			
			Putting both together:
			
			\begin{align}
				\mathcal{L} = \bigg(\sum^R_{r=1} n_r r\cdot \ln\bigg(\frac{1}{\model^{r-1}} \bigg)\bigg) - \lambda\bigg( \bigg(\sum^{R}_{r=0} n_r \model^r \bigg) - 1 \bigg)
			\end{align}
			
			So We have model parameters that are present in the function to be optimized $\model^r,\;r\in\{1\ldots R-1\}$. 
			
			Then the derivatives
			
			\begin{align}
				\frac{\partial}{\partial \model^j}\; \sum^R_{r=1} n_r r\cdot \ln\bigg(\frac{1}{\model^{r-1}} \bigg) &= \frac{\partial}{\partial \model^j}\; \sum^R_{r=1} -n_r r\cdot \ln(\model^{r-1})\\
				&=  \frac{-n_{j+1} (j+1)}{\model^{j}}
			\end{align}
			
			And the constraint
			
			\begin{align}
				\frac{\partial}{\partial \model^j}\; - \lambda\bigg( \sum^{R}_{r=0} n_r \model^r - 1 \bigg) &= -\lambda n_j
			\end{align}
			
			So then We get
			
			\begin{align}
				\frac{-n_{j+1} (j+1)}{\model^{j}} -\lambda n_j &= 0\\
				\frac{n_{j+1} (j+1)}{\model^{j}}  &= -\lambda n_j\\
				\frac{1}{\model^{j}}  &= -\frac{\lambda n_j}{n_{j+1} (j+1)}\\
				\model^{j}  &= -\frac{n_{j+1} (j+1)}{\lambda n_j}\qquad\text{for }j\in\{0\ldots R-1\}
			\end{align}
			
			Plugging this into the constraint:
			
			\begin{align}
				\sum^{R}_{r=0} n_r \model^r &= 1\\
				n_R\model^R + \sum^{R-1}_{r=0} n_r \model^r &= 1\\
				n_R\model^R + \sum^{R-1}_{r=0} -n_r \frac{n_{r+1} (r+1)}{\lambda n_r}&= 1\\
				 \sum^{R-1}_{r=0} -n_r \frac{n_{r+1} (r+1)}{\lambda n_r}&= 1 - n_R\model^R\\
				 \sum^{R-1}_{r=0} -n_r \frac{n_{r+1} (r+1)}{n_r}&= \lambda(1 - n_R\model^R)\\
				 \frac{\sum^{R-1}_{r=0} -n_r \frac{n_{r+1} (r+1)}{ n_r}}{1 - n_R\model^R} &= \lambda\\
				 \frac{\sum^{R-1}_{r=0} -n_{r+1} (r+1)}{1 - n_R\model^R} &= \lambda
			\end{align}
			
			Plugging this back in
			
			\begin{align}
				\model^{j}  &= -\frac{n_{j+1} (j+1)}{\lambda n_j}\\
				&= -\frac{n_{j+1} (j+1)}{ n_j}\frac{1}{\lambda}\\
				&= -\frac{n_{j+1} (j+1)}{ n_j}\bigg(\frac{\sum^{R-1}_{r=0} -n_{r+1} (r+1)}{1 - n_R\model^R} \bigg)^{-1}\\
				&= -\frac{n_{j+1} (j+1)}{ n_j} \frac{1 - n_R\model^R}{\sum^{R-1}_{r=0} -n_{r+1} (r+1)} \\
				&= -\frac{n_{j+1} (j+1)}{ n_j} \frac{1 - n_R\model^R}{N} \\
				&= \frac{n_{j+1} (j+1)}{ n_j} \frac{1}{N}\times (1 - n_R\model^R) \\
			\end{align}
			
			So a certain amount of mass is reserved for max count stuff, and it's up to You what that amount is.
			
			Also a minus sign got lost along the way lol.
			
	\subsection{Follow up on Katz}
	
		So to get to Katz We need a few modifications.
		
		Namely, heterogenous probability values for unseen n-grams and
		
		Mess with the count lol.
		
		Messing with the count then:
		
		There will be all kinds of problems here since Katz is trying to mimick the amount of mass discounted on the whole set by only focusing on a subset, so it'll be scaled somehow, as in the subset that is getting mass taken away will have to lose more in order to keep the amount of mass going to 0-count grams the same as in Good-Turing.
		
		But so the first requirement is proportionality.
		
		The adjusted counts are $d_r r$ where $r$ is the count in the full training set.
		
		So there are three pieces - the true count $r$, the Good-Turing discounted count $r^*$, and the Katz count, $d_r$.
		
		Nope lol. $d_r$ is a factor scaling the true count
		
		\begin{align}
			d_r = \mu \frac{r^*}{r}
		\end{align}
			
		Dumb examples:
		
		\begin{align}
			10(1-\frac{1}{2}) &= 5\\
			10(1-\frac{1}{4}) &= 7.5\\
			10(1-\frac{1}{8}) &= 8.75\\
		\end{align}
		
		Oh right We are trying to keep the discount factor the same.
		
		1 is not discounting at all
		
		$d_r$ is the factor We multiply by to get the discount, and it's strictly less than 1.
		
		So the amount of mass We lose as a proportion of the full thing is $1-d_r$
		
		So then
		
		\begin{align}
			1-d_r = \mu\bigg(1-\frac{r^*}{r} \bigg)
		\end{align}
		
		And that's true for $r\in\{ 1\ldots k\}$, where I guess We are kind of overloading $k$ here, but so We won't discount anything that occurs more than $k$ times.
		
		So that's the first constraint, the proportionality thing. The second one was that the amount of mass assigned to 0-count grams should remain the same as if though We did full Good-Turing across the entire dataset.
		
		This can be expressed as a count - We have $n_0$ grams that occur 0 times, and each one will get $\model^0\times N$ counts:
		
		\begin{align}
			n_0 \times\model^0\times N &= n_0\times\frac{n_{j+1} (j+1)}{ n_j} \frac{1}{N}\times (1 - n_R\model^R) \\
			 &= n_0\times\frac{n_{1} (1)}{ n_0} \frac{1}{N}\times (1 - n_R\model^R)\times N \\
			 &= n_0\times\frac{n_{1} }{ n_0} \frac{1}{N}\times N \\
			 &= n_{1} 
		\end{align}
		
		Cool. So then We have
		
		\begin{align}
			\sum^k_{r=0} (1-d_r) r n_r = n_1
		\end{align}
		
		So now what. 
		
		\begin{align}
			1-d_r = \mu\bigg(1-\frac{r^*}{r} \bigg)
		\end{align}
		
		Gotta combine these, and We're looking for $d_r$.	
		
		I'll just take them at their word for
		
		\begin{align}
			d_r = \frac{\frac{r^*}{r} - \frac{(k+1)n_{k+1}}{n_1} }{1 - \frac{(k+1)n_{k+1}}{n_1}}
		\end{align}
		
		Ach I would kind of like to derive it in case there's a clue in there about how it might be used in our context.
		
		So then
		
		\begin{align}
			1-d_r &= \mu\bigg(1-\frac{r^*}{r} \bigg)\\
			-d_r &= \mu\bigg(1-\frac{r^*}{r} \bigg)-1\\
			d_r &= 1-\mu\bigg(1-\frac{r^*}{r} \bigg)
		\end{align}
		
		And then maybe single one out?
		
		\begin{align}
			\sum^k_{r=0} (1-d_r) r n_r &= n_1\\
			\sum^k_{r=1} (1-d_r) r n_r &= n_1
		\end{align}
		
		Since if $r=0$ the term in the sum is also zero.
		
		\begin{align}
			\sum^k_{r=1} (1-d_r) r n_r &= n_1\\
			(1-d_1)  n_1 + \sum^k_{r=2} (1-d_r) r n_r &= n_1\\
			(1-d_1)  n_1  &= n_1 - \sum^k_{r=2} (1-d_r) r n_r\\
			(1-d_1)  n_1  &= n_1 - \sum^k_{r=2} (1-\bigg( 1-\mu\bigg(1-\frac{r^*}{r} \bigg)\bigg)) r n_r
		\end{align}
		
		eh it probably works out, it's just algebra. 
		
	\subsection{So now what}
	
		Layers of the onion:
		
		Tokens occurring more frequently than $k$ are left alone ah fuck doesn't this mean We need like separate model functions? Well We'll have like if terms
		

\newpage		
\section{Guess Again}

	\subsection{Add-delta smoothing}
		
		So We had
		
		\begin{align}
	  		\sum_{\context} \Big( {\color{blue} \KL\left(\tilde{p}(\cdot 			\mid \context) \mid\mid \model(\cdot \mid \context)					\right)} + {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)}\Big)
		\end{align}
		
		And He's using different notation. Given a context, We'll have an empirical distribution of words, which is just an n-gram distribution if You will, and then also adding that uniform term.
		
		So then ignoring the normalizer, We know that the 
		
	\subsection{Good-Turing}
		Different contexts essentially represent separate problems - given a context $\context$ We'll want to obtain a smoothed probability distribution for the next word. So for the rest of the derivation, assume there's a $|\context$. 
		
	    So then We have our model distribution $\model$ and the observed empirical distribution $\tilde{p}$, the latter being just the simple n-gram MLE, $\#(w\circ\context)/\#(\context)$, normalized counts.
	    
	    We want to come up with some distribution $\bar{p}$ such that minimizing
	    
	    \begin{align}
	    	\KL(\bar{p}(\cdot) \mid\mid \model(\cdot))
	    \end{align}
	    
	    Yields Good-Turing in $\model$. That's not difficult to do, as in just let $\bar{p} = \text{Good-Turing}$, but We also want to express $\tilde{p}$ as the convex combination of the observed empirical distribution and some other distribution $g$:
	    
	    \begin{align}
	    	\bar{p} = \alpha\tilde{p} + (1-\alpha)g
	    \end{align}
	    
	    So then We try to break down Good Turing - let $w_r$ be a word which occurred $r$ times given some context $|\context$, then
	    
	    \begin{align}
	    	GT(w_r) = \frac{(r+1)S(N_{r+1})}{NS(N_r)}
	    \end{align}
	    
	    Where $N$ is the number of observed words in the corpus (not the number of unique words), and $S(N_r)$ is a smoothed version of $N_r$, the number of unique words that has happened $r$ times.\footnote{$N_r$ needs to be smoothed since We use $N_{r+1}$ to estimate the probability for words that occur $r$ times, and We'll run into problems when We try to estimate the probability for words which occur $R$ times, where $R$ is the maximum frequency - no word that is more frequent than the most frequent word means no probability estimate,  unless You smooth $N_r$ and use some of the saved mass for the most frequent term. Good-Turing just trades the problem of not knowing what to do with unseen tokens for the problem of not knowing what to do with the most frequent tokens.}
	    
	    So $GT(w_r)$ is what We'll want our model to output, and We need to re-express it:
	    
	    \begin{align}
	    	\model(w_r) &= \frac{(r+1)S(N_{r+1})}{NS(N_r)} \\
	    	&\approx  \frac{(r+1)S(N_{r+1})}{S(N_r)}\qquad\text{Drop } N,\text{ Normalize later?}\\
	    	&\approx (r+1)C_r \qquad\qquad\qquad C_r=\frac{S(N_{r+1})}{S(N_r)}\\
	    	&\approx (N\tilde{p}(w_r)+1)C_r \\
	    	&\approx C_r N\tilde{p}(w_r) + C_r \\
	    	&\approx C_r N\tilde{p}(w_r) + C_r u(w_r)\qquad u(w_r)\text{ just uniform dist.} 
	    \end{align}
	    
	    If We let $a(w_r) = \frac{NC_r}{(N+1)C_r}$, We can write
	    
	     Question: shouldn't $C_r$ just cancel here?
	    
	    \begin{align}
	    	\model(w_r) &\approx C_r N\tilde{p}(w_r) + C_r u(w_r)\\
	    	&= \alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r)) u(w_r)
	    \end{align}
	    
	    And We went from approximately to equal since We normalized, I think.
	    
	    So then the proposition:
	    
	    Let $C_{w_r} = \frac{S(N_{r+1})}{S(N_r)}$, $N$ = the number of tokens satisfying a particular context $\context$, and $\alpha(w_r) = \frac{NC_r}{(N+1)C_r}$ then minimizing the following KL divergence:
	    
	    \begin{align}
	    	\KL(\alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r))u(w_r)\mid\mid \model(w_r))
	    \end{align}
	    
	    Yields the Good-Turing estimate, and then this is true by construction.
	    
	    Note: Strictly speaking, I guess You would write
	    
	    \begin{align}
	    	\KL(\alpha(\cdot)\tilde{p}(\cdot) + (1-\alpha(\cdot))u(\cdot)\mid\mid \model(\cdot))
	    \end{align}	
	    
	    Where the $\cdot\in w_r, r\in{0\ldots R}$, meaning $\cdot$ is a word that belongs to a particular partition of the vocabulary, and We partition based on observed frequency of tokens of the form $(w_r\circ\context)$.
	    
	    So then for this last bit
	    
	    \begin{align}
	    	\delta(w) &= (a(w_r) - 1)\tilde{p}(w_r) + (1-\alpha(w_r)) r(w_r)
	    \end{align}
	    
	    I'm pretty sure $r(w)$ is just the uniform distribution. 
	    
	    $\delta$ is introduced as an algebraic means to get to a cleaner formulation?
	    
	    \begin{align}
	    	\tilde{p}_w (w_r) + \delta(w_r) &= \tilde{p}_w (w_r) + (a(w_r) - 1)\tilde{p}(w_r) + (1-\alpha(w_r)) r(w_r)\\
	    	&= a(w_r)\tilde{p}(w_r) + (1-\alpha(w_r)) r(w_r)
	    \end{align}
	    
	    Now You have two clean separate terms whose sum is equivalent to Good-Turing. Since cross entropy can be broken down into entropy and KL divergence, minimizing KL divergence turns out to be equivalent to minimizing the cross entropy (since the entropy part remains constant), and cross entropy is linear in it's first argument, so You can use that to create two cross-entropy terms and then reduce them down the cross entropy to get
	    
	    \begin{align}
	    	\argmin_{p_\theta} \underbrace{\KL(\tilde{p}\mid\mid\model)}_{\text{Standard MLE}} + \lambda \cdot \underbrace{\KL(\delta\mid\mid\model)}_{\text{Regularizer}}
	    \end{align}
	    
	    And now You can use this objective function in training wherever You like, so long as You are able to partition the vocabulary according to token frequency in training corpus and provide $S(N_r)$.
	    
	    $\delta$ is not a proper distribution though right? $\tilde{p} + \delta$ is. Does that not matter?
	    
	    Oh is it okay because the first argument of the first KL term is $\tilde{p}$?
	
	    As a sidenote, let $CE$ be cross entropy:
	    
	    \begin{align}
			CE(\alpha P+(1-\alpha)W, Q) &= \sum_{x\in\mathcal{X}} (\alpha P(x)+(1-\alpha)W(x))\ln(Q(x))\\
			&= \sum_{x\in\mathcal{X}} \alpha P(x)\ln(Q(x)) +(1-\alpha)W(x)\ln(Q(x))\\
			&= \alpha CE(P, Q) + (1-\alpha)CE(W, Q)
		\end{align}
		
		So swap out KL for CE, then use linearity in CE, then go back to KL, is what I'm getting here. Only works when looking for minima (or maxima I guess).
		
		Another question:
		
		\begin{align}
	    	\model(w_r) &\approx C_r N\tilde{p}(w_r) + C_r u(w_r)\\
	    	&= \alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r)) u(w_r)
	    \end{align}
		
		And Ryan noted $u(w_r)=1$, so We should be able to just algebraically recover Good-Turing from this - 
		
		\begin{align}
	    	\model(w_r) &= \alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r)) u(w_r)\\
	    	&= \alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r)) 1\\
	    	&= \alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r))\\
	    	&= \frac{NC_r}{(N+1)C_r}\tilde{p}(w_r) + \frac{C_r}{(N+1)C_r}\\
	    	&= \frac{NC_r}{(N+1)C_r}\frac{r}{N} + \frac{C_r}{(N+1)C_r}\\
	    	&= \frac{NC_r\cdot r}{(N+1)C_r\cdot N} + \frac{NC_r}{N(N+1)C_r}\\
	    	&= \frac{r}{(N+1)} + \frac{1}{(N+1)}	
	    \end{align}
		
		Which is almost right? What is that (N+1) doing down there.
		
		$\model(w_r)$ is the probability of a word, and in general words that occur $r$ times in total will have $n_rp_r$ mass allotted to them. 
		
		So the uniform distribution here should be concerned with the size of the vocabulary, right? We have that subscript $r$ in $\model(w_r)$, but this equation is still concerning itself only with a single word from a vocabulary, the subscript $r$ is just keeping track of the frequency.
		
		And if $u(w_r)$ is uniform in the vocabulary, then $u(w_r) = 1/|V|$.
		
		This does not bode well lol:		
		
		\begin{align}
	    	\model(w_r) &= \frac{(r+1)S(N_{r+1})}{NS(N_r)} \\
	    	&\approx  \frac{(r+1)S(N_{r+1})}{S(N_r)}\qquad\text{Drop } N,\text{ Normalize later?}\\
	    	&\approx (r+1)C_r \qquad\qquad\qquad C_r=\frac{S(N_{r+1})}{S(N_r)}\\
	    	&\approx (N\tilde{p}(w_r)+1)C_r \\
	    	&\approx C_r N\tilde{p}(w_r) + C_r \\
	    	&\approx C_r N\tilde{p}(w_r) + C_r |V| u(w_r)\qquad u(w_r)\text{ just uniform dist.} 
	    \end{align}
	    
	    Then let 
	    
	    \begin{align}
	    	\alpha(w_r) &= \frac{C_r N}{ C_r N + C_r|V|}\\
	    	&= \frac{C_r N}{ C_r (N + |V|)}\\
	    	&= \frac{N}{N + |V|}
	    \end{align}
	    
	    \begin{align}
	    	\model(w_r) &\approx C_r N\tilde{p}(w_r) + C_r |V|u(w_r)\\
	    	&= \alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r))u(w_r )
	    \end{align}
	    
	    And, sure. But now You are supposed to be able to get back to Good-Turing from here, but We've cancelled $C_r$ lol:
	    
	    \begin{align}
	    	\model(w_r) &= \alpha(w_r)\tilde{p}(w_r) + (1-\alpha(w_r))u(w_r )\\
	    	&= \frac{N}{N + |V|}\tilde{p}(w_r) + \frac{|V|}{N + |V|}u(w_r )\\
	    	&= \frac{N}{N + |V|}\tilde{p}(w_r) + \frac{|V|}{N + |V|}\frac{1}{|V|}\\
	    	&= \frac{N}{N + |V|}\frac{r}{N} + \frac{|V|}{N + |V|}\frac{1}{|V|}\\
	    	&= \frac{r}{N + |V|} + \frac{1}{N + |V|}
	    \end{align}
	    
	    
	\subsection{TODO}
	
		Okay so We have 
		
		$h_{pos}(w)$ and $h_{neg}(w)$, the positive and negative parts of some function $w$. Then obviously $h(w) = h_{pos} + h_{neg}$.
		
		Then We come up with some $r_{pos}$ and $\lambda_{pos}$ such that 
		
		\begin{align}
			\lambda_{pos}r(w)_{pos} &= h_{pos}(w),\\
			\sum_w r_{pos}(w) &= 1
		\end{align}
		
		So $\lambda_{pos} = Z{-1}$, the inverted normalization term We had to apply to $r_{pos}$ get a distribution.
		
		Similarly
		
		\begin{align}
			\lambda_{neg} r(w)_{neg} &= -h_{neg}(w),\\
			\sum_w r_{neg}(w) &= 1
		\end{align}
		
		Then You can say
		
		\begin{align}
			GT(w) &= \tilde{p}(w) + \lambda_{pos}r_{pos}(w) - \lambda_{neg}r_{neg}(w)\\
			&= \tilde{p}(w) + h_{pos}(w) - (-h_{neg}(w))
		\end{align}
		
		So I'm guessing minus sign in $\lambda_{neg} r(w)_{neg} = -h_{neg}(w)$ to have both the terms on the right be positive.
		
		As I see it:
		
		1. Split the function $h(w)$ into positive and negative parts $h_{pos}(w)$ and $h_{neg}(w)$, so $h_{pos}(w)$ is always positive (or 0) and $h_{neg}(w)$ is always negative.
		
		2. Let $\lambda_{pos} = \sum_w h_{pos}(w)$, so $\sum_w h_{pos}(w)/\lambda_{pos} = 1$, and let $r_{pos}(w) = h_{pos}(w)/\lambda_{pos}$.
		
		3. Let $\sum_{w} h_{neg}(w) = \lambda_{neg}$, so $h_{neg}/\lambda_{neg} = 1$, and let $\sum_w r_{neg}(w) = h_{neg}(w)/\lambda_{neg}$. Note that since $h_{neg}(w)$ is always negative, that means that the $\lambda_{neg}$ term is also negative, which makes every term in $r_{neg}(w)$ positive.
		
		4. Using the above We can write $GT(w) = \tilde{p} + \lambda_{pos}r_{pos}(w) - $
		
		
		Okay so We have
		
		\begin{align}
			\sum_w r_{neg}(w) &= 1\\
			\sum_w \frac{h_{neg}(w)}{\lambda_{1}} &= 1
		\end{align}
		
		So $\lambda_{neg}$ is a negative term.
		
		So $r(w) = h_{neg}(w)/\lambda_1$ 
		
		So then
		
		\begin{align}
			GT(w) &= \tilde{p} + \lambda_{pos}r_{pos}(w) + \lambda_{neg}r_{neg}(w)\\
			&= \tilde{p} + h_{pos}(w) + h_{neg}(w)\\
			&= \tilde{p} + h(w)
		\end{align}
		
		So We have 
		
\section{Practice with Add-Delta smoothing}

	\begin{align}
		p_{\delta}(w) &= \frac{\#(w) + \delta}{N + |V|\delta}\\
		&= \frac{\#(w)}{N + |V|\delta} + \frac{\delta}{N + |V|\delta}
	\end{align}
	    
	Here, since the terms are all global, You actually could just normalize locally I think, but so
	
	\begin{align}
		p_{\delta}(w) &= \frac{\#(w)}{N + |V|\delta} + \frac{\delta}{N + |V|\delta}\\
		&= \#(w)C + \delta C\qquad\qquad {\color{gray} C=\frac{1}{N+|V|\delta}}\\
		&= \frac{\#(w)}{N} - \frac{\#(w)}{N} + \#(w)C + \delta C\\
		&= \frac{\#(w)}{N} + \bigg(C - \frac{1}{N}\bigg)\#(w) + \delta C\\
		&= \underbrace{\frac{\#(w)}{N}}_{\tilde{p}} + \underbrace{\bigg(C - \frac{1}{N}\bigg)\#(w) + \delta C}_{h(w)}
	\end{align}
	
	Then We do
	
	\begin{align}
		h_{pos}(w) = \begin{cases} h(w) & h(w) \geq 0\\
								0 &\textbf{otherwise}
					\end{cases}\\
		h_{neg}(w) = \begin{cases} h(w) & h(w) < 0\\
								0 &\textbf{otherwise}
					\end{cases}
	\end{align}
	    
	Then let 
	
	\begin{align}
		\lambda_{pos} &= \sum_w h_{pos}(w)\\
		\lambda_{neg} &= \sum_w h_{neg}(w)\\
		r_{pos}(w) &= \frac{h_{pos}(w)}{\lambda_{pos}}\\
		r_{neg}(w) &= \frac{h_{neg}(w)}{\lambda_{neg}}
	\end{align}
	
	We then have
	
	\begin{align}
		\sum_w r_{pos}(w) &= 1\\
		\sum_w r_{neg}(w) &= 1\\
		\lambda_{pos}r_{pos}(w) = h_{pos}(w)\\
		\lambda_{neg}r_{neg}(w) = h_{neg}(w)
	\end{align}
	
	and finally
	
	\begin{align}
		p_\delta(w) &= \tilde{p} + h(w)\\
		&= \tilde{p} + h_{pos}(w) + + h_{neg}(w)\\
		&= \tilde{p} + \lambda_{pos}r_{pos}(w) + \lambda_{neg}r_{neg}(w)
	\end{align}
	
	And from there it's just a hop skip and a jump to KL.
	    
	\newpage		
	\subsection{Add-delta smoothing}
	
		Let's use the same reasoning again:
		
		Just assume a context.
		
		\begin{align}
			p_\delta(w) &= \frac{\delta + \#(w)}{\delta |\mathcal{V}| + \sum_{w\in\mathcal{V}} \#(w)}\\
			&\approx \delta + \#(w)\\
			&\approx \delta u(w) + N\tilde{p}(w)
		\end{align}
		
		and then
		
		\begin{align}
			\alpha(w) = \frac{N}{\delta+N}
		\end{align}
		
		Plugging that in:
		
		\begin{align}
			p_\delta(w) &\approx \delta u(w) + N\tilde{p}(w)\\
			&= \alpha(w) \tilde{p}(w) + (1-\alpha(w)) u(w)  \\
			&= \frac{N}{\delta+N} \tilde{p}(w) + \frac{\delta}{\delta+N} u(w)\\
			&= \frac{N}{\delta+N} \frac{r}{N} + \frac{\delta}{\delta+N}\\
			&= \frac{r}{\delta+N} + \frac{\delta}{\delta+N}\\
			&= \frac{r+\delta}{\delta+N}
		\end{align}
		
		Hmm. I don't know if You can let the uniform term be equal to 1.
		
		So We had
		
		\begin{align}
			p_\delta(w) = \frac{\delta + \#(w)}{\delta |\mathcal{V}| + \sum_{w\in\mathcal{V}} \#(w)}
		\end{align}
		
		So then We wanna split this up, but this time use $u(w)=|V|^{-1}$:
		
		\begin{align}
			p_\delta(w) &= \frac{\delta + \#(w)}{\delta |\mathcal{V}| + \sum_{w\in\mathcal{V}} \#(w)}\\
			&\approx \delta + \#(w)\\
			&\approx \delta|V|u(w) + N\tilde{p}(w)
		\end{align}
		
		And then normalize? Let $\alpha = \frac{N}{|V|\delta + N}$
		
		\begin{align}
			p_\delta(w) &\approx \delta|V|u(w) + N\tilde{p}(w)\\
			&= \alpha\tilde{p}(w) + (1-\alpha)u(w) 
		\end{align}
		
		And I guess a sanity check is: is that true. It feels like the normalization is sus:
		
		\begin{align}
			p_\delta(w) &= \alpha\tilde{p}(w) + (1-\alpha)u(w) \\
			&= \alpha\frac{\#(w)}{N} + (1-\alpha)u(w)\\
			&= \frac{N}{|V|\delta + N}\frac{\#(w)}{N} + \frac{|V|\delta}{|V|\delta + N}u(w)\\
			&= \frac{N}{|V|\delta + N}\frac{\#(w)}{N} + \frac{|V|\delta}{|V|\delta + N}\frac{1}{|V|}\\
			&= \frac{\#(w)}{|V|\delta + N} + \frac{\delta}{|V|\delta + N}\\
			&= \frac{\#(w)}{|V|\delta + N} + \frac{\delta}{|V|\delta + N}
		\end{align}
		

		 Yep okay so then We can say
	    
	    \begin{align}
	    	\argmin_{\model}\KL(\alpha\tilde{p}(\cdot) + (1-\alpha)u(\cdot) \mid\mid \model(\cdot))
	    \end{align}		
	    
	    Will be minimized by add-$\delta$ smoothing.
	    
		Then We want a $\gamma(w)$ such that
		
		\begin{align}
			\tilde{p}(w) + \gamma(w) &= \alpha\tilde{p}(w) + (1-\alpha)u(w)\\
			\gamma(w) &= \alpha\tilde{p}(w) - \tilde{p}(w) + (1-\alpha)u(w)\\
			\gamma(w) &= (1-\alpha)\tilde{p}(w) + (1-\alpha)u(w)
		\end{align}
		
		And so then We can also say that minimizing
		
		\begin{align}
	    	\argmin_{\model}\KL( \tilde{p}(w) + \gamma(w) \mid\mid \model(\cdot))
	    \end{align}
	    
	    Will also yield add-$\delta$ smoothing.
	    
	    Then using the whole KL$\to$CE$\to$KL trick We can say
	    
		\begin{align}
	    	&\argmin_{\model}\KL( \tilde{p}(w) + \gamma(w) \mid\mid \model(\cdot)) \\
	    	&= \argmin_{\model}\KL( \tilde{p}(w)  \mid\mid \model(\cdot)) + \lambda\cdot \KL(\gamma(w) \mid\mid \model(\cdot))
	    \end{align}
	    
	    When $\lambda=1$.
		
\section{Katz again}

	Let $r = \#(w\circ\context_{N-1})$, then for an $N$-gram model, We have:
	
	\begin{align}
		p_{katz}(w|\context_{N-1}) = \begin{cases} 
										\tilde{p}(w|\context_{N-1}) & r > k\\
										d_r \cdot \tilde{p}(w_r|\context_{N-1}) & k \ge r > 0\\
										\alpha(w|\context)p_{katz}(w|\context_{N-2}) & r =0
									\end{cases}
	\end{align}
	
	Where $\context_{N}$ is some context of length $N$.
	
	If a token occurs more frequently than $k$ times, We do not discount it.
	
	If a token occurs 1 or more and up to $k$ times, then We discount the probability by some factor $d_r$.
	
	And if a token occurs 0 times, We back off to an $N-1$-gram model, appropriately weighted.
	
	\subsection{More than k}
	
		In this case We just have
		
		\begin{align}
			\argmin_{\model}\KL(\tilde{p} \mid\mid \model)
		\end{align}
		
	\subsection{Less than k but more than zero}
	
		I.e. now We have $r = \#(w\circ\context_{N-1})$, $0<r\le k$.
		
		\begin{align}
			p_{katz}(w|\context_{N-1}) = d_r \cdot \tilde{p}(w_r|\context_{N-1})
		\end{align}
		
		So here is what $d_r$ is lol:
		
		\begin{align}
			d_r = \bigg(\frac{r^*}{r} - \frac{(k+1)n_{k+1}}{n_1} \bigg)\cdot \bigg(1 - \frac{(k+1)n_{k+1}}{n_1} \bigg)^{-1}
		\end{align}
		
		And
		
		\begin{align}
			r^* = \frac{(r+1)n_{r+1}}{n_r}
		\end{align}
		
		So I suppose to lay it all out:
	
		\begin{align}
			d_r &= \bigg( \frac{(r+1)n_{r+1}}{n_r}\frac{1}{r} - \frac{(k+1)n_{k+1}}{n_1} \bigg)\cdot \bigg(1 - \frac{(k+1)n_{k+1}}{n_1} \bigg)^{-1}\\
			d_r &= \bigg( \frac{(r+1)n_{r+1}}{r n_r} - \frac{(k+1)n_{k+1}}{n_1} \bigg)\cdot \bigg(1 - \frac{(k+1)n_{k+1}}{n_1} \bigg)^{-1}
		\end{align}
		
		And recall that $k$ is just a hyperparameter.
		
		To bring this back to Katz:
		
		\begin{align}
			p_{katz}(w|\context_{N-1}) &= d_r \cdot \tilde{p}(w_r|\context_{N-1})\\
			&= d_r \cdot \tilde{p}(w_r|\context_{N-1})\\
			&= \tilde{p}(w_r|\context_{N-1}) - \tilde{p}(w_r|\context_{N-1}) + d_r \cdot \tilde{p}(w_r|\context_{N-1})\\
			&= \tilde{p}(w_r|\context_{N-1}) + \underbrace{(d_r-1)\tilde{p}(w_r|\context_{N-1})}_{h(w|\context_{N-1})}
		\end{align}
		
	\subsection{Zero}
	
		Isn't this just an inference-time switch? You won't even really be able to calculate $\beta$ until the distribution's settled down. 
		
\section{Absolute Discounting}

	Interpolation between a discounted count and a backoff model:
	
	\begin{align}
		p_{abs}(w_i|\context_{N-1}) = \frac{\max\{\#(w^i|\context_{N-1}) - D, 0 \}}{\sum_w \#(w|\context_{N-1})} + (1-\lambda_{\context_{N-1})} p_{abs}(w_i|\context_{N-2})
	\end{align}
		
	I'm ok with that
	
	\begin{align}
		1-\lambda_{\context_{N-1}} = \frac{D}{\sum_w \#(w|\context_{N-1})} N_{1+} (\context_{N-1}\bullet)
	\end{align}
	
	So $D$ normalized by the count of all tokens satisfying the context times the number of times We apply the absolute discount.
	
	$N_{1+} (w^{i-1}_{i-N+1}\bullet)$ is the number of unique words that follow the context $\context_{N-1}$.
	
	To avoid trouble they use $0 \le D \le 1$.
	
\newpage
\section{Kneser-Ney}

	Following the derivation from \href{https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf}{here, page 15}, in all cases the probability is the same:
	
	\begin{align}
		&P_{KN}(w|\context_{N-1}) \\
		&= \frac{\max\{\#(w|\context_{N-1}) - D, 0 \}}{\sum_w \#(w|\context_{N-1})} + \frac{D}{\sum_w \#(w|\context_{N-1})} N_{1+} (\context_{N-1}\bullet) \times \frac{N_{1+}(\bullet w))}{N_{1+}(\bullet \bullet)}
	\end{align}
	
	where
	
	\begin{align}
		N_{1+}(\bullet, w) &= |\{w_{i-n+1} : \#(w^i_{i-n+1} ) > 0	\} |\\
		N_{1+}(\bullet, \bullet) &= |\{w_{i-n+1}, w^i : \#(w^i_{i-n+1} ) > 0	\} | = \sum_w N_{1+}(\bullet, w)
	\end{align}
	
	$N_{1+}(\bullet, w)$, in words, is the number of unique contexts which have $w$ as the final word.
	
	Then observe that $N_{1+}(\bullet, \bullet)$ is simply the number of unique tokens of length $N$.
	
	And finally, to explain Kneser-Ney, We use absolute discounting to save some probability mass on a per-\textbf{observed}-unique-word-basis, and then distribute this mass according to a continuation probability, which is the probability that the observed word is the final word in a token.
	
	In practice We will not have KL divergence terms on unobserved tokens, so We can ignore the $0$ case (given that $0\le D \le 1$). Assume also that a particular context $\context_{N-1}\in V^{N-1}$ has been observed, (almost) reducing the problem to unigram counts:
	
	\begin{align} 
		p_{KN}(w) &= \frac{\#(w) - D}{N} + \frac{D}{N} N_{1+} (\bullet) \times \frac{N_{1+}(\bullet w))}{N_{1+}(\bullet \bullet)}
	\end{align}
	
	Where $N_{1+} (\bullet)$ is the number of unique observed words given $\context_{N-1}$, and $\frac{N_{1+}(\bullet w))}{N_{1+}(\bullet \bullet)}$ is the same as before.
	
	Then
	
	\begin{align} 
		p_{KN}(w) &= \frac{\#(w) - D}{N} + \frac{D}{N} N_{1+} (\bullet) \times \frac{N_{1+}(\bullet w))}{N_{1+}(\bullet \bullet)}\\
		&= \frac{\#(w)}{N} - \frac{D}{N} + \frac{D}{N} N_{1+} (\bullet) \times \frac{N_{1+}(\bullet w))}{N_{1+}(\bullet \bullet)}\\
		&= \frac{\#(w)}{N} + \frac{D}{N}\bigg( N_{1+} (\bullet) \times \frac{N_{1+}(\bullet w))}{N_{1+}(\bullet \bullet)} - 1 \bigg)\\
		&= \tilde{p}(w) + \underbrace{\frac{D}{N}\bigg( N_{1+} (\bullet) \times \frac{N_{1+}(\bullet w))}{N_{1+}(\bullet \bullet)} - 1 \bigg)}_{h(w)}
	\end{align}

		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		

\newpage\phantom{a}
\newpage		
\section{Appendix}

	\subsection{Entropy}
	
		Oooookay.
		
		This is a simple one - the entropy of a random variable is the expected value of information gained by observing a random variable outcome. How much info do We gain on average by an instance of the variable.
		
		\begin{align}
			Ent(X) = -\sum_{x\in\mathcal{X}} p(x)\cdot log(p(x))
		\end{align}
		
	\subsection{Cross Entropy}
	
		The difference here is that I think We are measuring the amount of information We gain given that events are in reality generated by some distribution $p$, but We are modeling the events with the distribution $q$.
		
		Or, according to A Probabilistic Perspective, if We model some distribution $p$ with our own version $q$, it's the number of bits We'll need on average to represent an event. It's strictly positive is the interesting thing, as in if Your model deviates at all from the truth, You goofed.
		
		\begin{align}
			CE(P, Q) = -\sum_{x\in\mathcal{X}} P(x)\cdot log(Q(x))
		\end{align}
		
		Now here comes a connection with KL divergence:
		
	\subsection{KL Divergence}
	
		Measures the difference in information between two distributions, with $p$ assumed to be the true one.
		
		\begin{align}
			KL(P, Q) &= \sum_{x\in\mathcal{X}} P(x)\cdot\big(\log(P(X)) - \log(Q(X)) \big)\\
			&= \sum_{x\in\mathcal{X}} P(x)\cdot\log\bigg(\frac{P(X)}{Q(X)}\bigg)
		\end{align}
		
		So on average how many more bits do You need to encode an event drawn from distribution $P$ if You are modelling the distribution with $Q$.
		
		The connection with cross entropy seems pretty clear - cross entropy measures the total number of bits needed to encode events from $P$ given $Q$, and KL measures the \textit{additional} bits needed.
		
		And so cross entropy in terms of KL divergence also makes sense - cross entropy keeps track of the number of bits needed to represent event from $P$ using $Q$, and that is of course just the amount of information needed to represent event from $P$, which is just the entropy of $P$, plus the additional bits We need due to shitty encoding, which is precisely the KL divergence, so
		
		\begin{align}
			CE(P, Q) = Ent(P) + KL(P, Q)
		\end{align}
		
		If You are just changing $Q$, then minimizing CE is equivalent to minimizing KL, since the entropy term remains constant. 
		
		Just assume the sum is convex lol:
				
		\begin{align}
			CE(\alpha P+(1-\alpha)W, Q) &= \sum_{x\in\mathcal{X}} (\alpha P(x)+(1-\alpha)W(x))\ln(Q(x))\\
			&= \sum_{x\in\mathcal{X}} \alpha P(x)\ln(Q(x)) +(1-\alpha)W(x)\ln(Q(x))\\
			&= \alpha CE(P, Q) + (1-\alpha)CE(W, Q)
		\end{align}
		
	\subsection{Perplexity}
	
		\subsection{As a measure of model confusion}
		
			A.k.a. how perplexed is the model.
			
			So We take a sentence from the test set, calculate it's probability which is probably a product of n-gram terms, then take the log, and the normalize the log probability by dividing through by $N$. Makes sense, don't want longer sentences to be unlikelier just due to length.
			
			And then... We take the exponent to get rid of the log? Seems ass backwards.
			
			\begin{align}
				e^{\frac{1}{N}\cdot\sum^N_{i=1}\ln(P(w_i))} &= \big(e^{\sum^N_{i=1}\ln(P(w_i))}\big)^{1/N}\\
				&= \bigg(\prod^N_{i=1} e^{ln(P(w_i))}\bigg)^{1/N}\\
				&= \bigg(\prod^N_{i=1} P(w_i)\bigg)^{1/N}
			\end{align}
			
			And so now that We have this normalized probability which I suppose We took a detour into logs for the intuition, now We take the inverse of this.
			
			Inverse because the larger the probability, the less the model should be confused. I guess You could also report the certainty of the model lol.
			
			Want to know the confusion? Take the base You calculated entropy with and exponentiate it with that base - this gives You the number of equally likely options the model had to consider. It's neat.
			
			
	\subsection{Lagrange Multipliers}
	
		Here We go again.
		
		There is only one real core piece of intuition here and it's about the direction of the gradient - suppose You have $x\in\mathcal{X}$ as Your range of possible values, and You're looking to maximize $f(x)$ such that $g(x) = 0$, so $g$ is the function expressing the constraint.
		
		Simple constraint is good old fashioned $x_1^2+x_2^2=r$ for a circle in 2D.
		
		So first You'll only be considering values of $x$ that satisfy $g(x)=0$, which will narrow down Your options.
		
		Now take the gradient of the constraint - which means the constraint has to be differentiable, which means You can kind of imagine it's shape. Introducing some kind of $=c$ in the constraint just locks down the value of $g(x)=c$, as in We just have to focus on some level curve. Then of course the gradient is perpendicular to the level curve.
		
		Then all You have to do is find $x$ such that $f'(x) = \lambda g'(x)$ and $g(x)=0$, skipping a lot of stuff here.
		
		If the gradients aren't parallel, then You can take a step such that You stay on the level curve of $g$ and increase $f$.
		
	\subsection{Unigram MLE}
	
		Find the original resource \href{https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/}{here.}
		
		Let $S$ (for sentence) be the random variable of size $n$ and let $\mathbf{s}$ be a vector of size $n$ which contains actual observations, $\mathbf{s}_i\in\{ w_1\ldots w_m \}$, so each observed word can be on of $m$ words.
		
		By the product rule then
		
		\begin{align}
			P(S=\mathbf{s}) &= P(S_1=\mathbf{s}_1, S_2=\mathbf{s}_2\ldots S_n=\mathbf{s}_n)\\
			&= P(S_1=\mathbf{s}_1)\times P(S_2=\mathbf{s}_2 | S_1=\mathbf{s}_1) \times P(S_3=\mathbf{s}_3 | S_1=\mathbf{s}_1, S_2=\mathbf{s}_2)...
		\end{align}		
			
		Then by the unigram assumption
		
		\begin{align}
			P(S=\mathbf{s}) &= P(S_1=\mathbf{s}_1)\times P(S_2=\mathbf{s}_2 | S_1=\mathbf{s}_1) \times P(S_3=\mathbf{s}_3 | S_1=\mathbf{s}_1, S_2=\mathbf{s}_2)...\\
			&= P(S_1=\mathbf{s}_1)\times P(S_2=\mathbf{s}_2)\times P(S_3=\mathbf{s}_3)\ldots \times P(S_n=\mathbf{s}_n)
		\end{align}
		
		Now of course these become position independent and $P(S_i=\mathbf{s}_i) = P(S_j=\mathbf{s}_j)$ if $\mathbf{s}_i = \mathbf{s}_j$. Let $P(\mathbf{s}_i) = P(w_k)$, then We can group the terms as follows:
		
		\begin{align}
			P(S=\mathbf{s}) &= P(S_1=\mathbf{s}_1)\times P(S_2=\mathbf{s}_2)\times P(S_3=\mathbf{s}_3)\ldots \times P(S_n=\mathbf{s}_n)\\
			&= \prod^m_{i=1} P(w_i)^{\#w_i}
		\end{align}
			
		Where
		
		\begin{align}
			\# w_i = \sum^n_{l = 1} \{1|\mathbf{s}_l = w_i \}
		\end{align}
		
		So just the number of times $w_i$ occurs in the sentence.
		
		But so okay We have a likelihood, and then for the log likelihood:
		
		\begin{align}
			P(S=\mathbf{s}) &= \prod^m_{i=1} P(w_i)^{\#w_i}\\
			\ln(P(S=\mathbf{s})) &= \ln\bigg(\prod^m_{i=1} P(w_i)^{\#w_i}\bigg)\\
			&= \sum^m_{i=1} (\#w_i)\ln (P(w_i))\\
		\end{align}
		
		And since $\ln$ is monotonous and We just care about the maximum, We can use take the log.
		
		Then We use Lagrange multipliers along with the constraint that 
		
		\begin{align}
			\sum^m_{i=1} P(w_i) &= 1\\
			\sum^m_{i=1} P(w_i)-1 &= 0
		\end{align}
		
		So then We are looking for the max of 
		
		\begin{align}
			\mathcal{L}	= \sum^m_{i=1} (\#w_i)\ln (P(w_i)) + \lambda\bigg( \sum^m_{i=1} P(w_i) - 1 \bigg)
		\end{align}
		
		So then We know
		
		\begin{align}
			\frac{\partial \mathcal{L}}{\partial P(w_i)}	= \frac{\partial }{\partial P(w_i)} \sum^m_{j=1} (\#w_j)\ln (P(w_j)) + \lambda\bigg( \sum^m_{k=1} P(w_k) - 1 \bigg) &= 0\\
			\frac{\#w_i}{P(w_i)} + \lambda &= 0\\
			P(w_i)&= -\frac{\#w_i}{\lambda} 
		\end{align}
		
		And then the derivatives. The count is a constant w.r.t. the probability assigned - should really think of the probability as $P_\theta$ or something.
		
		We are also taking the derivative w.r.t. a particular unigram - what value should the probability of this unigram be set to to maximize probability etc.
		
		So We have an expression which maximizes $\mathbf{L}$, now We use the constraint 
		
		\begin{align}
			\sum^m_{i=1} P(w_i) &= 1\\
			\sum^m_{i=1} -\frac{\#w_i}{\lambda}  &= 1\\
			\frac{1}{\lambda}\sum^m_{i=1} -\#w_i  &= 1\\
			\lambda &= -\sum^m_{i=1} \#w_i
		\end{align}
		
		and finally plugging this back in:
		
		\begin{align}
			P(w_i)&= -\frac{\#w_i}{\lambda} \\
			P(w_i)&= -\frac{\#w_i}{-\sum^m_{i=1} \#w_i}\\
			P(w_i)&= \frac{\#w_i}{\sum^m_{i=1} \#w_i}
		\end{align}
		
		Is the MLE. Wee.
		
	\subsection{Bigram MLE}
	
		Okay so weird:
		
		\begin{align}
			p(\mathbf{w}) = \prod^n_{i=1} p(w_i)^{s(w_i)}\prod^n_{i=1}\prod^n_{j=1} p(w_j|w_i)^{c(w_i, w_j)}
		\end{align}
		
		So what in tarnation is that
		
		Okay so first We are going through this word by word, so We are taking each words contributions to bigrams. 
		
		The index $i$ is shared, and $s(w_i) = 1$ if $w_i$ is the first word in a bigram.
		
		I mean I can probably derive this mess. 
		
		You'll have a product of a bunch of bigram probabilities, and We can further decompose those bigram probabilities into unigram probabilities and conditional probabilities by the product rule.
		
		Actually, I feel like this isn't even necessary as a derivation - to see that bigram MLE for some $(w_i, w_j)$ is
		
		\begin{align}
			\frac{c(w_i, w_j)}{\sum_{j=1}^n c(w_i, w_j)}
		\end{align}
		
		Just observe that this is really a bunch of unigram problems, since You are just looking for ways to distribute the mass of the second word given the first word has happened.
		
	\subsection{Unigram MLE with Uniform distribution KL}
	
		Kay so now We go all the way back:
		
		\begin{align}
	  	 \sum_{\context} \Big( {\color{blue} \KL\left(\tilde{p}(\cdot 			\mid \context) \mid\mid \model(\cdot \mid \context)					\right)} + {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)}\Big)
		\end{align}
		
		Okay so We are summing over contexts predicting labels, equivalent to any old thing really since it's predicting label given data.
		
		\begin{align}
			&\KL(P(\hat{y}|\context), P_\theta(\hat{y}|\context)\\
			&\sum_{y\in\mathcal{Y}} P(\hat{y}|\context )\cdot\ln\bigg(\frac{1}{P_\theta(\hat{y}|\context)} \bigg)\\
			& - \sum_{y\in\mathcal{Y}} P(\hat{y}|\context )\cdot\ln(P_\theta(\hat{y}|\context))\\
			& -\ln(P_\theta(\hat{y} = y_{true}|\context))
		\end{align}
		
		Where $P$ is the true probability and $P_\theta$ is the estimated probability.
		
		Basically We have negative log likelihood and the only probability that gets optimized (though due to unity constraints they all do I suppose) is the probability of the true label.
		
		I guess We again multiply by minus in order to turn this into a maximization problem and We arrive back at Unigram MLE.
		
		Now for the smoothing term:
		
		\begin{align}
			& \KL(u, P_\theta(\hat{y}|\context)\\
			& \sum_{y\in\mathcal{Y}} \frac{1}{|\mathcal{Y}|} \cdot \ln\bigg(\frac{1/|\mathcal{Y}}{P_\theta(\hat{y})} \bigg)\\
			& \frac{1}{|\mathcal{Y}|} \sum_{y\in\mathcal{Y}}   \ln(1/|\mathcal{Y}|) - \ln(P_\theta(\hat{y}) )\\
			& \bigg(\frac{1}{|\mathcal{Y}|} \sum_{y\in\mathcal{Y}}   \ln(1/|\mathcal{Y}|)\bigg)  + \frac{1}{|\mathcal{Y}|} \sum_{y\in\mathcal{Y}} - \ln(P_\theta(\hat{y}) )\\
			& - \frac{1}{|\mathcal{Y}|} \sum_{y\in\mathcal{Y}} \ln(P_\theta(\hat{y}) )\\
		\end{align}
		
		And this is all correct, as far as I can tell. Again We multiply by minus to turn this into a maximization problem. Though I guess We don't need to.
		
		Anyway then Your loss is
		
		\begin{align}
			\mathcal{L} = \ln(P_\theta(\hat{y} = y_{true}|\context)) + \bigg(\frac{1}{|\mathcal{Y}|} \sum_{y\in\mathcal{Y}} \ln(P_\theta(\hat{y} | \context) )\bigg) + \lambda\bigg(\sum_{y\in\mathcal{Y}}\model(y|\context) - 1 \bigg)
		\end{align}
		
		Except for one bit - multiples of the same context. If We are going to sum over possible contexts instead of instances of contexts, We need to account for the possibility of duplicate occurences, i.e. if "I am well" occurs more than once, and if the summation only counts that occurrence once, then it's undercounting. 
		
		And I think the count term applies to all the terms in the sum, since if You think about it, the way We arrived at the above equations is that We made the assumption that the "true" distribution was a mixture of a dirac delta and a univariate distribution, and this assumption will apply to every term in the summation equally, hence counting. This is difficult to follow.
		
		\begin{align}
			\mathcal{L} &=\\			
			&(\#(\context, y_{true}))\bigg(\ln(\model(\hat{y} = y_{true}|\context)) + \bigg(\frac{1}{|\mathcal{Y}|} \sum_{y\in\mathcal{Y}} \ln(P_\theta(\hat{y} | \context) )\bigg)\bigg) \\
			&+ \lambda\bigg(\sum_{y\in\mathcal{Y}}\model(y|\context) - 1 \bigg)
		\end{align}
		
		And now I suppose just the derivative w.r.t. $\model(\hat{y} = y_{true}|\context)$
		
		\begin{align}
			&\frac{\partial}{\partial \model(\hat{y} = y_{true}|\context)} (\#(\context, y_{true}))\bigg(\ln(\model(\hat{y} = y_{true}|\context)) + \bigg(\frac{1}{|\mathcal{Y}|} \sum_{y\in\mathcal{Y}} \ln(P_\theta(\hat{y} | \context) )\bigg)\bigg)\\
			&(\#(\context, y_{true}))\bigg(\frac{1}{\model(\hat{y} = y_{true}|\context))} + \bigg( \frac{1}{\model(\hat{y} = y_{true}|\context)) \cdot |\mathcal{Y}| )}\bigg)\bigg)\\
			&(\#(\context, y_{true})) \bigg( \frac{1 + |\mathcal{Y}|}{\model(\hat{y} = y_{true}|\context)) \cdot |\mathcal{Y}| )}\bigg)\\
			& \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{\model(\hat{y} = y_{true}|\context)) \cdot |\mathcal{Y}|}
		\end{align}
		
		And then the other term:
		
		\begin{align}
			&\frac{\partial}{\partial \model(\hat{y} = y_{true}|\context)} + \lambda\bigg(\sum_{y\in\mathcal{Y}}\model(y|\context) - 1 \bigg) = \lambda
		\end{align}
		
		lol
		
		So then in total We get
		
		\begin{align}
			\frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{\model(\hat{y} = y_{true}|\context)) \cdot |\mathcal{Y}|} + \lambda &= 0\\
			(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|) &= - \model(\hat{y} = y_{true}|\context) \cdot |\mathcal{Y}|\cdot \lambda \\
			\frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{- |\mathcal{Y}|\cdot \lambda 	} &=  \model(\hat{y} = y_{true}|\context) 
		\end{align}
		
		Alright.
		
		Then We plug that into the constraint:
		
		\begin{align}
			\sum_{y\in\mathcal{Y}} \model(y|\context) &= 1\\
			\sum_{y\in\mathcal{Y}} \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{- |\mathcal{Y}|\cdot \lambda } &= 1\\
			\sum_{y\in\mathcal{Y}} \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{- |\mathcal{Y}| } &= \lambda
		\end{align}
		
		oof.
		
		\begin{align}
			\model(\hat{y} = y_{true}|\context) &= \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{- |\mathcal{Y}|\cdot \lambda}\\
			&= \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{- |\mathcal{Y}|\cdot \sum_{y\in\mathcal{Y}} \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{- |\mathcal{Y}| }}\\
			&= \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{\sum_{y\in\mathcal{Y}} (\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}\\
			&= \frac{(\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}{\sum_{y\in\mathcal{Y}} (\#(\context, y_{true}))\cdot (1 + |\mathcal{Y}|)}\\
			&= \frac{(\#\context, y_{true})}{\sum_{y\in\mathcal{Y}} \#(\context, y)}\\
		\end{align}
		
		Uhh.
		
		Something's gone wrong here. If I take the KL divergence of just the univariate part, surely I'll be able to get the Univariate distribution?
		
		\subsubsection{Sanity Check}
		
			\begin{align}
		  		 \sum_{\context} \Big( {\color{blue} \KL\left(\tilde{p}(\cdot 			\mid \context) \mid\mid \model(\cdot \mid \context)					\right)} + {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)}\Big)
			\end{align}
			
			And again, KL divergence is:
			
			\begin{align}
				\KL(P, Q) = \sum_{x\in\mathcal{X}} P(x)\cdot \ln\bigg(\frac{P(x)}{Q(x)}\bigg)
			\end{align}
		
			So the red term is
			
			\begin{align}
				\sum_{\context} {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)} &= \gamma(\context)\cdot\sum u(x)\cdot \ln\bigg(\frac{u(x)}{\model(x|\context)}\bigg)\\
			&= \gamma(\context)\cdot\sum u(x)\cdot\ln(u(x)) - u(x)\cdot\ln(\model(x|\context))\\
				&= \gamma(\context)\cdot\sum -\big(-u(x)\cdot\ln(u(x))\big) - u(x)\cdot\ln(\model(x|\context))\\
				&= \gamma(\context)\cdot\sum -Ent(u) + Cross(u, \model)
			\end{align}
			
			Where $Ent$ is the entropy and $Cross$ is the cross entropy. Sure, this all makes sense. 
			
			So then what function are We trying to find the extrema of? 
			
			\begin{align}
				\mathcal{L} = \bigg(\sum_{\context} \gamma(\context)\cdot\sum_{y\in\mathcal{Y}} u(y)\cdot\ln(u(y)) - u(y)\cdot\ln(\model(y|\context))\bigg) + \lambda\bigg(\sum_{y\in\mathcal{Y}} \model(y|\context) -1 \bigg)
			\end{align}
			
			This is as general as it gets right? Literally just taking the objective and adding a Lagrange multiplier to the equation. 

			Well not quite. The interpretation here is that We are summing over unique contexts, right?
			
			So in the original smoothing paper they just go over each data item and try to optimize the KL divergence between the observed distribution (which is just discrete) and the model distribution. Well, I suppose the first set up is the cross entropy formulation which selects just one model distribution term to analyze at a time, and then they introduce the change to the assumed true distribution by converting it to a mixture of a dirac delta and a uniform noise distribution.
		
			If instead of summing over data items You want to sum over contexts, You'll need to account for duplicates:
			
			\begin{align}
				&\bigg(\sum_{\context} (\#(y, \context)\gamma(\context)\cdot\sum_{y\in\mathcal{Y}} u(y)\cdot\ln(u(y)) - u(y)\cdot\ln(\model(y|\context))\bigg) \\
			&+ \lambda\bigg(\sum_{y\in\mathcal{Y}} \model(y|\context) -1 \bigg)
			\end{align}
			
			Big boi lol. $\#(y, \context)$ is just the count of times that a particular label $y$ was observed with a given context $\context$.
			
			\begin{align}
				&\bigg(\sum_{y\in\mathcal{Y}} (\#(y, \context))\cdot \gamma(\context)\cdot \big( u(y)\cdot\ln(u(y)) - u(y)\cdot\ln(\model(y|\context))\big)\bigg) \\
			&+ \lambda\bigg(\sum_{y\in\mathcal{Y}} \model(y|\context) -1 \bigg)
			\end{align}
			
			But the indexing is all wrong.
			
			We have a bunch of contexts. Sure.
			
			For each unique context, We have the number of instances that that context occurred with each individual label, sure.
			
			For each of those occurrences of a label and a context, We get a KL term. That KL term is the same for all cases, right? Yep.
			
			So this all reduces down to
			
			\begin{align}
				&(\#(\context))\cdot \gamma(\context)\cdot \bigg( \sum_{y\in\mathcal{Y}} u(y)\cdot\ln(u(y)) - u(y)\cdot\ln(\model(y|\context))\bigg) + \lambda\bigg(\sum_{y\in\mathcal{Y}} \model(y|\context) -1 \bigg)
			\end{align}
			
			And then piecewise derivative:
			
			\begin{align}
				& \frac{\partial}{\partial \model(y_j|\context)} (\#(\context))\cdot \gamma(\context)\cdot  \sum_{y\in\mathcal{Y}} u(y)\cdot\ln(u(y)) - u(y)\cdot\ln(\model(y|\context))\\
				&=  (\#(\context))\cdot \gamma(\context)\cdot    - u(y)\frac{1}{\model(y_j|\context)}
			\end{align}
			
			Similarly
			
			\begin{align}
				&\frac{\partial}{\partial \model(y_j|\context)} \lambda\bigg(\sum_{y\in\mathcal{Y}} \model(y|\context) -1 \bigg)\\
				&=\lambda
			\end{align}
			
			So then We in total get $|\mathcal{Y}|$ equations which read:
			
			\begin{align}
				 \forall y_j\in\mathcal{Y}:(\#(\context))\cdot \gamma(\context)\cdot    - u(y)\frac{1}{\model(y_j|\context)} + \lambda &= 0\\
				(\#(\context))\cdot \gamma(\context)\cdot - u(y)\frac{1}{\model(y_j|\context)}  &= -\lambda\\
				(\#(\context))\cdot \gamma(\context)\cdot u(y)\frac{1}{\model(y_j|\context)}  &= \lambda\\
				\frac{1}{\model(y_j|\context)}  &= \frac{\lambda}{(\#(\context))\cdot \gamma(\context)\cdot u(y)}\\
				\model(y_j|\context)  &= \frac{(\#(\context))\cdot \gamma(\context)\cdot u(y)}{\lambda}
			\end{align}
			
			And We can plug that into the constraint yet again
			
			\begin{align}
				\sum_{y\in\mathcal{Y}} \model(y|\context) &= 1\\
				\sum_{y\in\mathcal{Y}} \frac{(\#(\context))\cdot \gamma(\context)\cdot u(y)}{\lambda} &= 1\\
				 \frac{\sum_{y\in\mathcal{Y}} (\#(\context))\cdot \gamma(\context)\cdot u(y)}{\lambda} &= 1\\
				 \sum_{y\in\mathcal{Y}} (\#(\context))\cdot \gamma(\context)\cdot u(y) &= \lambda
			\end{align}
			
			And then back again
			
			\begin{align}
				\model(y_j|\context)  &= \frac{(\#(\context))\cdot \gamma(\context)\cdot u(y)}{\lambda}\\
				\model(y_j|\context)  &= \frac{(\#(\context))\cdot \gamma(\context)\cdot u(y)}{\sum_{y\in\mathcal{Y}} (\#(\context))\cdot \gamma(\context)\cdot u(y)} = \frac{1}{|\mathcal{Y}|}
			\end{align}
			
			We did it reddit, We calculated a trivial MLE solution lol
			
	\subsection{Unigram MLE with Uniform distribution KL but right lol}
	
		\begin{align}
	  		\sum_{\context} \Big( {\color{blue} \KL\left(\tilde{p}(\cdot 			\mid \context) \mid\mid \model(\cdot \mid \context)					\right)} + {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)}\Big)
		\end{align}
		
		\begin{align}
			\KL(p(\cdot|\context), \model(\cdot|\context) = \sum_{y\in\mathcal{Y}} p(y|\context)\ln\bigg(\frac{p(y|\context)}{\model(y|\context)} \bigg)
		\end{align}
		
		\begin{align}
			\sum_{\context} {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)} &= \gamma(\context)\cdot\sum u(x)\cdot \ln\bigg(\frac{u(x)}{\model(x|\context)}\bigg)\\
		&= \gamma(\context)\cdot\sum u(x)\cdot\ln(u(x)) - u(x)\cdot\ln(\model(x|\context))\\
		&= \gamma(\context)\cdot\sum - u(x)\cdot\ln(\model(x|\context))
		\end{align}
		
		We just drop the term that depends only on the uniform distribution since it's going to be 0 when We take the derivative anyway.
		
		Now for the indexing. 
		
		We're conditioning on a unique context.
		
		Need to sum over labels twice?
		
		\begin{align}
			\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot \bigg( \sum_{y_j\in\mathcal{Y}} p(y_j|\context)\ln\bigg(\frac{p(y_j|\context)}{\model(y_j|\context)} \bigg) + \gamma(\context)\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context)) \bigg)
		\end{align}	
		
		Now I feel like some of those indexes can be unified.
		
		The outermost counts the occurrences, so that's fine I think.
		
		$y_i$ is the "true" label - so if $y_j\neq y_i$, then $p(y_j|context) = 0$. 
		
		\begin{align}
			\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot \bigg(  \ln\bigg(\frac{p(y_i|\context)}{\model(y_i|\context)} \bigg) + \gamma(\context)\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context)) \bigg)
		\end{align}		
			
		Now for the third summation - it really depends on the context rather than any particular true label, so it can be just split out all together:
		
		\begin{align}
			&\bigg(\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot \bigg( \ln\bigg(\frac{p(y_i|\context)}{\model(y_i|\context)} \bigg)\bigg) + \#(\context)\gamma(\context)\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context)) 
		\end{align}	
		
		Yeet. Then in total We get 
		
		\begin{align}
			&\bigg(\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot  \ln\bigg(\frac{1}{\model(y_i|\context)} 	\bigg)\bigg) \\
			&+ \#(\context)\gamma(\context)\bigg(\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context))\bigg) \\
			&+ \bigg(\lambda\sum_{y_l\in\mathcal{Y}} \model(y_l|\context) -1\bigg) 
		\end{align}	
		
		Then We take the derivative w.r.t. $\model(y_m|\context)$, where $y_m$ is just some label
		
		\begin{align}
			&\#(y_m, \context)\cdot  \frac{1}{\model(y_m|\context)} 	 \\
			&- \#(\context)\cdot \gamma(\context)\cdot u(y_m)\cdot\frac{1}{\model(y_m|\context)} \\
			&+ \lambda \\
			&=0
		\end{align}	
		
		lol
		
		We have $|\mathcal{Y}|$ of those expressions. 
		
		So then 
		
		\begin{align}
			\#(y_m, \context)\cdot  \frac{1}{\model(y_m|\context)} - \#(\context)\cdot \gamma(\context)\cdot u(y_m)\cdot\frac{1}{\model(y_m|\context)} + \lambda &= 0\\
			(\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m))   \frac{1}{\model(y_m|\context)} + \lambda &= 0\\
			(\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m))   \frac{1}{\model(y_m|\context)} &= - \lambda \\
			\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)  &= - \lambda\cdot \model(y_m|\context) \\
			\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)}{-\lambda}  &=  \model(y_m|\context) \\
		\end{align}	
		
		And then
		
		\begin{align}
			\sum_{y\in\mathcal{Y}} \model(y|\context) &= 1\\
			\sum_{y\in\mathcal{Y}} \frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)}{-\lambda} &= 1\\
			  \sum_{y\in\mathcal{Y}} \#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m) &= -\lambda\\
		\end{align}
		
		And then
		
		\begin{align}
			\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)}{-\lambda}  &=  \model(y_m|\context) \\
			\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)}{\sum_{y\in\mathcal{Y}} \#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)}  &=  \model(y_m|\context)
		\end{align}
		
		So does this simplify. Sorta?
		
		\begin{align}
			&\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)}{\sum_{y\in\mathcal{Y}} \#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y_m)}  \\
			&\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y)}{\sum_{y\in\mathcal{Y}} \#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y)}  \\
			&\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot u(y)}{ - |\mathcal{Y}| \#(\context)\cdot \gamma(\context)\cdot u(y) + \sum_{y\in\mathcal{Y}} \#(y_m, \context)} \\
			&\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot (1/|\mathcal{Y}|)}{ - |\mathcal{Y}| \#(\context)\cdot \gamma(\context)\cdot (1/|\mathcal{Y}|) + \sum_{y\in\mathcal{Y}} \#(y_m, \context)} \\
			&\frac{\#(y_m, \context) - \#(\context)\cdot \gamma(\context)\cdot (1/|\mathcal{Y}|)}{ -  \#(\context)\cdot \gamma(\context) + \sum_{y\in\mathcal{Y}} \#(y_m, \context)} \\
			&\frac{\#(y_m, \context) - \delta\cdot (1/|\mathcal{Y}|)}{ -  \delta + \sum_{y\in\mathcal{Y}} \#(y_m, \context)}
		\end{align}
		
		\subsubsection{Mistakes}
		
			\begin{align}
		  		\sum_{\context} \Big( {\color{blue} \KL\left(\tilde{p}(\cdot 			\mid \context) \mid\mid \model(\cdot \mid \context)					\right)} + {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)}\Big)
			\end{align}
			
			\begin{align}
				\KL(p(\cdot|\context), \model(\cdot|\context) = \sum_{y\in\mathcal{Y}} p(y|\context)\ln\bigg(\frac{p(y|\context)}{\model(y|\context)} \bigg)
			\end{align}
			
			\begin{align}
				\sum_{\context} {\color{red}\gamma(\context) \cdot 						\KL(\uniform \mid\mid \model(\cdot \mid \context)} &= \gamma(\context)\cdot\sum u(x)\cdot \ln\bigg(\frac{u(x)}{\model(x|\context)}\bigg)\\
			&= \gamma(\context)\cdot\sum u(x)\cdot\ln(u(x)) - u(x)\cdot\ln(\model(x|\context))\\
			&= \gamma(\context)\cdot\sum_{y\in\mathcal{Y}} - u(y)\cdot\ln(\model(y|\context))
			\end{align}
			
			We just drop the term that depends only on the uniform distribution since it's going to be 0 when We take the derivative anyway.
			
			Now for the indexing. 
			
			We're conditioning on a unique context.
			
			Need to sum over labels twice?
			
			\begin{align}
				\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot \bigg( \sum_{y_j\in\mathcal{Y}} p(y_j|\context)\ln\bigg(\frac{p(y_j|\context)}{\model(y_j|\context)} \bigg) + \gamma(\context)\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context)) \bigg)
			\end{align}	
			
			Now I feel like some of those indexes can be unified.
			
			The outermost counts the occurrences, so that's fine I think.
			
			$y_i$ is the "true" label - so if $y_j\neq y_i$, then $p(y_j|context) = 0$. 
			
			\begin{align}
				\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot \bigg(  \ln\bigg(\frac{p(y_i|\context)}{\model(y_i|\context)} \bigg) + \gamma(\context)\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context)) \bigg)
			\end{align}		
				
			Now for the third summation - it really depends on the context rather than any particular true label, so it can be just split out all together:
			
			\begin{align}
				&\bigg(\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot \bigg( \ln\bigg(\frac{p(y_i|\context)}{\model(y_i|\context)} \bigg)\bigg) + \#(\context)\gamma(\context)\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context)) 
			\end{align}	
			
			Yeet. Then in total We get 
			
			\begin{align}
				&\bigg(\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot  \ln\bigg(\frac{1}{\model(y_i|\context)} 	\bigg)\bigg) \\
				&+ \#(\context)\gamma(\context)\bigg(\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context))\bigg) \\
				&+ \bigg(\lambda\sum_{y_l\in\mathcal{Y}} \model(y_l|\context) -1\bigg) 
			\end{align}	
			
			Then We take the derivative w.r.t. $\model(y_m|\context)$, where $y_m$ is just some label
			
			\begin{align}
				&\#(y_m, \context)\cdot  \frac{1}{\model(y_m|\context)} 	 \\
				&- \#(\context)\cdot \gamma(\context)\cdot u(y_m)\cdot\frac{1}{\model(y_m|\context)} \\
				&+ \lambda \\
				&=0
			\end{align}	
			
			And this is where things went wrong. Instead take the derivative piece by piece:
			
			\begin{align}
				\frac{\partial}{\partial \model(y_m|\context)} &\bigg(\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot  \ln\bigg(\frac{1}{\model(y_i|\context)} 	\bigg)\bigg) \\
			\end{align}
			
			Let's just do
			
			\begin{align}
				\frac{d}{d x} \ln\bigg(\frac{1}{x} \bigg) &= \frac{d}{du}\ln(u)\cdot \frac{d}{dx} \frac{1}{x},\quad\quad u=\frac{1}{x}\\
			&= \frac{1}{u}\cdot \frac{-1}{x^2}\\
			&= \frac{1}{x^{-1}}\cdot \frac{-1}{x^2}\\
			&= -x^{-1}
			\end{align}
			
			So then back again
		
			\begin{align}
				\frac{\partial}{\partial \model(y_m|\context)} &\sum_{y_i\in\mathcal{Y}} (\#(y_i, \context))\cdot  \ln\bigg(\frac{1}{\model(y_i|\context)} 	\bigg)  =  \frac{-\#(y_m, \context)}{\model(y_m|\context)}
			\end{align}
			
			Next We have
			
			\begin{align}
				\frac{\partial}{\partial \model(y_m|\context)} \#(\context)\gamma(\context)\bigg(\sum_{y_k\in\mathcal{Y}} - u(y_k)\cdot\ln(\model(y_k|\context))\bigg) = -\frac{\#(\context)\gamma(\context)u(y)}{\model(y_m|\context)} \\
			\end{align}	
		
			and the last one's just $\lambda$ lol
			
			All together then:
			
			\begin{align}
				\frac{-\#(y_m, \context)}{\model(y_m|\context)} -\frac{\#(\context)\gamma(\context)u(y)}{\model(y_m|\context)} +\lambda &= 0\\
				\frac{-\#(y_m, \context)}{\model(y_m|\context)} -\frac{\#(\context)\gamma(\context)u(y)}{\model(y_m|\context)}  &= -\lambda	\\
				-\#(y_m, \context) -\#(\context)\gamma(\context)u(y)  &= -\lambda \model(y_m|\context)\\
				\frac{\#(y_m, \context) +\#(\context)\gamma(\context)u(y)}{\lambda}  &= \model(y_m|\context)	
			\end{align}
		
			Then back again
			
			\begin{align}
				\sum_{y_i\in\mathcal{Y}} \model(y_i|\context) &= 1\\\
				\sum_{y_i\in\mathcal{Y}} \frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{\lambda} &= 1\\
				\frac{ \sum_{y_i\in\mathcal{Y}} \#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{\lambda} &= 1\\
				 \sum_{y_i\in\mathcal{Y}} \#(y_i, \context) +\#(\context)\gamma(\context)u(y) &= \lambda
			\end{align}
			
			Then back again
		
			\begin{align}
				\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{\lambda}  &= \model(y_m|\context)\\
				\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{\sum_{y_i\in\mathcal{Y}} \#(y_i, \context) +\#(\context)\gamma(\context)u(y)}  &= \model(y_m|\context)	
			\end{align}
			
			And then You try to simplify I suppose
			
			\begin{align}
				&\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{\sum_{y_i\in\mathcal{Y}} \#(y_i, \context) +\#(\context)\gamma(\context)u(y)}\\
				&\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{ \#(\context) + \sum_{y_i\in\mathcal{Y}} \#(\context)\gamma(\context)u(y)}\\
				&\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{ \#(\context) + |\mathcal{Y}|\#(\context)\gamma(\context)u(y)}\\
				&\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{ \#(\context) + \#(\context)\gamma(\context)}	
			\end{align}
	
			Then letting $\gamma(\context) = \alpha/\#(\context)$.
		
			\begin{align}
				\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{ \#(\context) + \#(\context)\gamma(\context)}	&= \frac{\#(y_i, \context) +\alpha u(y)}{ \#(\context) + \alpha}\\
				&= \frac{\#(y_i, \context) + \frac{\alpha}{|\mathcal{Y}|}}{ \#(\context) + \alpha}
			\end{align}
		
			Das ist correct, I think. To recover the standard formulation for Laplace smoothing let $\gamma(\context) = \frac{|\mathcal{Y}|\alpha}{\#(\context)}$.
		
			\begin{align}
				\frac{\#(y_i, \context) +\#(\context)\gamma(\context)u(y)}{ \#(\context) + \#(\context)\gamma(\context)}	&= \frac{\#(y_i, \context) +|\mathcal{Y}|\alpha u(y)}{ \#(\context) + |\mathcal{Y}|\alpha}\\
				&= \frac{\#(y_i, \context) + \alpha}{ \#(\context) + |\mathcal{Y}|\alpha}
			\end{align}
		
		
	\subsection{Sanity Check}
	
		\begin{align}
			\sum_w \frac{(r_w+1)S(N_{r_w+1})}{NS(N_{r_w})} &= \sum_w \frac{(r_w+1)N_{r_w+1}}{NN_{r_w}}\\
			&= \frac{1}{N}\sum_w \frac{(r_w+1)N_{r_w+1}}{N_{r_w}}
		\end{align}
		
		Then You group the words by frequency to cancel the denominator and You're done.
		
		Question: What does it mean to sum over all words?
		
		You know, in practice, the amount of mass saved for words that occur zero times is $n_1/N$.
		
	\subsection{General? Counts}
	
		\begin{align}
			\KL(\tilde{p}\mid\mid\model) + \lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)
		\end{align}
		
		$g_p = r_{pos}$ is the original derivation. Changing $r$ to $g$ since $r$ usually designated frequency of some word.
		
		That first equation is from the "general" perspective - $\tilde{p}$ is the empirical distribution of tokens in the dataset.
		
		But in practice We will be using NLL loss, and that loss is done per-token, so We can instead let $\tilde{p}$ be the empirical distribution of a particular token - the perspective is that You are going along the dataset, one token at a time, and at each token You observe a one-hot distribution, where 1 is the probability of the token which was actually observed.
		
	    The loss over all tokens will then be:
		
		\begin{align}
			\sum^{|V|}_{i=1} \#(w_i)\big(\KL(\tilde{p}\mid\mid\model) + \alpha_i(\lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model))\big)
		\end{align}
		
		Where We introduced some scaling factor $\alpha_i$ to restore the balance between the empirical distribution KL terms and the $g_{pos}$ and all that.
		
		
		For now lump the KL terms together: $h = \lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)$
		
		\begin{align}
			\sum^{|V|}_{i=1} \#(w_i)\big(\KL(\tilde{p}\mid\mid\model) + \alpha_i h\big)
		\end{align}
		
		So then on a token by token basis:
		
		\begin{align}
			\sum^{|V|}_{i=1} \#(w_i) \big(\KL(\tilde{p}\mid\mid\model) + \alpha_i h \big)&= \sum^{|V|}_{i=1} \#(w_i) \bigg(\ln\bigg(\frac{1}{\model(w_i)}\bigg) + \alpha_i h \bigg)
		\end{align}
		
		That's all We need really, since the Lagrange equation is then:
		
		\begin{align}
			\mathcal{L} &= \bigg(\sum^{|V|}_{i=1} \#(w_i) \bigg(\ln\bigg(\frac{1}{\model(w)}\bigg) + \alpha_i h \bigg)\bigg) + \lambda\bigg(\bigg(\sum^{|V|}_{i=1} \model(w_i)\bigg) -1 \bigg) 
		\end{align}
		
		Then the derivative piece by piece:
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \ln\bigg(\frac{1}{\model(w_j)}\bigg) = -\frac{1}{\model(w_j)}
		\end{align}
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \alpha_j h &= \frac{\partial}{\partial \model(w_j)} \alpha_j\big(\lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)\big)
		\end{align}
		
		Let's just focus on an individual KL term I suppose:
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \lambda_p\KL(g_p\mid\mid\model) &= \frac{\partial}{\partial \model(w_j)} \sum^{|V|}_{i=1} g_p(w_i)\ln\bigg(\frac{g_p(w_i)}{\model(w_i)}\bigg)\\
			&= \frac{\partial}{\partial \model(w_j)} g_p(w_j)\ln\bigg(\frac{g_p(w_j)}{\model(w_j)}\bigg)\\
			&= \frac{\partial}{\partial \model(w_j)} -g_p(w_j)\ln\bigg(\frac{\model(w_j)}{g_p(w_j)}\bigg)\\
			&=  -g_p(w_j)\frac{1}{g_p(w_j)}\frac{g_p(w_j)}{\model(w_j)}\\
			&=  -g_p(w_j)\frac{1}{\model(w_j)}
		\end{align}
		
		Alright so then 
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \alpha_j h &= \frac{\partial}{\partial \model(w_j)} \alpha_j\big(\lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)\big)\\
			&= \alpha_j\bigg(-\lambda_p g_p(w_j)\frac{1}{\model(w_j)} -\lambda_n g_n(w_j)\frac{1}{\model(w_j)}\bigg)\\
			&= -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
		\end{align}
		
		And of course
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)}  \lambda\bigg(\bigg(\sum^{|V|}_{i=1} \model(w_i)\bigg) -1 \bigg) = \lambda
		\end{align}
		
		So putting it all together:
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \mathcal{L} &= \#(w_j)\bigg(-\frac{1}{\model(w_j)} -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg) \bigg) + \lambda = 0			
		\end{align}
		
		And now We need to get an expression for $\model(w_j)$
		
		\begin{align}
			\#(w_j)\bigg(-\frac{1}{\model(w_j)} -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg) \bigg) +\lambda&= 0\\
			-\frac{1}{\model(w_j)} -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)  &= \frac{-\lambda}{\#(w_j)}\\
			-\frac{1}{\model(w_j)}\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)  &= \frac{-\lambda}{\#(w_j)}\\
			\frac{-\model(w_j)}{\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)}  &= \frac{\#(w_j)}{-\lambda}
		\end{align}
		
		And so then
		
		\begin{align}
			\model(w_j)  &= \bigg(1 -\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)\frac{\#(w_j)}{\lambda}
		\end{align}
		
		Plugging this into the constraint:
		
		\begin{align}
		 	\sum^{|V|}_{i=1} \model(w_i) &= 1\\
		 	\sum^{|V|}_{i=1} \bigg(1 -\alpha_o\bigg(\lambda_p g_p(w_i) +\lambda_n g_n(w_i)\bigg)\bigg)\frac{\#(w_i)}{\lambda} &= 1\\
		 	\sum^{|V|}_{i=1} \bigg(1 -\alpha_i\bigg(\lambda_p g_p(w_i) +\lambda_n g_n(w_i)\bigg)\bigg)\#(w_i) &= \lambda
		\end{align}
		
		Alright lol, so then
		
		\begin{align}
			\model(w_j)  &= \bigg(1 -\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)\frac{\#(w_j)}{\lambda}\\
			&= \frac{\#(w_j)\bigg(1 -\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)}{\lambda}\\
			&= \frac{\#(w_j)\bigg(1 -\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)}{\sum^{|V|}_{i=1} \bigg(1 -\alpha_i\bigg(\lambda_p g_p(w_i) +\lambda_n g_n(w_i)\bigg)\bigg)\#(w_i)}
		\end{align}
		
		Lot of ado about not much lol.
		
		What does $\alpha$ need to be for this to equal add-delta?
		
		\begin{align}
			\model(w_j) = \frac{\#(w_j) + \delta}{|V|*\delta + \sum^{|V|}_{i=1} \#(w_i)}
		\end{align}
		
		I guess We just need 
		
		\begin{align}
			\#(w_j) + \delta &= \#(w_j)\bigg(1 -\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			&= \#(w_j) -\#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			 \delta &= -\#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			 \frac{-\delta}{\#(w_j)\big(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\big)} &= \alpha_j\\
		\end{align}
		
		Huh.
		
		\begin{align}
			\delta &=-\#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			\delta &= -\#(w_j)\alpha_j h(w_j)\\
			\delta &= -\#(w_j)\alpha_j \bigg(\bigg(C - \frac{1}{N}\bigg)\#(w_j) + \delta C\bigg)\\
			\frac{\delta}{\#(w_j)\bigg(\bigg(C - \frac{1}{N}\bigg)\#(w_j) + \delta C\bigg)} &= \alpha_j \\	
		\end{align}
		

\newpage
\subsection{Mistakes}
	
		\begin{align}
			\KL(\tilde{p}\mid\mid\model) + \lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)
		\end{align}
		
		$g_p = r_{pos}$ is the original derivation. Changing $r$ to $g$ since $r$ usually designated frequency of some word.
		
		That first equation is from the "general" perspective - $\tilde{p}$ is the empirical distribution of tokens in the dataset.
		
		But in practice We will be using NLL loss, and that loss is done per-token, so We can instead let $\tilde{p}$ be the empirical distribution of a particular token - the perspective is that You are going along the dataset, one token at a time, and at each token You observe a one-hot distribution, where 1 is the probability of the token which was actually observed.
		
	    The loss over all tokens will then be:
		
		\begin{align}
			\sum^{|V|}_{i=1} \#(w_i)\big(\KL(\tilde{p}\mid\mid\model) + \alpha_i(\lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model))\big)
		\end{align}
		
		Where We introduced some scaling factor $\alpha_i$ to restore the balance between the empirical distribution KL terms and the $g_{pos}$ and all that.
		
		
		For now lump the KL terms together: $h = \lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)$
		
		\begin{align}
			\sum^{|V|}_{i=1} \#(w_i)\big(\KL(\tilde{p}\mid\mid\model) + \alpha_i h\big)
		\end{align}
		
		So then on a token by token basis:
		
		\begin{align}
			\sum^{|V|}_{i=1} \#(w_i) \big(\KL(\tilde{p}\mid\mid\model) + \alpha_i h \big)&= \sum^{|V|}_{i=1} \#(w_i) \bigg(\ln\bigg(\frac{1}{\model(w_i)}\bigg) + \alpha_i h \bigg)
		\end{align}
		
		That's all We need really, since the Lagrange equation is then:
		
		\begin{align}
			\mathcal{L} &= \bigg(\sum^{|V|}_{i=1} \#(w_i) \bigg(\ln\bigg(\frac{1}{\model(w)}\bigg) + \alpha_i h \bigg)\bigg) + \lambda\bigg(\bigg(\sum^{|V|}_{i=1} \model(w_i)\bigg) -1 \bigg) 
		\end{align}
		
		Then the derivative piece by piece:
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \ln\bigg(\frac{1}{\model(w_j)}\bigg) = -\frac{1}{\model(w_j)}
		\end{align}
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \alpha_j h &= \frac{\partial}{\partial \model(w_j)} \alpha_j\big(\lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)\big)
		\end{align}
		
		Let's just focus on an individual KL term I suppose:
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \lambda_p\KL(g_p\mid\mid\model) &= \frac{\partial}{\partial \model(w_j)} \sum^{|V|}_{i=1} g_p(w_i)\ln\bigg(\frac{g_p(w_i)}{\model(w_i)}\bigg)\\
			&= \frac{\partial}{\partial \model(w_j)} g_p(w_j)\ln\bigg(\frac{g_p(w_j)}{\model(w_j)}\bigg)\\
			&= \frac{\partial}{\partial \model(w_j)} -g_p(w_j)\ln\bigg(\frac{\model(w_j)}{g_p(w_j)}\bigg)\\
			&=  -g_p(w_j)\frac{1}{g_p(w_j)}\frac{g_p(w_j)}{\model(w_j)}\\
			&=  -g_p(w_j)\frac{1}{\model(w_j)}
		\end{align}
		
		Alright so then 
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \alpha_j h &= \frac{\partial}{\partial \model(w_j)} \alpha_j\big(\lambda_p\KL(g_p\mid\mid\model) + \lambda_n\KL(g_n\mid\mid\model)\big)\\
			&= \alpha_j\bigg(-\lambda_p g_p(w_j)\frac{1}{\model(w_j)} -\lambda_n g_n(w_j)\frac{1}{\model(w_j)}\bigg)\\
			&= -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
		\end{align}
		
		And of course
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)}  \lambda\bigg(\bigg(\sum^{|V|}_{i=1} \model(w_i)\bigg) -1 \bigg) = \lambda
		\end{align}
		
		So putting it all together:
		
		\begin{align}
			\frac{\partial}{\partial \model(w_j)} \mathcal{L} &= \#(w_j)\bigg(-\frac{1}{\model(w_j)} -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg) \bigg) + \lambda = 0			
		\end{align}
		
		And now We need to get an expression for $\model(w_j)$
		
		\begin{align}
			\#(w_j)\bigg(-\frac{1}{\model(w_j)} -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg) \bigg) +\lambda&= 0\\
			-\frac{1}{\model(w_j)} -\alpha_j\frac{1}{\model(w_j)}\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)  &= \frac{-\lambda}{\#(w_j)}\\
			-\frac{1}{\model(w_j)}\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)  &= \frac{-\lambda}{\#(w_j)}\\
			\frac{-\model(w_j)}{\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)}  &= \frac{\#(w_j)}{-\lambda}
		\end{align}
		
		And so then
		
		\begin{align}
			\model(w_j)  &= \bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)\frac{\#(w_j)}{\lambda}
		\end{align}
		
		Plugging this into the constraint:
		
		\begin{align}
		 	\sum^{|V|}_{i=1} \model(w_i) &= 1\\
		 	\sum^{|V|}_{i=1} \bigg(1 +\alpha_o\bigg(\lambda_p g_p(w_i) +\lambda_n g_n(w_i)\bigg)\bigg)\frac{\#(w_i)}{\lambda} &= 1\\
		 	\sum^{|V|}_{i=1} \bigg(1 +\alpha_i\bigg(\lambda_p g_p(w_i) +\lambda_n g_n(w_i)\bigg)\bigg)\#(w_i) &= \lambda
		\end{align}
		
		Alright lol, so then
		
		\begin{align}
			\model(w_j)  &= \bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)\frac{\#(w_j)}{\lambda}\\
			&= \frac{\#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)}{\lambda}\\
			&= \frac{\#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)}{\sum^{|V|}_{i=1} \bigg(1 +\alpha_i\bigg(\lambda_p g_p(w_i) +\lambda_n g_n(w_i)\bigg)\bigg)\#(w_i)}
		\end{align}
		
		Lot of ado about not much lol.
		
		What does $\alpha$ need to be for this to equal add-delta?
		
		\begin{align}
			\model(w_j) = \frac{\#(w_j) + \delta}{|V|*\delta + \sum^{|V|}_{i=1} \#(w_i)}
		\end{align}
		
		I guess We just need 
		
		\begin{align}
			\#(w_j) + \delta &= \#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			&= \#(w_j) +\#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			 \delta &= \#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			 \frac{\delta}{\#(w_j)\big(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\big)} &= \alpha_j\\
		\end{align}
		
		Huh.
		
		\begin{align}
			\delta &=-\#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			\delta &= -\#(w_j)\alpha_j h(w_j)\\
			\delta &= -\#(w_j)\alpha_j \bigg(\bigg(C - \frac{1}{N}\bigg)\#(w_j) + \delta C\bigg)\\
			\frac{\delta}{\#(w_j)\bigg(\bigg(C - \frac{1}{N}\bigg)\#(w_j) + \delta C\bigg)} &= \alpha_j \\	
		\end{align}
		

	\subsection{More Generally?}
	
		Complications:
		
		We want the smoothed probability in terms of smoothed counts and a normalizer, such that the normalizer is the sum of the smoothed counts. Sort of works by definition.
		
		In the case of add-delta We invented instances, so it was a little more complicated.
		
		What about Good-Turing?
		
		\begin{align}
			\model(w_j) &= \frac{(r_{w_j}+1)S(N_{r_{w_j}+1})}{NS(N_{r_{w_j}})}
		\end{align}
		
		But it's my understanding that the "total count" is still supposed to be $N$, so
		
		\begin{align}
			\frac{(r_{w_j}+1)S(N_{r_{w_j}+1})}{S(N_{r_{w_j}})} &= \#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			\frac{(\#(w_j)+1)S(N_{r_{w_j}+1})}{S(N_{r_{w_j}})} &= \#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			\frac{(\#(w_j)+1)S(N_{r_{w_j}+1})}{S(N_{r_{w_j}})} &= \#(w_j) +\#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			\frac{(\#(w_j)+1)S(N_{r_{w_j}+1})}{S(N_{r_{w_j}})} - \#(w_j) &= \#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			\frac{\frac{(\#(w_j)+1)S(N_{r_{w_j}+1})}{S(N_{r_{w_j}})} - \#(w_j)}{\#(w_j)\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)} &= \alpha_j
		\end{align}
		
		Beautiful lol. I think in general You can write:
		
		\begin{align}
			\frac{\frac{(\#(w_j)+1)S(N_{r_{w_j}+1})}{S(N_{r_{w_j}})} - \#(w_j)}{\#(w_j)\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)} &= \alpha_j\\
			\frac{p'(w_j) * z' + \tilde{p}(w_j)*N}{\#(w_j)\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)} &= \alpha_j
		\end{align}
		
		Where $z'$ is the normalizer for $p'$, the smoothed probability.
		
	\subsection{General Formula}
	
		Without any assumptions (I think) We have:
	
		\begin{align}
			\model(w_j)&= \frac{\#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)}{\sum^{|V|}_{i=1} \bigg(1 +\alpha_i\bigg(\lambda_p g_p(w_i) +\lambda_n g_n(w_i)\bigg)\bigg)\#(w_i)}
		\end{align}
		
		We then simply need
		
		\begin{align}
			p'(w_j)*z' = \#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)
		\end{align}
		
		Where $z'$ is the normalizer for the new smoothed distribution (which is usually just $N$ if the count is "preserved", but it isn't always, like in add-$\delta$ smoothing for example). Then in general We can say
		
		\begin{align}
			p'(w_j)*z' &= \#(w_j)\bigg(1 +\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\bigg)\\
			p'(w_j)*z' - \#(w_j) &= \#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			p'(w_j)*z' - \tilde{p}*N &= \#(w_j)\alpha_j\bigg(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\bigg)\\
			\frac{p'(w_j)*z' - \tilde{p}*N}{\#(w_j) \big(\lambda_p g_p(w_j) +\lambda_n g_n(w_j)\big)} &= \alpha_j
		\end{align}
		
	\subsection{Checking Code}
	
		Okay so NoSmoother and AddLambda are just classes implementing the true smoothing counts.
		
		The main line:
		
		\begin{align}
			KL(\tilde{p}\mid\mid p_\theta
		\end{align}






		
		
		
		
		
		
		
		
		
\end{document}